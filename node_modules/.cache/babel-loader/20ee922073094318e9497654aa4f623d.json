{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the softmax normalized vector given the logits.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\n\nfunction softmax_(logits, dim = -1) {\n  const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');\n\n  if (dim === -1) {\n    dim = $logits.rank - 1;\n  }\n\n  if (dim !== $logits.rank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + `Logits was rank ${$logits.rank} and dim was ${dim}`);\n  }\n\n  const inputsToSave = [];\n  const outputsToSave = [true];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.softmax($logits, dim);\n    save([y]);\n    return y;\n  }, {\n    logits: $logits\n  }, (dy, saved) => {\n    const [y] = saved;\n    const dyTimesY = dy.mul(y);\n    const keepDims = true;\n    return {\n      logits: () => dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y))\n    };\n  }, 'Softmax', {\n    dim\n  }, inputsToSave, outputsToSave);\n}\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\n\n\nfunction logSoftmax_(logits, axis = -1) {\n  const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n\n  if (axis === -1) {\n    axis = $logits.rank - 1;\n  }\n\n  if (axis !== $logits.rank - 1) {\n    throw Error('Log Softmax along a non-last dimension is not yet supported. ' + `Logits was rank ${$logits.rank} and axis was ${axis}`);\n  }\n\n  const customOp = customGrad((logits, save) => {\n    const keepDims = true;\n    const xMax = logits.max(axis, true);\n    const shifted = logits.sub(xMax);\n    const value = shifted.toFloat().sub(shifted.exp().sum(axis, keepDims).log());\n    save([value]);\n\n    const gradFunc = (dy, saved) => {\n      const [value] = saved;\n      const softmax = value.exp();\n      return dy.sub(dy.sum(axis, keepDims).mul(softmax));\n    };\n\n    return {\n      value,\n      gradFunc\n    };\n  });\n  return customOp($logits);\n}\n\nexport const softmax = op({\n  softmax_\n});\nexport const logSoftmax = op({\n  logSoftmax_\n});","map":{"version":3,"sources":["../../src/ops/softmax.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,UAAR,QAAyB,cAAzB;AAGA,SAAQ,eAAR,QAA8B,oBAA9B;AAGA,SAAQ,EAAR,QAAiB,aAAjB;AAEA;;;;;;;;;;;;;;;;;;;;AAmBA;;AACA,SAAS,QAAT,CAAoC,MAApC,EAA0D,GAAG,GAAG,CAAC,CAAjE,EAAkE;AAChE,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,EAA8B,SAA9B,CAA/B;;AAEA,MAAI,GAAG,KAAK,CAAC,CAAb,EAAgB;AACd,IAAA,GAAG,GAAG,OAAO,CAAC,IAAR,GAAe,CAArB;AACD;;AACD,MAAI,GAAG,KAAK,OAAO,CAAC,IAAR,GAAe,CAA3B,EAA8B;AAC5B,UAAM,KAAK,CACP,8DACA,mBAAmB,OAAO,CAAC,IAAI,gBAAgB,GAAG,EAF3C,CAAX;AAGD;;AAED,QAAM,YAAY,GAAa,EAA/B;AACA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB;AAEA,SAAO,MAAM,CAAC,aAAP,CACH,CAAC,OAAD,EAAU,IAAV,KAAkB;AAChB,UAAM,CAAC,GAAG,OAAO,CAAC,OAAR,CAAgB,OAAhB,EAAyB,GAAzB,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GALE,EAMH;AAAC,IAAA,MAAM,EAAE;AAAT,GANG,EAOH,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACzB,UAAM,CAAC,CAAD,IAAM,KAAZ;AACA,UAAM,QAAQ,GAAG,EAAE,CAAC,GAAH,CAAO,CAAP,CAAjB;AACA,UAAM,QAAQ,GAAG,IAAjB;AAEA,WAAO;AACL,MAAA,MAAM,EAAE,MAAM,QAAQ,CAAC,GAAT,CAAa,QAAQ,CAAC,GAAT,CAAa,CAAC,GAAD,CAAb,EAAoB,QAApB,EAA8B,GAA9B,CAAkC,CAAlC,CAAb;AADT,KAAP;AAGD,GAfE,EAgBH,SAhBG,EAgBQ;AAAC,IAAA;AAAD,GAhBR,EAgBe,YAhBf,EAgB6B,aAhB7B,CAAP;AAiBD;AAED;;;;;;;;;;;;;;;;;;;;AAmBA;;;AACA,SAAS,WAAT,CAAuC,MAAvC,EAA6D,IAAI,GAAG,CAAC,CAArE,EAAsE;AACpE,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,YAAnB,CAA/B;;AAEA,MAAI,IAAI,KAAK,CAAC,CAAd,EAAiB;AACf,IAAA,IAAI,GAAG,OAAO,CAAC,IAAR,GAAe,CAAtB;AACD;;AACD,MAAI,IAAI,KAAK,OAAO,CAAC,IAAR,GAAe,CAA5B,EAA+B;AAC7B,UAAM,KAAK,CACP,kEACA,mBAAmB,OAAO,CAAC,IAAI,iBAAiB,IAAI,EAF7C,CAAX;AAGD;;AAED,QAAM,QAAQ,GAAG,UAAU,CAAC,CAAC,MAAD,EAAiB,IAAjB,KAAuC;AACjE,UAAM,QAAQ,GAAG,IAAjB;AACA,UAAM,IAAI,GAAG,MAAM,CAAC,GAAP,CAAW,IAAX,EAAiB,IAAjB,CAAb;AACA,UAAM,OAAO,GAAG,MAAM,CAAC,GAAP,CAAW,IAAX,CAAhB;AACA,UAAM,KAAK,GACP,OAAO,CAAC,OAAR,GAAkB,GAAlB,CAAsB,OAAO,CAAC,GAAR,GAAc,GAAd,CAAkB,IAAlB,EAAwB,QAAxB,EAAkC,GAAlC,EAAtB,CADJ;AAEA,IAAA,IAAI,CAAC,CAAC,KAAD,CAAD,CAAJ;;AACA,UAAM,QAAQ,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AAC1C,YAAM,CAAC,KAAD,IAAU,KAAhB;AACA,YAAM,OAAO,GAAG,KAAK,CAAC,GAAN,EAAhB;AACA,aAAO,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,CAAO,IAAP,EAAa,QAAb,EAAuB,GAAvB,CAA2B,OAA3B,CAAP,CAAP;AACD,KAJD;;AAMA,WAAO;AAAC,MAAA,KAAD;AAAQ,MAAA;AAAR,KAAP;AACD,GAd0B,CAA3B;AAgBA,SAAO,QAAQ,CAAC,OAAD,CAAf;AACD;;AAED,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,UAAU,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAArB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the softmax normalized vector given the logits.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.softmax().print();  // or tf.softmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\nfunction softmax_(logits, dim = -1) {\n    const $logits = convertToTensor(logits, 'logits', 'softmax', 'float32');\n    if (dim === -1) {\n        dim = $logits.rank - 1;\n    }\n    if (dim !== $logits.rank - 1) {\n        throw Error('Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${$logits.rank} and dim was ${dim}`);\n    }\n    const inputsToSave = [];\n    const outputsToSave = [true];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.softmax($logits, dim);\n        save([y]);\n        return y;\n    }, { logits: $logits }, (dy, saved) => {\n        const [y] = saved;\n        const dyTimesY = dy.mul(y);\n        const keepDims = true;\n        return {\n            logits: () => dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y))\n        };\n    }, 'Softmax', { dim }, inputsToSave, outputsToSave);\n}\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\nfunction logSoftmax_(logits, axis = -1) {\n    const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n    if (axis === -1) {\n        axis = $logits.rank - 1;\n    }\n    if (axis !== $logits.rank - 1) {\n        throw Error('Log Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${$logits.rank} and axis was ${axis}`);\n    }\n    const customOp = customGrad((logits, save) => {\n        const keepDims = true;\n        const xMax = logits.max(axis, true);\n        const shifted = logits.sub(xMax);\n        const value = shifted.toFloat().sub(shifted.exp().sum(axis, keepDims).log());\n        save([value]);\n        const gradFunc = (dy, saved) => {\n            const [value] = saved;\n            const softmax = value.exp();\n            return dy.sub(dy.sum(axis, keepDims).mul(softmax));\n        };\n        return { value, gradFunc };\n    });\n    return customOp($logits);\n}\nexport const softmax = op({ softmax_ });\nexport const logSoftmax = op({ logSoftmax_ });\n//# sourceMappingURL=softmax.js.map"]},"metadata":{},"sourceType":"module"}