{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { scalar, zerosLike } from './tensor_ops';\n/**\n * Computes `-1 * x` element-wise.\n *\n * ```js\n * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);\n *\n * x.neg().print();  // or tf.neg(x)\n * ```\n *\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\nfunction neg_(x) {\n  const $x = convertToTensor(x, 'x', 'neg');\n\n  const grad = dy => {\n    return {\n      x: () => dy.neg()\n    };\n  };\n\n  const attrs = {};\n  const inputsToSave = [$x];\n  return ENGINE.runKernelFunc(backend => backend.neg($x), {\n    x: $x\n  }, grad, 'Neg', attrs, inputsToSave);\n}\n/**\n * Computes ceiling of input `tf.Tensor` element-wise: `ceil(x)`\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.ceil().print();  // or tf.ceil(x)\n * ```\n * @param x The input Tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction ceil_(x) {\n  const $x = convertToTensor(x, 'x', 'ceil'); // TODO(manrajgrover): Return null for gradients when backprop supports it.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.ceil($x), {\n    $x\n  }, grad);\n}\n/**\n * Computes floor of input `tf.Tensor` element-wise: `floor(x)`.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.floor().print();  // or tf.floor(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction floor_(x) {\n  const $x = convertToTensor(x, 'x', 'floor'); // TODO(nsthorat): Let gradients be null for cases where we want to stop\n  // backpropgation.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.floor($x), {\n    $x\n  }, grad);\n}\n/**\n * Returns an element-wise indication of the sign of a number.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3, NaN, 0]);\n *\n * x.sign().print();  // or tf.sign(x)\n * ```\n * @param x The input Tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction sign_(x) {\n  const $x = convertToTensor(x, 'x', 'sign');\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.sign($x), {\n    $x\n  }, grad);\n}\n/**\n * RReturns which elements of x are NaN.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isNaN().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction isNaN_(x) {\n  const $x = convertToTensor(x, 'x', 'isNaN'); // TODO(nsthorat): Let gradients be null for cases where we want to stop\n  // backpropgation.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.isNaN($x), {\n    $x\n  }, grad);\n}\n/**\n * Returns which elements of x are Infinity or -Infinity.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isInf().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction isInf_(x) {\n  const $x = convertToTensor(x, 'x', 'isInf'); // TODO(nsthorat): Let gradients be null for cases where we want to stop\n  // backpropgation.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.isInf($x), {\n    $x\n  }, grad);\n}\n/**\n * Returns which elements of x are finite.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isFinite().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction isFinite_(x) {\n  const $x = convertToTensor(x, 'x', 'isFinite'); // TODO(nsthorat): Let gradients be null for cases where we want to stop\n  // backpropgation.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.isFinite($x), {\n    $x\n  }, grad);\n}\n/**\n * Computes round of input `tf.Tensor` element-wise: `round(x)`.\n * It implements banker's rounding.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.round().print();  // or tf.round(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction round_(x) {\n  const $x = convertToTensor(x, 'x', 'round'); // TODO(nsthorat): Let gradients be null for cases where we want to stop\n  // backpropgation.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.round($x), {\n    $x\n  }, grad);\n}\n/**\n * Computes exponential of the input `tf.Tensor` element-wise. `e ^ x`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, -3]);\n *\n * x.exp().print();  // or tf.exp(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction exp_(x) {\n  const $x = convertToTensor(x, 'x', 'exp');\n\n  const bck = (dy, saved) => {\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    return {\n      x: () => dy.mul(saved[0])\n    };\n  };\n\n  const attrs = {};\n  const inputsToSave = [];\n  const outputsToSave = [true];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.exp($x);\n    save([y]);\n    return y;\n  }, {\n    x: $x\n  }, bck, 'Exp', attrs, inputsToSave, outputsToSave);\n}\n/**\n * Computes exponential of the input `tf.Tensor` minus one element-wise.\n * `e ^ x - 1`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, -3]);\n *\n * x.expm1().print();  // or tf.expm1(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction expm1_(x) {\n  const $x = convertToTensor(x, 'x', 'expm1');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.mul($x.exp())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.expm1($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E]);\n *\n * x.log().print();  // or tf.log(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction log_(x) {\n  const $x = convertToTensor(x, 'x', 'log');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => dy.div($x.toFloat())\n    };\n  };\n\n  const attrs = {};\n  const inputsToSave = [$x];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.log($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Log', attrs, inputsToSave);\n}\n/**\n * Computes natural logarithm of the input `tf.Tensor` plus one\n * element-wise: `ln(1 + x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E - 1]);\n *\n * x.log1p().print();  // or tf.log1p(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction log1p_(x) {\n  const $x = convertToTensor(x, 'x', 'log1p');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.div($x.add(1))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.log1p($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes square root of the input `tf.Tensor` element-wise: `y = sqrt(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 4, -1]);\n *\n * x.sqrt().print();  // or tf.sqrt(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction sqrt_(x) {\n  const $x = convertToTensor(x, 'x', 'sqrt');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => dy.div($x.toFloat().sqrt().mul(2))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.sqrt($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Sqrt', {});\n}\n/**\n * Computes reciprocal of square root of the input `tf.Tensor` element-wise:\n * `y = 1 / sqrt(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 4, -1]);\n *\n * x.rsqrt().print();  // or tf.rsqrt(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction rsqrt_(x) {\n  const $x = convertToTensor(x, 'x', 'rsqrt');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => dy.div($x.pow(1.5).mul(2)).neg()\n    };\n  };\n\n  const inputsToSave = [$x];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.rsqrt($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Rsqrt', {}\n  /* attrs */\n  , inputsToSave);\n}\n/**\n * Computes reciprocal of x element-wise: `1 / x`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, 2]);\n *\n * x.reciprocal().print();  // or tf.reciprocal(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction reciprocal_(x) {\n  const $x = convertToTensor(x, 'x', 'reciprocal');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.div($x.square().neg())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.reciprocal($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes absolute value element-wise: `abs(x)`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.abs().print();  // or tf.abs(x)\n * ```\n * @param x The input `tf.Tensor`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction abs_(x) {\n  const $x = convertToTensor(x, 'x', 'abs');\n\n  if ($x.dtype === 'complex64') {\n    return ENGINE.runKernelFunc(backend => backend.complexAbs($x), {\n      $x\n    });\n  }\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => dy.mul($x.toFloat().step(-1))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.abs($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Abs');\n}\n/**\n * Clips values element-wise. `max(min(x, clipValueMax), clipValueMin)`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.clipByValue(-2, 3).print();  // or tf.clipByValue(x, -2, 3)\n * ```\n * @param x The input tensor.\n * @param clipValueMin Lower-bound of range to be clipped to.\n * @param clipValueMax Upper-bound of range to be clipped to.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction clipByValue_(x, clipValueMin, clipValueMax) {\n  const $x = convertToTensor(x, 'x', 'clipByValue');\n  util.assert(clipValueMin <= clipValueMax, () => `Error in clip: min (${clipValueMin}) must be ` + `less than or equal to max (${clipValueMax}).`);\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => dy.where($x.greaterEqual(clipValueMin).logicalAnd($x.lessEqual(clipValueMax)), zerosLike(dy))\n    };\n  };\n\n  const inputsToSave = [$x];\n  const attr = {\n    min: clipValueMin,\n    max: clipValueMax\n  };\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.clip($x, clipValueMin, clipValueMax);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'ClipByValue', attr, inputsToSave);\n}\n/**\n * Computes sigmoid element-wise, `1 / (1 + exp(-x))`\n *\n * ```js\n * const x = tf.tensor1d([0, -1, 2, -3]);\n *\n * x.sigmoid().print();  // or tf.sigmoid(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction sigmoid_(x) {\n  const $x = convertToTensor(x, 'x', 'sigmoid');\n\n  const grad = (dy, saved) => {\n    const [y] = saved;\n    return {\n      x: () => dy.mul(y.mul(scalar(1).sub(y)))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.sigmoid($x);\n    save([y]);\n    return y;\n  }, {\n    x: $x\n  }, grad, 'Sigmoid');\n}\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction logSigmoid_(x) {\n  const $x = convertToTensor(x, 'x', 'logSigmoid');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.mul($x.neg().sigmoid())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.softplus($x.neg()).neg();\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes softplus of the input `tf.Tensor` element-wise: `log(exp(x) + 1)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.softplus().print();  // or tf.softplus(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction softplus_(x) {\n  const $x = convertToTensor(x, 'x', 'softplus');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.mul($x.sigmoid())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.softplus($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes sin of the input Tensor element-wise: `sin(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n *\n * x.sin().print();  // or tf.sin(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction sin_(x) {\n  const $x = convertToTensor(x, 'x', 'sin');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => $x.toFloat().cos().mul(dy)\n    };\n  };\n\n  const inputsToSave = [$x];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.sin($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Sin', {}\n  /* attrs */\n  , inputsToSave);\n}\n/**\n * Computes cos of the input `tf.Tensor` element-wise: `cos(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n *\n * x.cos().print();  // or tf.cos(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction cos_(x) {\n  const $x = convertToTensor(x, 'x', 'cos');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => $x.toFloat().sin().neg().mul(dy)\n    };\n  };\n\n  const inputsToSave = [$x];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.cos($x);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'Cos', {}\n  /* attrs */\n  , inputsToSave);\n}\n/**\n * Computes tan of the input `tf.Tensor` element-wise, `tan(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n *\n * x.tan().print();  // or tf.tan(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction tan_(x) {\n  const $x = convertToTensor(x, 'x', 'tan');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.div($x.cos().square())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.tan($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes asin of the input `tf.Tensor` element-wise: `asin(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.asin().print();  // or tf.asin(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction asin_(x) {\n  const $x = convertToTensor(x, 'x', 'asin');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      $x: () => dy.div(scalar(1).sub($x.toFloat().square()).sqrt())\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.asin($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes acos of the input `tf.Tensor` element-wise: `acos(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.acos().print();  // or tf.acos(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction acos_(x) {\n  const $x = convertToTensor(x, 'x', 'acos');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => {\n        const a = $x.toFloat().square();\n        const b = scalar(1).sub(a).sqrt(); // tslint:disable-next-line: no-unnecessary-type-assertion\n\n        return dy.div(b).neg();\n      }\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.acos($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes atan of the input `tf.Tensor` element-wise: `atan(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.atan().print();  // or tf.atan(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction atan_(x) {\n  const $x = convertToTensor(x, 'x', 'atan');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.div($x.toFloat().square().add(1))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.atan($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes hyperbolic sin of the input `tf.Tensor` element-wise: `sinh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.sinh().print();  // or tf.sinh(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction sinh_(x) {\n  const $x = convertToTensor(x, 'x', 'sinh');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved; // tslint:disable-next-line: no-unnecessary-type-assertion\n\n    return {\n      $x: () => $x.toFloat().cosh().mul(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.sinh($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes hyperbolic cos of the input `tf.Tensor` element-wise: `cosh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.cosh().print();  // or tf.cosh(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction cosh_(x) {\n  const $x = convertToTensor(x, 'x', 'cosh');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved; // tslint:disable-next-line: no-unnecessary-type-assertion\n\n    return {\n      $x: () => $x.toFloat().sinh().mul(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.cosh($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes hyperbolic tangent of the input `tf.Tensor` element-wise: `tanh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, 70]);\n *\n * x.tanh().print();  // or tf.tanh(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction tanh_(x) {\n  const $x = convertToTensor(x, 'x', 'tanh');\n\n  const grad = (dy, saved) => {\n    const [y] = saved; // tslint:disable-next-line: no-unnecessary-type-assertion\n\n    return {\n      x: () => scalar(1).sub(y.square()).mul(dy)\n    };\n  };\n\n  const outputsToSave = [true];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.tanh($x);\n    save([y]);\n    return y;\n  }, {\n    x: $x\n  }, grad, 'Tanh', {}\n  /* attrs */\n  , null\n  /* inputsToSave */\n  , outputsToSave);\n}\n/**\n * Computes inverse hyperbolic sin of the input `tf.Tensor` element-wise:\n * `asinh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.asinh().print();  // or tf.asinh(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction asinh_(x) {\n  const $x = convertToTensor(x, 'x', 'asinh');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => {\n        const a = scalar(1).add($x.toFloat().square()).sqrt(); // tslint:disable-next-line: no-unnecessary-type-assertion\n\n        return dy.div(a);\n      }\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.asinh($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes the inverse hyperbolic cos of the input `tf.Tensor` element-wise:\n * `acosh(x)`\n *\n * ```js\n * const x = tf.tensor1d([10, 1, 3, 5.7]);\n *\n * x.acosh().print();  // or tf.acosh(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction acosh_(x) {\n  const $x = convertToTensor(x, 'x', 'acosh');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => {\n        const a = $x.toFloat().square().sub(1).sqrt(); // tslint:disable-next-line: no-unnecessary-type-assertion\n\n        return dy.div(a);\n      }\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.acosh($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes inverse hyperbolic tan of the input `tf.Tensor` element-wise:\n * `atanh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, .1, -.1, .7]);\n *\n * x.atanh().print();  // or tf.atanh(x)\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction atanh_(x) {\n  const $x = convertToTensor(x, 'x', 'atanh');\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.div(scalar(1).sub($x.toFloat().square()))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.atanh($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes gause error function of the input `tf.Tensor` element-wise:\n * `erf(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, .1, -.1, .7]);\n *\n * x.erf().print(); // or tf.erf(x);\n * ```\n * @param x The input tensor.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction erf_(x) {\n  let $x = convertToTensor(x, 'x', 'erf');\n  util.assert($x.dtype === 'int32' || $x.dtype === 'float32', () => 'Input dtype must be `int32` or `float32`.');\n\n  if ($x.dtype === 'int32') {\n    $x = $x.toFloat();\n  }\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => dy.mul($x.square().neg().exp().mul(2 / Math.sqrt(Math.PI)))\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.erf($x);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Computes step of the input `tf.Tensor` element-wise: `x > 0 ? 1 : alpha * x`\n *\n * ```js\n * const x = tf.tensor1d([0, 2, -1, -3]);\n *\n * x.step(.5).print();  // or tf.step(x, .5)\n * ```\n * @param x The input tensor.\n * @param alpha The gradient when input is negative.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\n\n\nfunction step_(x, alpha = 0.0) {\n  const $x = convertToTensor(x, 'x', 'step'); // TODO(manrajgrover): Return null for gradients when backprop supports\n  // it.\n\n  const grad = dy => {\n    return {\n      $x: () => zerosLike(dy)\n    };\n  };\n\n  return ENGINE.runKernelFunc(backend => backend.step($x, alpha), {\n    $x\n  }, grad);\n}\n\nexport const abs = op({\n  abs_\n});\nexport const acos = op({\n  acos_\n});\nexport const acosh = op({\n  acosh_\n});\nexport const asin = op({\n  asin_\n});\nexport const asinh = op({\n  asinh_\n});\nexport const atan = op({\n  atan_\n});\nexport const atanh = op({\n  atanh_\n});\nexport const ceil = op({\n  ceil_\n});\nexport const clipByValue = op({\n  clipByValue_\n});\nexport const cos = op({\n  cos_\n});\nexport const cosh = op({\n  cosh_\n});\nexport const erf = op({\n  erf_\n});\nexport const exp = op({\n  exp_\n});\nexport const expm1 = op({\n  expm1_\n});\nexport const floor = op({\n  floor_\n});\nexport const log = op({\n  log_\n});\nexport const log1p = op({\n  log1p_\n});\nexport const logSigmoid = op({\n  logSigmoid_\n});\nexport const neg = op({\n  neg_\n});\nexport const reciprocal = op({\n  reciprocal_\n});\nexport const round = op({\n  round_\n});\nexport const rsqrt = op({\n  rsqrt_\n});\nexport const sigmoid = op({\n  sigmoid_\n});\nexport const sign = op({\n  sign_\n});\nexport const isNaN = op({\n  isNaN_\n});\nexport const isInf = op({\n  isInf_\n});\nexport const isFinite = op({\n  isFinite_\n});\nexport const sin = op({\n  sin_\n});\nexport const sinh = op({\n  sinh_\n});\nexport const softplus = op({\n  softplus_\n});\nexport const sqrt = op({\n  sqrt_\n});\nexport const step = op({\n  step_\n});\nexport const tan = op({\n  tan_\n});\nexport const tanh = op({\n  tanh_\n});","map":{"version":3,"sources":["../../src/ops/unary_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AAEA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,SAAtB;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,MAAR,EAAgB,SAAhB,QAAgC,cAAhC;AAEA;;;;;;;;;;;;AAWA;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH;AAAV,KAAP;AACD,GAFD;;AAIA,QAAM,KAAK,GAAG,EAAd;AACA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,SAAO,MAAM,CAAC,aAAP,CACH,OAAO,IAAI,OAAO,CAAC,GAAR,CAAY,EAAZ,CADR,EACyB;AAAC,IAAA,CAAC,EAAE;AAAJ,GADzB,EACkC,IADlC,EACwC,KADxC,EAC+C,KAD/C,EACsD,YADtD,CAAP;AAED;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B,CAD8C,CAG9C;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,IAAR,CAAa,EAAb,CAAhC,EAAkD;AAAC,IAAA;AAAD,GAAlD,EAAwD,IAAxD,CAAP;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B,CAD+C,CAG/C;AACA;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,KAAR,CAAc,EAAd,CAAhC,EAAmD;AAAC,IAAA;AAAD,GAAnD,EAAyD,IAAzD,CAAP;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,IAAR,CAAa,EAAb,CAAhC,EAAkD;AAAC,IAAA;AAAD,GAAlD,EAAwD,IAAxD,CAAP;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B,CAD+C,CAG/C;AACA;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,KAAR,CAAc,EAAd,CAAhC,EAAmD;AAAC,IAAA;AAAD,GAAnD,EAAyD,IAAzD,CAAP;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B,CAD+C,CAG/C;AACA;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,KAAR,CAAc,EAAd,CAAhC,EAAmD;AAAC,IAAA;AAAD,GAAnD,EAAyD,IAAzD,CAAP;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,SAAT,CAAqC,CAArC,EAAoD;AAClD,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,UAAT,CAA1B,CADkD,CAGlD;AACA;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,QAAR,CAAiB,EAAjB,CAAhC,EAAsD;AAAC,IAAA;AAAD,GAAtD,EAA4D,IAA5D,CAAP;AACD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B,CAD+C,CAG/C;AACA;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,KAAR,CAAc,EAAd,CAAhC,EAAmD;AAAC,IAAA;AAAD,GAAnD,EAAyD,IAAzD,CAAP;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,GAAG,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACrC;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,KAAK,CAAC,CAAD,CAAZ;AAAV,KAAP;AACD,GAHD;;AAIA,QAAM,KAAK,GAAG,EAAd;AACA,QAAM,YAAY,GAAa,EAA/B;AACA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,CAAC,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,GAJL,EAIU,KAJV,EAIiB,KAJjB,EAIwB,YAJxB,EAIsC,aAJtC,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,EAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,EAAP;AAAV,KAAP;AACD,GAHD;;AAKA,QAAM,KAAK,GAAG,EAAd;AACA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AAEA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,KAJX,EAIkB,KAJlB,EAIyB,YAJzB,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,CAAO,CAAP,CAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,GAAa,IAAb,GAAoB,GAApB,CAAwB,CAAxB,CAAP;AAAV,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,MAJX,EAImB,EAJnB,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,CAAO,GAAP,EAAY,GAAZ,CAAgB,CAAhB,CAAP,EAA2B,GAA3B;AAAV,KAAP;AACD,GAHD;;AAIA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,OAJX,EAIoB;AAAG;AAJvB,IAIoC,YAJpC,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,WAAT,CAAuC,CAAvC,EAAsD;AACpD,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,YAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,MAAH,GAAY,GAAZ,EAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,UAAR,CAAmB,EAAnB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,WAAjB,EAA8B;AAC5B,WAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,UAAR,CAAmB,EAAnB,CAAhC,EAAwD;AAAC,MAAA;AAAD,KAAxD,CAAP;AACD;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,GAAa,IAAb,CAAkB,CAAC,CAAnB,CAAP;AAAV,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,KAJX,CAAP;AAKD;AAED;;;;;;;;;;;;;AAYA;;;AACA,SAAS,YAAT,CACI,CADJ,EACqB,YADrB,EAC2C,YAD3C,EAC+D;AAC7D,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,aAAT,CAA1B;AACA,EAAA,IAAI,CAAC,MAAL,CACK,YAAY,IAAI,YADrB,EAEI,MAAM,uBAAuB,YAAY,YAAnC,GACF,8BAA8B,YAAY,IAHlD;;AAKA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,KAAH,CACI,EAAE,CAAC,YAAH,CAAgB,YAAhB,EACK,UADL,CACgB,EAAE,CAAC,SAAH,CAAa,YAAb,CADhB,CADJ,EAGI,SAAS,CAAC,EAAD,CAHb;AADJ,KAAP;AAMD,GARD;;AASA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,QAAM,IAAI,GAAG;AAAC,IAAA,GAAG,EAAE,YAAN;AAAoB,IAAA,GAAG,EAAE;AAAzB,GAAb;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,EAAiB,YAAjB,EAA+B,YAA/B,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,aAJX,EAI0B,IAJ1B,EAIgC,YAJhC,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,QAAT,CAAoC,CAApC,EAAmD;AACjD,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,CAAD,IAAM,KAAZ;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,CAAC,CAAC,GAAF,CAAM,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,CAAd,CAAN,CAAP;AAAV,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,CAAC,GAAG,OAAO,CAAC,OAAR,CAAgB,EAAhB,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,SAJX,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,WAAT,CAAuC,CAAvC,EAAsD;AACpD,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,YAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,GAAS,OAAT,EAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,QAAR,CAAiB,EAAE,CAAC,GAAH,EAAjB,EAA2B,GAA3B,EAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,SAAT,CAAqC,CAArC,EAAoD;AAClD,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,UAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,EAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,QAAR,CAAiB,EAAjB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,OAAH,GAAa,GAAb,GAAmB,GAAnB,CAAuB,EAAvB;AAAV,KAAP;AACD,GAHD;;AAIA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,KAJX,EAIkB;AAAG;AAJrB,IAIkC,YAJlC,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,OAAH,GAAa,GAAb,GAAmB,GAAnB,GAAyB,GAAzB,CAA6B,EAA7B;AAAV,KAAP;AACD,GAHD;;AAIA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,KAJX,EAIkB;AAAG;AAJrB,IAIkC,YAJlC,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,GAAH,GAAS,MAAT,EAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL;AACA,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,EAAE,CAAC,OAAH,GAAa,MAAb,EAAd,EAAqC,IAArC,EAAP;AAFL,KAAP;AAID,GAND;;AAOA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL,MAAA,EAAE,EAAE,MAAK;AACP,cAAM,CAAC,GAAG,EAAE,CAAC,OAAH,GAAa,MAAb,EAAV;AACA,cAAM,CAAC,GAAG,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,CAAd,EAAiB,IAAjB,EAAV,CAFO,CAGP;;AACA,eAAQ,EAAE,CAAC,GAAH,CAAO,CAAP,EAAgB,GAAhB,EAAR;AACD;AANI,KAAP;AASD,GAXD;;AAYA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,OAAH,GAAa,MAAb,GAAsB,GAAtB,CAA0B,CAA1B,CAAP;AAAX,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb,CADsC,CAEtC;;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,OAAH,GAAa,IAAb,GAAoB,GAApB,CAAwB,EAAxB;AAAX,KAAP;AACD,GAJD;;AAKA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb,CADsC,CAEtC;;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,OAAH,GAAa,IAAb,GAAoB,GAApB,CAAwB,EAAxB;AAAX,KAAP;AACD,GAJD;;AAKA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAgD;AAC9C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,CAAD,IAAM,KAAZ,CADsC,CAEtC;;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,CAAC,CAAC,MAAF,EAAd,EAA0B,GAA1B,CAA8B,EAA9B;AAAV,KAAP;AACD,GAJD;;AAKA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB;AACA,SAAO,MAAM,CAAC,aAAP,CACH,CAAC,OAAD,EAAU,IAAV,KAAkB;AAChB,UAAM,CAAC,GAAG,OAAO,CAAC,IAAR,CAAa,EAAb,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GALE,EAMH;AAAC,IAAA,CAAC,EAAE;AAAJ,GANG,EAMM,IANN,EAMY,MANZ,EAMoB;AAAG;AANvB,IAMoC;AAAK;AANzC,IAOH,aAPG,CAAP;AAQD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL,MAAA,EAAE,EAAE,MAAK;AACP,cAAM,CAAC,GAAG,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,EAAE,CAAC,OAAH,GAAa,MAAb,EAAd,EAAqC,IAArC,EAAV,CADO,CAEP;;AACA,eAAO,EAAE,CAAC,GAAH,CAAO,CAAP,CAAP;AACD;AALI,KAAP;AAOD,GATD;;AAUA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL,MAAA,EAAE,EAAE,MAAK;AACP,cAAM,CAAC,GAAG,EAAE,CAAC,OAAH,GAAa,MAAb,GAAsB,GAAtB,CAA0B,CAA1B,EAA6B,IAA7B,EAAV,CADO,CAEP;;AACA,eAAO,EAAE,CAAC,GAAH,CAAO,CAAP,CAAP;AACD;AALI,KAAP;AAOD,GATD;;AAUA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,MAAT,CAAkC,CAAlC,EAAiD;AAC/C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,OAAT,CAA1B;;AAEA,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,EAAE,CAAC,OAAH,GAAa,MAAb,EAAd,CAAP;AAAX,KAAP;AAED,GAJD;;AAKA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,KAAR,CAAc,EAAd,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,IAAT,CAAgC,CAAhC,EAA+C;AAC7C,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;AACA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,KAAH,KAAa,OAAb,IAAwB,EAAE,CAAC,KAAH,KAAa,SADzC,EAEI,MAAM,2CAFV;;AAIA,MAAI,EAAE,CAAC,KAAH,KAAa,OAAjB,EAA0B;AACxB,IAAA,EAAE,GAAG,EAAE,CAAC,OAAH,EAAL;AACD;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AACL,MAAA,EAAE,EAAE,MAAM,EAAE,CAAC,GAAH,CAAO,EAAE,CAAC,MAAH,GAAY,GAAZ,GAAkB,GAAlB,GAAwB,GAAxB,CAA4B,IAAI,IAAI,CAAC,IAAL,CAAU,IAAI,CAAC,EAAf,CAAhC,CAAP;AADL,KAAP;AAGD,GALD;;AAMA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;AAWA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAkD,KAAK,GAAG,GAA1D,EAA6D;AAC3D,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B,CAD2D,CAG3D;AACA;;AACA,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAFD;;AAGA,SAAO,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,IAAR,CAAa,EAAb,EAAiB,KAAjB,CAAhC,EAAyD;AAAC,IAAA;AAAD,GAAzD,EAA+D,IAA/D,CAAP;AACD;;AAED,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,WAAW,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAtB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,UAAU,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAArB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,UAAU,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAArB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,QAAQ,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAnB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,QAAQ,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAnB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { scalar, zerosLike } from './tensor_ops';\n/**\n * Computes `-1 * x` element-wise.\n *\n * ```js\n * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);\n *\n * x.neg().print();  // or tf.neg(x)\n * ```\n *\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction neg_(x) {\n    const $x = convertToTensor(x, 'x', 'neg');\n    const grad = (dy) => {\n        return { x: () => dy.neg() };\n    };\n    const attrs = {};\n    const inputsToSave = [$x];\n    return ENGINE.runKernelFunc(backend => backend.neg($x), { x: $x }, grad, 'Neg', attrs, inputsToSave);\n}\n/**\n * Computes ceiling of input `tf.Tensor` element-wise: `ceil(x)`\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.ceil().print();  // or tf.ceil(x)\n * ```\n * @param x The input Tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction ceil_(x) {\n    const $x = convertToTensor(x, 'x', 'ceil');\n    // TODO(manrajgrover): Return null for gradients when backprop supports it.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.ceil($x), { $x }, grad);\n}\n/**\n * Computes floor of input `tf.Tensor` element-wise: `floor(x)`.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.floor().print();  // or tf.floor(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction floor_(x) {\n    const $x = convertToTensor(x, 'x', 'floor');\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.floor($x), { $x }, grad);\n}\n/**\n * Returns an element-wise indication of the sign of a number.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3, NaN, 0]);\n *\n * x.sign().print();  // or tf.sign(x)\n * ```\n * @param x The input Tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction sign_(x) {\n    const $x = convertToTensor(x, 'x', 'sign');\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.sign($x), { $x }, grad);\n}\n/**\n * RReturns which elements of x are NaN.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isNaN().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction isNaN_(x) {\n    const $x = convertToTensor(x, 'x', 'isNaN');\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.isNaN($x), { $x }, grad);\n}\n/**\n * Returns which elements of x are Infinity or -Infinity.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isInf().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction isInf_(x) {\n    const $x = convertToTensor(x, 'x', 'isInf');\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.isInf($x), { $x }, grad);\n}\n/**\n * Returns which elements of x are finite.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isFinite().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction isFinite_(x) {\n    const $x = convertToTensor(x, 'x', 'isFinite');\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.isFinite($x), { $x }, grad);\n}\n/**\n * Computes round of input `tf.Tensor` element-wise: `round(x)`.\n * It implements banker's rounding.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.round().print();  // or tf.round(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction round_(x) {\n    const $x = convertToTensor(x, 'x', 'round');\n    // TODO(nsthorat): Let gradients be null for cases where we want to stop\n    // backpropgation.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.round($x), { $x }, grad);\n}\n/**\n * Computes exponential of the input `tf.Tensor` element-wise. `e ^ x`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, -3]);\n *\n * x.exp().print();  // or tf.exp(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction exp_(x) {\n    const $x = convertToTensor(x, 'x', 'exp');\n    const bck = (dy, saved) => {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        return { x: () => dy.mul(saved[0]) };\n    };\n    const attrs = {};\n    const inputsToSave = [];\n    const outputsToSave = [true];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.exp($x);\n        save([y]);\n        return y;\n    }, { x: $x }, bck, 'Exp', attrs, inputsToSave, outputsToSave);\n}\n/**\n * Computes exponential of the input `tf.Tensor` minus one element-wise.\n * `e ^ x - 1`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, -3]);\n *\n * x.expm1().print();  // or tf.expm1(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction expm1_(x) {\n    const $x = convertToTensor(x, 'x', 'expm1');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.mul($x.exp()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.expm1($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E]);\n *\n * x.log().print();  // or tf.log(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction log_(x) {\n    const $x = convertToTensor(x, 'x', 'log');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => dy.div($x.toFloat()) };\n    };\n    const attrs = {};\n    const inputsToSave = [$x];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.log($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Log', attrs, inputsToSave);\n}\n/**\n * Computes natural logarithm of the input `tf.Tensor` plus one\n * element-wise: `ln(1 + x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E - 1]);\n *\n * x.log1p().print();  // or tf.log1p(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction log1p_(x) {\n    const $x = convertToTensor(x, 'x', 'log1p');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.div($x.add(1)) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.log1p($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes square root of the input `tf.Tensor` element-wise: `y = sqrt(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 4, -1]);\n *\n * x.sqrt().print();  // or tf.sqrt(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction sqrt_(x) {\n    const $x = convertToTensor(x, 'x', 'sqrt');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => dy.div($x.toFloat().sqrt().mul(2)) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.sqrt($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Sqrt', {});\n}\n/**\n * Computes reciprocal of square root of the input `tf.Tensor` element-wise:\n * `y = 1 / sqrt(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 4, -1]);\n *\n * x.rsqrt().print();  // or tf.rsqrt(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction rsqrt_(x) {\n    const $x = convertToTensor(x, 'x', 'rsqrt');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => dy.div($x.pow(1.5).mul(2)).neg() };\n    };\n    const inputsToSave = [$x];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.rsqrt($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Rsqrt', {} /* attrs */, inputsToSave);\n}\n/**\n * Computes reciprocal of x element-wise: `1 / x`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, 2]);\n *\n * x.reciprocal().print();  // or tf.reciprocal(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction reciprocal_(x) {\n    const $x = convertToTensor(x, 'x', 'reciprocal');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.div($x.square().neg()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.reciprocal($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes absolute value element-wise: `abs(x)`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.abs().print();  // or tf.abs(x)\n * ```\n * @param x The input `tf.Tensor`.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction abs_(x) {\n    const $x = convertToTensor(x, 'x', 'abs');\n    if ($x.dtype === 'complex64') {\n        return ENGINE.runKernelFunc(backend => backend.complexAbs($x), { $x });\n    }\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => dy.mul($x.toFloat().step(-1)) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.abs($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Abs');\n}\n/**\n * Clips values element-wise. `max(min(x, clipValueMax), clipValueMin)`\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.clipByValue(-2, 3).print();  // or tf.clipByValue(x, -2, 3)\n * ```\n * @param x The input tensor.\n * @param clipValueMin Lower-bound of range to be clipped to.\n * @param clipValueMax Upper-bound of range to be clipped to.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction clipByValue_(x, clipValueMin, clipValueMax) {\n    const $x = convertToTensor(x, 'x', 'clipByValue');\n    util.assert((clipValueMin <= clipValueMax), () => `Error in clip: min (${clipValueMin}) must be ` +\n        `less than or equal to max (${clipValueMax}).`);\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            x: () => dy.where($x.greaterEqual(clipValueMin)\n                .logicalAnd($x.lessEqual(clipValueMax)), zerosLike(dy)),\n        };\n    };\n    const inputsToSave = [$x];\n    const attr = { min: clipValueMin, max: clipValueMax };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.clip($x, clipValueMin, clipValueMax);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'ClipByValue', attr, inputsToSave);\n}\n/**\n * Computes sigmoid element-wise, `1 / (1 + exp(-x))`\n *\n * ```js\n * const x = tf.tensor1d([0, -1, 2, -3]);\n *\n * x.sigmoid().print();  // or tf.sigmoid(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction sigmoid_(x) {\n    const $x = convertToTensor(x, 'x', 'sigmoid');\n    const grad = (dy, saved) => {\n        const [y] = saved;\n        return { x: () => dy.mul(y.mul(scalar(1).sub(y))) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.sigmoid($x);\n        save([y]);\n        return y;\n    }, { x: $x }, grad, 'Sigmoid');\n}\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction logSigmoid_(x) {\n    const $x = convertToTensor(x, 'x', 'logSigmoid');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.mul($x.neg().sigmoid()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.softplus($x.neg()).neg();\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes softplus of the input `tf.Tensor` element-wise: `log(exp(x) + 1)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.softplus().print();  // or tf.softplus(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction softplus_(x) {\n    const $x = convertToTensor(x, 'x', 'softplus');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.mul($x.sigmoid()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.softplus($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes sin of the input Tensor element-wise: `sin(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n *\n * x.sin().print();  // or tf.sin(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction sin_(x) {\n    const $x = convertToTensor(x, 'x', 'sin');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => $x.toFloat().cos().mul(dy) };\n    };\n    const inputsToSave = [$x];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.sin($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Sin', {} /* attrs */, inputsToSave);\n}\n/**\n * Computes cos of the input `tf.Tensor` element-wise: `cos(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n *\n * x.cos().print();  // or tf.cos(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction cos_(x) {\n    const $x = convertToTensor(x, 'x', 'cos');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => $x.toFloat().sin().neg().mul(dy) };\n    };\n    const inputsToSave = [$x];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.cos($x);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'Cos', {} /* attrs */, inputsToSave);\n}\n/**\n * Computes tan of the input `tf.Tensor` element-wise, `tan(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);\n *\n * x.tan().print();  // or tf.tan(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction tan_(x) {\n    const $x = convertToTensor(x, 'x', 'tan');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.div($x.cos().square()) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.tan($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes asin of the input `tf.Tensor` element-wise: `asin(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.asin().print();  // or tf.asin(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction asin_(x) {\n    const $x = convertToTensor(x, 'x', 'asin');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            $x: () => dy.div(scalar(1).sub($x.toFloat().square()).sqrt())\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.asin($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes acos of the input `tf.Tensor` element-wise: `acos(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.acos().print();  // or tf.acos(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction acos_(x) {\n    const $x = convertToTensor(x, 'x', 'acos');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            $x: () => {\n                const a = $x.toFloat().square();\n                const b = scalar(1).sub(a).sqrt();\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                return dy.div(b).neg();\n            }\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.acos($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes atan of the input `tf.Tensor` element-wise: `atan(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.atan().print();  // or tf.atan(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction atan_(x) {\n    const $x = convertToTensor(x, 'x', 'atan');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.div($x.toFloat().square().add(1)) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.atan($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes hyperbolic sin of the input `tf.Tensor` element-wise: `sinh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.sinh().print();  // or tf.sinh(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction sinh_(x) {\n    const $x = convertToTensor(x, 'x', 'sinh');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        return { $x: () => $x.toFloat().cosh().mul(dy) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.sinh($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes hyperbolic cos of the input `tf.Tensor` element-wise: `cosh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.cosh().print();  // or tf.cosh(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction cosh_(x) {\n    const $x = convertToTensor(x, 'x', 'cosh');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        return { $x: () => $x.toFloat().sinh().mul(dy) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.cosh($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes hyperbolic tangent of the input `tf.Tensor` element-wise: `tanh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, 70]);\n *\n * x.tanh().print();  // or tf.tanh(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction tanh_(x) {\n    const $x = convertToTensor(x, 'x', 'tanh');\n    const grad = (dy, saved) => {\n        const [y] = saved;\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        return { x: () => scalar(1).sub(y.square()).mul(dy) };\n    };\n    const outputsToSave = [true];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.tanh($x);\n        save([y]);\n        return y;\n    }, { x: $x }, grad, 'Tanh', {} /* attrs */, null /* inputsToSave */, outputsToSave);\n}\n/**\n * Computes inverse hyperbolic sin of the input `tf.Tensor` element-wise:\n * `asinh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.asinh().print();  // or tf.asinh(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction asinh_(x) {\n    const $x = convertToTensor(x, 'x', 'asinh');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            $x: () => {\n                const a = scalar(1).add($x.toFloat().square()).sqrt();\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                return dy.div(a);\n            }\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.asinh($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes the inverse hyperbolic cos of the input `tf.Tensor` element-wise:\n * `acosh(x)`\n *\n * ```js\n * const x = tf.tensor1d([10, 1, 3, 5.7]);\n *\n * x.acosh().print();  // or tf.acosh(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction acosh_(x) {\n    const $x = convertToTensor(x, 'x', 'acosh');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            $x: () => {\n                const a = $x.toFloat().square().sub(1).sqrt();\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                return dy.div(a);\n            }\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.acosh($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes inverse hyperbolic tan of the input `tf.Tensor` element-wise:\n * `atanh(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, .1, -.1, .7]);\n *\n * x.atanh().print();  // or tf.atanh(x)\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction atanh_(x) {\n    const $x = convertToTensor(x, 'x', 'atanh');\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => dy.div(scalar(1).sub($x.toFloat().square())) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.atanh($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes gause error function of the input `tf.Tensor` element-wise:\n * `erf(x)`\n *\n * ```js\n * const x = tf.tensor1d([0, .1, -.1, .7]);\n *\n * x.erf().print(); // or tf.erf(x);\n * ```\n * @param x The input tensor.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction erf_(x) {\n    let $x = convertToTensor(x, 'x', 'erf');\n    util.assert($x.dtype === 'int32' || $x.dtype === 'float32', () => 'Input dtype must be `int32` or `float32`.');\n    if ($x.dtype === 'int32') {\n        $x = $x.toFloat();\n    }\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return {\n            $x: () => dy.mul($x.square().neg().exp().mul(2 / Math.sqrt(Math.PI)))\n        };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.erf($x);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Computes step of the input `tf.Tensor` element-wise: `x > 0 ? 1 : alpha * x`\n *\n * ```js\n * const x = tf.tensor1d([0, 2, -1, -3]);\n *\n * x.step(.5).print();  // or tf.step(x, .5)\n * ```\n * @param x The input tensor.\n * @param alpha The gradient when input is negative.\n */\n/** @doc {heading: 'Operations', subheading: 'Basic math'} */\nfunction step_(x, alpha = 0.0) {\n    const $x = convertToTensor(x, 'x', 'step');\n    // TODO(manrajgrover): Return null for gradients when backprop supports\n    // it.\n    const grad = (dy) => {\n        return { $x: () => zerosLike(dy) };\n    };\n    return ENGINE.runKernelFunc(backend => backend.step($x, alpha), { $x }, grad);\n}\nexport const abs = op({ abs_ });\nexport const acos = op({ acos_ });\nexport const acosh = op({ acosh_ });\nexport const asin = op({ asin_ });\nexport const asinh = op({ asinh_ });\nexport const atan = op({ atan_ });\nexport const atanh = op({ atanh_ });\nexport const ceil = op({ ceil_ });\nexport const clipByValue = op({ clipByValue_ });\nexport const cos = op({ cos_ });\nexport const cosh = op({ cosh_ });\nexport const erf = op({ erf_ });\nexport const exp = op({ exp_ });\nexport const expm1 = op({ expm1_ });\nexport const floor = op({ floor_ });\nexport const log = op({ log_ });\nexport const log1p = op({ log1p_ });\nexport const logSigmoid = op({ logSigmoid_ });\nexport const neg = op({ neg_ });\nexport const reciprocal = op({ reciprocal_ });\nexport const round = op({ round_ });\nexport const rsqrt = op({ rsqrt_ });\nexport const sigmoid = op({ sigmoid_ });\nexport const sign = op({ sign_ });\nexport const isNaN = op({ isNaN_ });\nexport const isInf = op({ isInf_ });\nexport const isFinite = op({ isFinite_ });\nexport const sin = op({ sin_ });\nexport const sinh = op({ sinh_ });\nexport const softplus = op({ softplus_ });\nexport const sqrt = op({ sqrt_ });\nexport const step = op({ step_ });\nexport const tan = op({ tan_ });\nexport const tanh = op({ tanh_ });\n//# sourceMappingURL=unary_ops.js.map"]},"metadata":{},"sourceType":"module"}