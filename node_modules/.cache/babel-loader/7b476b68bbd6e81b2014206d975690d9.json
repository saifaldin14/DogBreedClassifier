{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertShapesMatch } from '../util';\nimport { expandShapeToKeepDim } from './axis_util';\nimport { minimum } from './minimum';\nimport { op } from './operation';\nimport { ones, scalar } from './tensor_ops';\nexport var Reduction;\n\n(function (Reduction) {\n  Reduction[Reduction[\"NONE\"] = 0] = \"NONE\";\n  Reduction[Reduction[\"MEAN\"] = 1] = \"MEAN\";\n  Reduction[Reduction[\"SUM\"] = 2] = \"SUM\";\n  Reduction[Reduction[\"SUM_BY_NONZERO_WEIGHTS\"] = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n})(Reduction || (Reduction = {}));\n/**\n * Computes the weighted loss between two tensors.\n *\n * @param losses Tensor of shape `[batch_size, d1, ... dN]`.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `losses`, and must be broadcastable to `losses` (i.e., all\n *    dimensions must be either `1`, or the same as the corresponding\n *    `losses` dimension).\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction computeWeightedLoss_(losses, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  const $losses = convertToTensor(losses, 'losses', 'computeWeightedLoss');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'computeWeightedLoss');\n  }\n\n  const weightedLoss = $weights == null ? $losses : $losses.mul($weights);\n\n  if (reduction === Reduction.NONE) {\n    return weightedLoss;\n  }\n\n  if (reduction === Reduction.SUM) {\n    return weightedLoss.sum();\n  }\n\n  if (reduction === Reduction.MEAN) {\n    if ($weights == null) {\n      return weightedLoss.mean();\n    } else {\n      const broadcastFactor = $losses.size / $weights.size;\n      const result = weightedLoss.sum().div($weights.sum());\n      return broadcastFactor > 1 ? result.div(scalar(broadcastFactor)) : result;\n    }\n  }\n\n  if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    if ($weights == null) {\n      return weightedLoss.sum().div(scalar($losses.size));\n    } else {\n      const broadcastedWeights = $weights.mul(ones($losses.shape));\n      const numNonZeros = broadcastedWeights.notEqual(scalar(0)).sum().toFloat();\n      return weightedLoss.sum().div(numNonZeros);\n    }\n  }\n\n  throw Error(`Unknown reduction: ${reduction}`);\n}\n/**\n * Computes the absolute difference loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  const $labels = convertToTensor(labels, 'labels', 'absoluteDifference');\n  const $predictions = convertToTensor(predictions, 'predictions', 'absoluteDifference');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'absoluteDifference');\n  }\n\n  assertShapesMatch($labels.shape, $predictions.shape, 'Error in absoluteDifference: ');\n  const losses = $labels.sub($predictions).abs();\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the mean squared error between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  const $labels = convertToTensor(labels, 'labels', 'meanSquaredError');\n  const $predictions = convertToTensor(predictions, 'predictions', 'meanSquaredError');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'meanSquaredError');\n  }\n\n  assertShapesMatch($labels.shape, $predictions.shape, 'Error in meanSquaredError: ');\n  const losses = $labels.squaredDifference($predictions);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the cosine distance loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param axis The dimension along which the cosine distance is computed.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  const $labels = convertToTensor(labels, 'labels', 'cosineDistance');\n  const $predictions = convertToTensor(predictions, 'predictions', 'cosineDistance');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'cosineDistance');\n  }\n\n  assertShapesMatch($labels.shape, $predictions.shape, 'Error in cosineDistance: ');\n  const one = scalar(1);\n  const losses = one.sub($labels.mul($predictions).sum(axis, true));\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the Hinge loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  let $labels = convertToTensor(labels, 'labels', 'hingeLoss');\n  const $predictions = convertToTensor(predictions, 'predictions', 'hingeLoss');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'hingeLoss');\n  }\n\n  assertShapesMatch($labels.shape, $predictions.shape, 'Error in hingeLoss: ');\n  const one = scalar(1); // Convert binary labels to (-1, 1)\n\n  $labels = scalar(2).mul($labels).sub(one);\n  const losses = one.sub($labels.mul($predictions)).relu();\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the log loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param epsilon A small increment to avoid taking log of zero\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  const $labels = convertToTensor(labels, 'labels', 'logLoss');\n  const $predictions = convertToTensor(predictions, 'predictions', 'logLoss');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'logLoss');\n  }\n\n  assertShapesMatch($labels.shape, $predictions.shape, 'Error in logLoss: ');\n  const one = scalar(1);\n  const epsilonScalar = scalar(epsilon);\n  const losses = $labels.mul($predictions.add(epsilonScalar).log()).neg().sub(one.sub($labels).mul(one.sub($predictions).add(epsilonScalar).log()));\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n  const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n  const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n  assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n  /**\n   * Implementation Details:\n   *\n   * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n   *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n   *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n   *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n   *   = (1 - z) * x + log(1 + exp(-x))\n   *   = x - x * z + log(1 + exp(-x))\n   *\n   *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n   *     x - x * z + log(1 + exp(-x))\n   *   = log(exp(x)) - x * z + log(1 + exp(-x))\n   *   = - x * z + log(1 + exp(x))\n   *\n   * Hence, to ensure stability and avoid overflow, the implementation uses\n   * this equivalent formulation:\n   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n   */\n\n  const maxOutput = $logits.relu();\n  const outputXTarget = $logits.mul($labels);\n  const sigmoidOutput = $logits.abs().neg().exp().log1p();\n  return maxOutput.sub(outputXTarget).add(sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' } */\n\n\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n  const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n  }\n\n  assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    const labelSmoothingScalar = scalar(labelSmoothing);\n    const one = scalar(1);\n    const half = scalar(0.5);\n    $multiClassLabels = $multiClassLabels.mul(one.sub(labelSmoothingScalar)).add(half.mul(labelSmoothingScalar));\n  }\n\n  const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the huber loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param delta Point where huber loss changes from quadratic to linear.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`.\n */\n\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\n\n\nfunction huberLoss_(labels, predictions, weights, delta = 1.0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  const $labels = convertToTensor(labels, 'labels', 'huberLoss');\n  const $predictions = convertToTensor(predictions, 'predictions', 'huberLoss');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'huberLoss');\n  }\n\n  assertShapesMatch($labels.shape, $predictions.shape, 'Error in huberLoss: ');\n  const deltaScalar = scalar(delta);\n  const error = $predictions.sub($labels).abs();\n  const quadratic = minimum(error, deltaScalar);\n  const linear = error.sub(quadratic);\n  const losses = scalar(0.5).mul(quadratic.square()).add(deltaScalar.mul(linear));\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\n\n\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n  if (dim === -1) {\n    dim = logits.rank - 1;\n  }\n\n  if (dim !== logits.rank - 1) {\n    throw Error(`Softmax cross entropy along a non-last dimension is not yet ` + `supported. Labels / logits was rank ${logits.rank} ` + `and dim was ${dim}`);\n  } // Use a custom gradient for numerical stability.\n\n\n  const customOp = customGrad((labels, logits, save) => {\n    // Reference:\n    //   1. http://cs231n.github.io/linear-classify/#softmax\n    //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n    const keepDims = true;\n    const lse = logits.logSumExp([dim], keepDims);\n    const logResult = logits.toFloat().sub(lse);\n    save([labels, logResult]);\n    const costVector = logResult.mul(labels).neg();\n    const value = costVector.sum([dim]);\n\n    const gradFunc = (dy, saved) => {\n      const [labels, logResult] = saved;\n      const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n      return [dy.reshape(dyShape).mul(labels.toFloat().sub(logResult.exp())), dy.reshape(dyShape).mul(logResult.exp().sub(labels.toFloat()))];\n    };\n\n    return {\n      value,\n      gradFunc\n    };\n  });\n  return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n\n/** @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' } */\n\n\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n  const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n  }\n\n  assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    const labelSmoothingScalar = scalar(labelSmoothing);\n    const one = scalar(1);\n    const numClasses = scalar($onehotLabels.shape[1]);\n    $onehotLabels = $onehotLabels.mul(one.sub(labelSmoothingScalar)).add(labelSmoothingScalar.div(numClasses));\n  }\n\n  const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nexport const absoluteDifference = op({\n  absoluteDifference_\n});\nexport const computeWeightedLoss = op({\n  computeWeightedLoss_\n});\nexport const cosineDistance = op({\n  cosineDistance_\n});\nexport const hingeLoss = op({\n  hingeLoss_\n});\nexport const huberLoss = op({\n  huberLoss_\n});\nexport const logLoss = op({\n  logLoss_\n});\nexport const meanSquaredError = op({\n  meanSquaredError_\n});\nexport const sigmoidCrossEntropy = op({\n  sigmoidCrossEntropy_\n});\nexport const softmaxCrossEntropy = op({\n  softmaxCrossEntropy_\n});","map":{"version":3,"sources":["../../src/ops/loss_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,UAAR,QAAyB,cAAzB;AAGA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,SAAQ,iBAAR,QAAgC,SAAhC;AACA,SAAQ,oBAAR,QAAmC,aAAnC;AACA,SAAQ,OAAR,QAAsB,WAAtB;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,IAAR,EAAc,MAAd,QAA2B,cAA3B;AAEA,OAAA,IAAY,SAAZ;;AAAA,CAAA,UAAY,SAAZ,EAAqB;AACnB,EAAA,SAAA,CAAA,SAAA,CAAA,MAAA,CAAA,GAAA,CAAA,CAAA,GAAA,MAAA;AACA,EAAA,SAAA,CAAA,SAAA,CAAA,MAAA,CAAA,GAAA,CAAA,CAAA,GAAA,MAAA;AACA,EAAA,SAAA,CAAA,SAAA,CAAA,KAAA,CAAA,GAAA,CAAA,CAAA,GAAA,KAAA;AACA,EAAA,SAAA,CAAA,SAAA,CAAA,wBAAA,CAAA,GAAA,CAAA,CAAA,GAAA,wBAAA;AACD,CALD,EAAY,SAAS,KAAT,SAAS,GAAA,EAAA,CAArB;AAOA;;;;;;;;;;AASA;;;AACA,SAAS,oBAAT,CACI,MADJ,EAC0B,OAD1B,EAEI,SAAS,GAAG,SAAS,CAAC,sBAF1B,EAEgD;AAC9C,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;AACA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;AACD;;AAED,QAAM,YAAY,GAAI,QAAQ,IAAI,IAAb,GAAqB,OAArB,GAA+B,OAAO,CAAC,GAAR,CAAY,QAAZ,CAApD;;AAEA,MAAI,SAAS,KAAK,SAAS,CAAC,IAA5B,EAAkC;AAChC,WAAO,YAAP;AACD;;AACD,MAAI,SAAS,KAAK,SAAS,CAAC,GAA5B,EAAiC;AAC/B,WAAO,YAAY,CAAC,GAAb,EAAP;AACD;;AACD,MAAI,SAAS,KAAK,SAAS,CAAC,IAA5B,EAAkC;AAChC,QAAI,QAAQ,IAAI,IAAhB,EAAsB;AACpB,aAAO,YAAY,CAAC,IAAb,EAAP;AACD,KAFD,MAEO;AACL,YAAM,eAAe,GAAG,OAAO,CAAC,IAAR,GAAe,QAAQ,CAAC,IAAhD;AACA,YAAM,MAAM,GAAG,YAAY,CAAC,GAAb,GAAmB,GAAnB,CAAuB,QAAQ,CAAC,GAAT,EAAvB,CAAf;AACA,aAAO,eAAe,GAAG,CAAlB,GAAsB,MAAM,CAAC,GAAP,CAAW,MAAM,CAAC,eAAD,CAAjB,CAAtB,GACsB,MAD7B;AAED;AACF;;AACD,MAAI,SAAS,KAAK,SAAS,CAAC,sBAA5B,EAAoD;AAClD,QAAI,QAAQ,IAAI,IAAhB,EAAsB;AACpB,aAAO,YAAY,CAAC,GAAb,GAAmB,GAAnB,CAAuB,MAAM,CAAC,OAAO,CAAC,IAAT,CAA7B,CAAP;AACD,KAFD,MAEO;AACL,YAAM,kBAAkB,GAAG,QAAQ,CAAC,GAAT,CAAa,IAAI,CAAC,OAAO,CAAC,KAAT,CAAjB,CAA3B;AAEA,YAAM,WAAW,GACb,kBAAkB,CAAC,QAAnB,CAA4B,MAAM,CAAC,CAAD,CAAlC,EAAuC,GAAvC,GAA6C,OAA7C,EADJ;AAEA,aAAO,YAAY,CAAC,GAAb,GAAmB,GAAnB,CAAuB,WAAvB,CAAP;AACD;AACF;;AAED,QAAM,KAAK,CAAC,sBAAsB,SAAS,EAAhC,CAAX;AACD;AAED;;;;;;;;;;;;;;AAaA;;;AACA,SAAS,mBAAT,CACI,MADJ,EAC0B,WAD1B,EAEI,OAFJ,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,oBAAnB,CAA/B;AACA,QAAM,YAAY,GACd,eAAe,CAAC,WAAD,EAAc,aAAd,EAA6B,oBAA7B,CADnB;AAEA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,oBAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CACb,OAAO,CAAC,KADK,EACE,YAAY,CAAC,KADf,EACsB,+BADtB,CAAjB;AAGA,QAAM,MAAM,GAAG,OAAO,CAAC,GAAR,CAAY,YAAZ,EAA0B,GAA1B,EAAf;AACA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;AAED;;;;;;;;;;;;;;AAaA;;;AACA,SAAS,iBAAT,CACI,MADJ,EAC0B,WAD1B,EAEI,OAFJ,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,kBAAnB,CAA/B;AACA,QAAM,YAAY,GACd,eAAe,CAAC,WAAD,EAAc,aAAd,EAA6B,kBAA7B,CADnB;AAEA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,kBAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CACb,OAAO,CAAC,KADK,EACE,YAAY,CAAC,KADf,EACsB,6BADtB,CAAjB;AAGA,QAAM,MAAM,GAAG,OAAO,CAAC,iBAAR,CAA0B,YAA1B,CAAf;AACA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;AAED;;;;;;;;;;;;;;;AAcA;;;AACA,SAAS,eAAT,CACI,MADJ,EAC0B,WAD1B,EACqD,IADrD,EAEI,OAFJ,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,gBAAnB,CAA/B;AACA,QAAM,YAAY,GACd,eAAe,CAAC,WAAD,EAAc,aAAd,EAA6B,gBAA7B,CADnB;AAEA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,gBAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CACb,OAAO,CAAC,KADK,EACE,YAAY,CAAC,KADf,EACsB,2BADtB,CAAjB;AAGA,QAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB;AACA,QAAM,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,OAAO,CAAC,GAAR,CAAY,YAAZ,EAA0B,GAA1B,CAA8B,IAA9B,EAAoC,IAApC,CAAR,CAAf;AACA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;AAED;;;;;;;;;;;;;;AAaA;;;AACA,SAAS,UAAT,CACI,MADJ,EAC0B,WAD1B,EAEI,OAFJ,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,MAAI,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,WAAnB,CAA7B;AACA,QAAM,YAAY,GAAG,eAAe,CAAC,WAAD,EAAc,aAAd,EAA6B,WAA7B,CAApC;AACA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,WAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CAAC,OAAO,CAAC,KAAT,EAAgB,YAAY,CAAC,KAA7B,EAAoC,sBAApC,CAAjB;AAEA,QAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB,CAT8C,CAU9C;;AACA,EAAA,OAAO,GAAG,MAAM,CAAC,CAAD,CAAN,CAAU,GAAV,CAAc,OAAd,EAAuB,GAAvB,CAA2B,GAA3B,CAAV;AACA,QAAM,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,OAAO,CAAC,GAAR,CAAY,YAAZ,CAAR,EAAmC,IAAnC,EAAf;AACA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;AAED;;;;;;;;;;;;;;;AAcA;;;AACA,SAAS,QAAT,CACI,MADJ,EAC0B,WAD1B,EAEI,OAFJ,EAEiC,OAAO,GAAG,IAF3C,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA/B;AACA,QAAM,YAAY,GAAG,eAAe,CAAC,WAAD,EAAc,aAAd,EAA6B,SAA7B,CAApC;AACA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,SAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CAAC,OAAO,CAAC,KAAT,EAAgB,YAAY,CAAC,KAA7B,EAAoC,oBAApC,CAAjB;AAEA,QAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB;AACA,QAAM,aAAa,GAAG,MAAM,CAAC,OAAD,CAA5B;AACA,QAAM,MAAM,GAAG,OAAO,CAAC,GAAR,CAAY,YAAY,CAAC,GAAb,CAAiB,aAAjB,EAAgC,GAAhC,EAAZ,EACK,GADL,GAEK,GAFL,CAES,GAAG,CAAC,GAAJ,CAAQ,OAAR,EAAiB,GAAjB,CACD,GAAG,CAAC,GAAJ,CAAQ,YAAR,EAAsB,GAAtB,CAA0B,aAA1B,EAAyC,GAAzC,EADC,CAFT,CAAf;AAIA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;;AAED,SAAS,8BAAT,CACI,MADJ,EAC0B,MAD1B,EAC8C;AAC5C,QAAM,OAAO,GACT,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,+BAAnB,CADnB;AAEA,QAAM,OAAO,GACT,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,+BAAnB,CADnB;AAEA,EAAA,iBAAiB,CACb,OAAO,CAAC,KADK,EACE,OAAO,CAAC,KADV,EACiB,0CADjB,CAAjB;AAGA;;;;;;;;;;;;;;;;;;;;;AAoBA,QAAM,SAAS,GAAG,OAAO,CAAC,IAAR,EAAlB;AACA,QAAM,aAAa,GAAG,OAAO,CAAC,GAAR,CAAY,OAAZ,CAAtB;AACA,QAAM,aAAa,GAAG,OAAO,CAAC,GAAR,GAAc,GAAd,GAAoB,GAApB,GAA0B,KAA1B,EAAtB;AAEA,SAAO,SAAS,CAAC,GAAV,CAAc,aAAd,EAA6B,GAA7B,CAAiC,aAAjC,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;AAmBA;;;AACA,SAAS,oBAAT,CACI,gBADJ,EACoC,MADpC,EAEI,OAFJ,EAEiC,cAAc,GAAG,CAFlD,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,MAAI,iBAAiB,GAAG,eAAe,CACnC,gBADmC,EACjB,kBADiB,EACG,qBADH,CAAvC;AAEA,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;AACA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CACb,iBAAiB,CAAC,KADL,EACY,OAAO,CAAC,KADpB,EAC2B,gCAD3B,CAAjB;;AAGA,MAAI,cAAc,GAAG,CAArB,EAAwB;AACtB,UAAM,oBAAoB,GAAG,MAAM,CAAC,cAAD,CAAnC;AACA,UAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB;AACA,UAAM,IAAI,GAAG,MAAM,CAAC,GAAD,CAAnB;AAEA,IAAA,iBAAiB,GAAG,iBAAiB,CAAC,GAAlB,CAAsB,GAAG,CAAC,GAAJ,CAAQ,oBAAR,CAAtB,EACK,GADL,CACS,IAAI,CAAC,GAAL,CAAS,oBAAT,CADT,CAApB;AAED;;AACD,QAAM,MAAM,GAAG,8BAA8B,CAAC,iBAAD,EAAoB,OAApB,CAA7C;AAEA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;AAED;;;;;;;;;;;;;;;AAcA;;;AACA,SAAS,UAAT,CACI,MADJ,EAC0B,WAD1B,EAEI,OAFJ,EAEiC,KAAK,GAAG,GAFzC,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,WAAnB,CAA/B;AACA,QAAM,YAAY,GAAG,eAAe,CAAC,WAAD,EAAc,aAAd,EAA6B,WAA7B,CAApC;AACA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,WAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CAAC,OAAO,CAAC,KAAT,EAAgB,YAAY,CAAC,KAA7B,EAAoC,sBAApC,CAAjB;AAEA,QAAM,WAAW,GAAG,MAAM,CAAC,KAAD,CAA1B;AACA,QAAM,KAAK,GAAG,YAAY,CAAC,GAAb,CAAiB,OAAjB,EAA0B,GAA1B,EAAd;AACA,QAAM,SAAS,GAAG,OAAO,CAAC,KAAD,EAAQ,WAAR,CAAzB;AACA,QAAM,MAAM,GAAG,KAAK,CAAC,GAAN,CAAU,SAAV,CAAf;AAEA,QAAM,MAAM,GACR,MAAM,CAAC,GAAD,CAAN,CAAY,GAAZ,CAAgB,SAAS,CAAC,MAAV,EAAhB,EAAoC,GAApC,CAAwC,WAAW,CAAC,GAAZ,CAAgB,MAAhB,CAAxC,CADJ;AAEA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;AAwBA,SAAS,8BAAT,CACI,MADJ,EACe,MADf,EAC0B,GAAG,GAAG,CAAC,CADjC,EACkC;AAChC,MAAI,GAAG,KAAK,CAAC,CAAb,EAAgB;AACd,IAAA,GAAG,GAAG,MAAM,CAAC,IAAP,GAAc,CAApB;AACD;;AAED,MAAI,GAAG,KAAK,MAAM,CAAC,IAAP,GAAc,CAA1B,EAA6B;AAC3B,UAAM,KAAK,CACP,8DAAA,GACA,uCAAuC,MAAM,CAAC,IAAI,GADlD,GAEA,eAAe,GAAG,EAHX,CAAX;AAID,GAV+B,CAWhC;;;AACA,QAAM,QAAQ,GACV,UAAU,CAAC,CAAC,MAAD,EAAiB,MAAjB,EAAiC,IAAjC,KAAuD;AAChE;AACA;AACA;AACA,UAAM,QAAQ,GAAG,IAAjB;AACA,UAAM,GAAG,GAAG,MAAM,CAAC,SAAP,CAAiB,CAAC,GAAD,CAAjB,EAAwB,QAAxB,CAAZ;AACA,UAAM,SAAS,GAAG,MAAM,CAAC,OAAP,GAAiB,GAAjB,CAAqB,GAArB,CAAlB;AACA,IAAA,IAAI,CAAC,CAAC,MAAD,EAAS,SAAT,CAAD,CAAJ;AAEA,UAAM,UAAU,GAAG,SAAS,CAAC,GAAV,CAAc,MAAd,EAAsB,GAAtB,EAAnB;AACA,UAAM,KAAK,GAAM,UAAU,CAAC,GAAX,CAAe,CAAC,GAAD,CAAf,CAAjB;;AAEA,UAAM,QAAQ,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AAC1C,YAAM,CAAC,MAAD,EAAS,SAAT,IAAsB,KAA5B;AACA,YAAM,OAAO,GAAG,oBAAoB,CAAC,EAAE,CAAC,KAAJ,EAAW,CAAC,GAAD,CAAX,CAApC;AACA,aAAO,CACL,EAAE,CAAC,OAAH,CAAW,OAAX,EAAoB,GAApB,CAAwB,MAAM,CAAC,OAAP,GAAiB,GAAjB,CAAqB,SAAS,CAAC,GAAV,EAArB,CAAxB,CADK,EAEL,EAAE,CAAC,OAAH,CAAW,OAAX,EAAoB,GAApB,CAAwB,SAAS,CAAC,GAAV,GAAgB,GAAhB,CAAoB,MAAM,CAAC,OAAP,EAApB,CAAxB,CAFK,CAAP;AAID,KAPD;;AAQA,WAAO;AAAC,MAAA,KAAD;AAAQ,MAAA;AAAR,KAAP;AACD,GArBS,CADd;AAwBA,SAAO,QAAQ,CAAC,MAAD,EAAS,MAAT,CAAf;AACD;AAED;;;;;;;;;;;;;;;;;;AAiBA;;;AACA,SAAS,oBAAT,CACI,YADJ,EACgC,MADhC,EAEI,OAFJ,EAEiC,cAAc,GAAG,CAFlD,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,MAAI,aAAa,GACb,eAAe,CAAC,YAAD,EAAe,cAAf,EAA+B,qBAA/B,CADnB;AAEA,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;AACA,MAAI,QAAQ,GAAW,IAAvB;;AAEA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;AACD;;AAED,EAAA,iBAAiB,CACb,aAAa,CAAC,KADD,EACQ,OAAO,CAAC,KADhB,EACuB,gCADvB,CAAjB;;AAGA,MAAI,cAAc,GAAG,CAArB,EAAwB;AACtB,UAAM,oBAAoB,GAAG,MAAM,CAAC,cAAD,CAAnC;AACA,UAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB;AACA,UAAM,UAAU,GAAG,MAAM,CAAC,aAAa,CAAC,KAAd,CAAoB,CAApB,CAAD,CAAzB;AAEA,IAAA,aAAa,GAAG,aAAa,CAAC,GAAd,CAAkB,GAAG,CAAC,GAAJ,CAAQ,oBAAR,CAAlB,EACK,GADL,CACS,oBAAoB,CAAC,GAArB,CAAyB,UAAzB,CADT,CAAhB;AAED;;AAED,QAAM,MAAM,GAAG,8BAA8B,CAAC,aAAD,EAAgB,OAAhB,CAA7C;AAEA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;;AAED,OAAO,MAAM,kBAAkB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA7B;AACP,OAAO,MAAM,mBAAmB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA9B;AACP,OAAO,MAAM,cAAc,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAzB;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,gBAAgB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA3B;AACP,OAAO,MAAM,mBAAmB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA9B;AACP,OAAO,MAAM,mBAAmB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA9B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertShapesMatch } from '../util';\nimport { expandShapeToKeepDim } from './axis_util';\nimport { minimum } from './minimum';\nimport { op } from './operation';\nimport { ones, scalar } from './tensor_ops';\nexport var Reduction;\n(function (Reduction) {\n    Reduction[Reduction[\"NONE\"] = 0] = \"NONE\";\n    Reduction[Reduction[\"MEAN\"] = 1] = \"MEAN\";\n    Reduction[Reduction[\"SUM\"] = 2] = \"SUM\";\n    Reduction[Reduction[\"SUM_BY_NONZERO_WEIGHTS\"] = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n})(Reduction || (Reduction = {}));\n/**\n * Computes the weighted loss between two tensors.\n *\n * @param losses Tensor of shape `[batch_size, d1, ... dN]`.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `losses`, and must be broadcastable to `losses` (i.e., all\n *    dimensions must be either `1`, or the same as the corresponding\n *    `losses` dimension).\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction computeWeightedLoss_(losses, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $losses = convertToTensor(losses, 'losses', 'computeWeightedLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'computeWeightedLoss');\n    }\n    const weightedLoss = ($weights == null) ? $losses : $losses.mul($weights);\n    if (reduction === Reduction.NONE) {\n        return weightedLoss;\n    }\n    if (reduction === Reduction.SUM) {\n        return weightedLoss.sum();\n    }\n    if (reduction === Reduction.MEAN) {\n        if ($weights == null) {\n            return weightedLoss.mean();\n        }\n        else {\n            const broadcastFactor = $losses.size / $weights.size;\n            const result = weightedLoss.sum().div($weights.sum());\n            return broadcastFactor > 1 ? result.div(scalar(broadcastFactor)) :\n                result;\n        }\n    }\n    if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n        if ($weights == null) {\n            return weightedLoss.sum().div(scalar($losses.size));\n        }\n        else {\n            const broadcastedWeights = $weights.mul(ones($losses.shape));\n            const numNonZeros = broadcastedWeights.notEqual(scalar(0)).sum().toFloat();\n            return weightedLoss.sum().div(numNonZeros);\n        }\n    }\n    throw Error(`Unknown reduction: ${reduction}`);\n}\n/**\n * Computes the absolute difference loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'absoluteDifference');\n    const $predictions = convertToTensor(predictions, 'predictions', 'absoluteDifference');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'absoluteDifference');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in absoluteDifference: ');\n    const losses = $labels.sub($predictions).abs();\n    return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the mean squared error between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'meanSquaredError');\n    const $predictions = convertToTensor(predictions, 'predictions', 'meanSquaredError');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'meanSquaredError');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in meanSquaredError: ');\n    const losses = $labels.squaredDifference($predictions);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the cosine distance loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param axis The dimension along which the cosine distance is computed.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'cosineDistance');\n    const $predictions = convertToTensor(predictions, 'predictions', 'cosineDistance');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'cosineDistance');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in cosineDistance: ');\n    const one = scalar(1);\n    const losses = one.sub($labels.mul($predictions).sum(axis, true));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the Hinge loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $labels = convertToTensor(labels, 'labels', 'hingeLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'hingeLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'hingeLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in hingeLoss: ');\n    const one = scalar(1);\n    // Convert binary labels to (-1, 1)\n    $labels = scalar(2).mul($labels).sub(one);\n    const losses = one.sub($labels.mul($predictions)).relu();\n    return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the log loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param epsilon A small increment to avoid taking log of zero\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'logLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'logLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'logLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in logLoss: ');\n    const one = scalar(1);\n    const epsilonScalar = scalar(epsilon);\n    const losses = $labels.mul($predictions.add(epsilonScalar).log())\n        .neg()\n        .sub(one.sub($labels).mul(one.sub($predictions).add(epsilonScalar).log()));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n    const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n    assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n    /**\n     * Implementation Details:\n     *\n     * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n     *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n     *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n     *   = (1 - z) * x + log(1 + exp(-x))\n     *   = x - x * z + log(1 + exp(-x))\n     *\n     *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n     *     x - x * z + log(1 + exp(-x))\n     *   = log(exp(x)) - x * z + log(1 + exp(-x))\n     *   = - x * z + log(1 + exp(x))\n     *\n     * Hence, to ensure stability and avoid overflow, the implementation uses\n     * this equivalent formulation:\n     *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n     */\n    const maxOutput = $logits.relu();\n    const outputXTarget = $logits.mul($labels);\n    const sigmoidOutput = $logits.abs().neg().exp().log1p();\n    return maxOutput.sub(outputXTarget).add(sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' } */\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n    }\n    assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const half = scalar(0.5);\n        $multiClassLabels = $multiClassLabels.mul(one.sub(labelSmoothingScalar))\n            .add(half.mul(labelSmoothingScalar));\n    }\n    const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes the huber loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param delta Point where huber loss changes from quadratic to linear.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`.\n */\n/** @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'} */\nfunction huberLoss_(labels, predictions, weights, delta = 1.0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'huberLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'huberLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'huberLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in huberLoss: ');\n    const deltaScalar = scalar(delta);\n    const error = $predictions.sub($labels).abs();\n    const quadratic = minimum(error, deltaScalar);\n    const linear = error.sub(quadratic);\n    const losses = scalar(0.5).mul(quadratic.square()).add(deltaScalar.mul(linear));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n    if (dim === -1) {\n        dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n        throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +\n            `supported. Labels / logits was rank ${logits.rank} ` +\n            `and dim was ${dim}`);\n    }\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((labels, logits, save) => {\n        // Reference:\n        //   1. http://cs231n.github.io/linear-classify/#softmax\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n        const keepDims = true;\n        const lse = logits.logSumExp([dim], keepDims);\n        const logResult = logits.toFloat().sub(lse);\n        save([labels, logResult]);\n        const costVector = logResult.mul(labels).neg();\n        const value = costVector.sum([dim]);\n        const gradFunc = (dy, saved) => {\n            const [labels, logResult] = saved;\n            const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n            return [\n                dy.reshape(dyShape).mul(labels.toFloat().sub(logResult.exp())),\n                dy.reshape(dyShape).mul(logResult.exp().sub(labels.toFloat())),\n            ];\n        };\n        return { value, gradFunc };\n    });\n    return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n */\n/** @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' } */\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n    }\n    assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const numClasses = scalar($onehotLabels.shape[1]);\n        $onehotLabels = $onehotLabels.mul(one.sub(labelSmoothingScalar))\n            .add(labelSmoothingScalar.div(numClasses));\n    }\n    const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const absoluteDifference = op({ absoluteDifference_ });\nexport const computeWeightedLoss = op({ computeWeightedLoss_ });\nexport const cosineDistance = op({ cosineDistance_ });\nexport const hingeLoss = op({ hingeLoss_ });\nexport const huberLoss = op({ huberLoss_ });\nexport const logLoss = op({ logLoss_ });\nexport const meanSquaredError = op({ meanSquaredError_ });\nexport const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });\nexport const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });\n//# sourceMappingURL=loss_ops.js.map"]},"metadata":{},"sourceType":"module"}