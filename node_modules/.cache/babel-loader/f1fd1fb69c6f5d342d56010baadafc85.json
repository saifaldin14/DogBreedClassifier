{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { TensorBuffer } from '../tensor';\nimport { convertToTensor, convertToTensorArray } from '../tensor_util_env';\nimport * as util from '../util';\nimport { concat } from './concat';\nimport { op } from './operation';\n/**\n * Reshapes a `tf.Tensor` to a given shape.\n *\n * Given an input tensor, returns a new tensor with the same values as the\n * input tensor with shape `shape`.\n *\n * If one component of shape is the special value -1, the size of that\n * dimension is computed so that the total size remains constant. In\n * particular, a shape of [-1] flattens into 1-D. At most one component of\n * shape can be -1.\n *\n * If shape is 1-D or higher, then the operation returns a tensor with shape\n * shape filled with the values of tensor. In this case, the number of\n * elements implied by shape must be the same as the number of elements in\n * tensor.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * x.reshape([2, 2]).print();\n * ```\n *\n * @param x The input tensor to be reshaped.\n * @param shape An array of integers defining the output tensor shape.\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\n\nfunction reshape_(x, shape) {\n  const $x = convertToTensor(x, 'x', 'reshape', null);\n  shape = util.inferFromImplicitShape(shape, $x.size);\n  util.assert($x.size === util.sizeFromShape(shape), () => 'new shape and old shape must have the same number of elements.');\n\n  const grad = dy => {\n    return {\n      x: () => dy.reshape($x.shape)\n    };\n  };\n\n  const attrs = {\n    shape\n  };\n  return ENGINE.runKernelFunc(backend => backend.reshape($x, shape), {\n    x: $x\n  }, grad, 'Reshape', attrs);\n}\n/**\n * Removes dimensions of size 1 from the shape of a `tf.Tensor`.\n *\n * ```js\n * const x = tf.tensor([1, 2, 3, 4], [1, 1, 4]);\n * x.squeeze().print();\n * ```\n *\n * @param x The input tensor to be squeezed.\n * @param axis An optional list of numbers. If specified, only\n *     squeezes the dimensions listed. The dimension index starts at 0. It\n * is an error to squeeze a dimension that is not 1.\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\n\n\nfunction squeeze_(x, axis) {\n  const $x = convertToTensor(x, 'x', 'squeeze');\n  return reshape($x, util.squeezeShape($x.shape, axis).newShape);\n}\n/**\n * Casts a `tf.Tensor` to a new dtype.\n *\n * ```js\n * const x = tf.tensor1d([1.5, 2.5, 3]);\n * tf.cast(x, 'int32').print();\n * ```\n * @param x The input tensor to be casted.\n * @param dtype The dtype to cast the input tensor to.\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\n\n\nfunction cast_(x, dtype) {\n  const $x = convertToTensor(x, 'x', 'cast'); // Sanity checks.\n\n  if (!util.isValidDtype(dtype)) {\n    throw new Error(`Failed to cast to unknown dtype ${dtype}`);\n  }\n\n  if (dtype === 'string' && $x.dtype !== 'string' || dtype !== 'string' && $x.dtype === 'string') {\n    throw new Error('Only strings can be casted to strings');\n  }\n\n  const grad = dy => {\n    return {\n      x: () => dy.clone()\n    };\n  };\n\n  const attrs = {\n    dtype\n  };\n  return ENGINE.runKernelFunc(backend => backend.cast($x, dtype), {\n    x: $x\n  }, grad, 'Cast', attrs);\n}\n/**\n * Stacks a list of rank-`R` `tf.Tensor`s into one rank-`(R+1)` `tf.Tensor`.\n *\n * ```js\n * const a = tf.tensor1d([1, 2]);\n * const b = tf.tensor1d([3, 4]);\n * const c = tf.tensor1d([5, 6]);\n * tf.stack([a, b, c]).print();\n * ```\n *\n * @param tensors A list of tensor objects with the same shape and dtype.\n * @param axis The axis to stack along. Defaults to 0 (the first dim).\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Slicing and Joining'} */\n\n\nfunction stack_(tensors, axis = 0) {\n  const $tensors = convertToTensorArray(tensors, 'tensors', 'stack');\n  util.assert($tensors.length >= 1, () => 'Pass at least one tensor to tf.stack');\n\n  if ($tensors.length === 1) {\n    return $tensors[0].expandDims(axis);\n  }\n\n  const rank = $tensors[0].rank;\n  const shape = $tensors[0].shape;\n  const dtype = $tensors[0].dtype;\n  util.assert(axis <= rank, () => 'Axis must be <= rank of the tensor');\n  $tensors.forEach(t => {\n    util.assertShapesMatch(shape, t.shape, 'All tensors passed to stack must have matching shapes');\n  });\n  $tensors.forEach(t => {\n    util.assert(dtype === t.dtype, () => 'All tensors passed to stack must have matching dtypes');\n  });\n  const expandedTensors = $tensors.map(t => t.expandDims(axis));\n  return concat(expandedTensors, axis);\n}\n/**\n * Unstacks a `tf.Tensor` of rank-`R` into a list of rank-`(R-1)` `tf.Tensor`s.\n *\n * ```js\n * const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * tf.unstack(a).forEach(tensor => tensor.print());\n * ```\n *\n * @param x A tensor object.\n * @param axis The axis to unstack along. Defaults to 0 (the first dim).\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Slicing and Joining'} */\n\n\nfunction unstack_(x, axis = 0) {\n  axis = axis || 0;\n  const $x = convertToTensor(x, 'x', 'unstack');\n  util.assert(axis >= -$x.shape.length && axis < $x.shape.length, () => `Axis = ${axis} is not in [-${$x.shape.length}, ${$x.shape.length})`);\n\n  if (axis < 0) {\n    axis += $x.shape.length;\n  }\n\n  const grad = dy => {\n    return {\n      x: () => stack(dy, axis)\n    };\n  };\n\n  const attrs = {\n    axis\n  };\n  return ENGINE.runKernelFunc(backend => backend.unstack($x, axis), {\n    x: $x\n  }, grad, 'Unpack', attrs);\n}\n/**\n * Returns a `tf.Tensor` that has expanded rank, by inserting a dimension\n * into the tensor's shape.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * const axis = 1;\n * x.expandDims(axis).print();\n * ```\n *\n * @param x The input tensor whose dimensions to be expanded.\n * @param axis The dimension index at which to insert shape of `1`. Defaults\n *     to 0 (the first dimension).\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\n\n\nfunction expandDims_(x, axis = 0) {\n  const parseAs = null;\n  const $x = convertToTensor(x, 'x', 'expandDims', parseAs);\n  util.assert(axis <= $x.rank, () => 'Axis must be <= rank of the tensor');\n  const newShape = $x.shape.slice();\n\n  if (axis < 0) {\n    // Negative value is counted from the tail of rank.\n    util.assert(-($x.rank + 1) <= axis, () => `Axis must be in the interval [${-($x.rank + 1)}, ${$x.rank}]`);\n    axis = $x.rank + axis + 1;\n  }\n\n  newShape.splice(axis, 0, 1);\n  return reshape($x, newShape);\n}\n/**\n * Computes the difference between two lists of numbers.\n *\n * Given a Tensor `x` and a Tensor `y`, this operation returns a Tensor `out`\n * that represents all values that are in `x` but not in `y`. The returned\n * Tensor `out` is sorted in the same order that the numbers appear in `x`\n * (duplicates are preserved). This operation also returns a Tensor indices that\n * represents the position of each out element in `x`. In other words:\n *\n * `out[i] = x[idx[i]] for i in [0, 1, ..., out.length - 1]`\n *\n * ```js\n * const x = [1, 2, 3, 4, 5, 6];\n * const y = [1, 3, 5];\n *\n * const [out, indices] = await tf.setdiff1dAsync(x, y);\n * out.print(); // [2, 4, 6]\n * indices.print(); // [1, 3, 5]\n * ```\n *\n * @param x 1-D Tensor. Values to keep.\n * @param y 1-D Tensor. Must have the same type as x. Values to exclude in the\n *     output.\n * @returns Promise of Tensor tuple [out, indices].\n *  out: Tensor with the same type as x.\n *  indices: A Tensor of type int32.\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\n\n\nasync function setdiff1dAsync_(x, y) {\n  const $x = convertToTensor(x, 'x', 'setdiff1d');\n  const $y = convertToTensor(y, 'y', 'setdiff1d');\n  util.assert($x.dtype === $y.dtype, () => `x and y should have the same dtype, but got x (${$x.dtype}) and y (${$y.dtype}).`);\n  util.assert($x.rank === 1, () => `x should be 1D tensor, but got x (${$x.shape}).`);\n  util.assert($y.rank === 1, () => `y should be 1D tensor, but got y (${$y.shape}).`);\n  const xVals = await $x.data();\n  const yVals = await $y.data();\n  const ySet = new Set(yVals);\n  let outputSize = 0;\n\n  for (let i = 0; i < xVals.length; i++) {\n    if (!ySet.has(xVals[i])) {\n      outputSize++;\n    }\n  }\n\n  const buffer = new TensorBuffer([outputSize], $x.dtype);\n  const indices = new TensorBuffer([outputSize], 'int32');\n\n  for (let i = 0, p = 0; i < xVals.length; i++) {\n    if (!ySet.has(xVals[i])) {\n      buffer.values[p] = xVals[i];\n      indices.values[p] = i;\n      p++;\n    }\n  }\n\n  return [buffer.toTensor(), indices.toTensor()];\n}\n/**\n * Creates an empty `tf.TensorBuffer` with the specified `shape` and `dtype`.\n *\n * The values are stored in CPU as `TypedArray`. Fill the buffer using\n * `buffer.set()`, or by modifying directly `buffer.values`.\n *\n * When done, call `buffer.toTensor()` to get an immutable `tf.Tensor` with\n * those values.\n *\n * ```js\n * // Create a buffer and set values at particular indices.\n * const buffer = tf.buffer([2, 2]);\n * buffer.set(3, 0, 0);\n * buffer.set(5, 1, 0);\n *\n * // Convert the buffer back to a tensor.\n * buffer.toTensor().print();\n * ```\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param dtype The dtype of the buffer. Defaults to 'float32'.\n * @param values The values of the buffer as `TypedArray`. Defaults to\n * zeros.\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Creation'} */\n\n\nexport function buffer(shape, dtype = 'float32', values) {\n  dtype = dtype || 'float32';\n  util.assertNonNegativeIntegerDimensions(shape);\n  return new TensorBuffer(shape, dtype, values);\n}\n/**\n * Prints information about the `tf.Tensor` including its data.\n *\n * ```js\n * const verbose = true;\n * tf.tensor2d([1, 2, 3, 4], [2, 2]).print(verbose);\n * ```\n * @param x The tensor to be printed.\n * @param verbose Whether to print verbose information about the ` Tensor`,\n * including dtype and size.\n */\n\n/** @doc {heading: 'Tensors', subheading: 'Creation'} */\n\nfunction print(x, verbose = false) {\n  console.log(x.toString(verbose));\n}\n\nexport { print // Not wrapped in op() since no need to increase stack trace.\n};\nexport const cast = op({\n  cast_\n});\nexport const expandDims = op({\n  expandDims_\n});\nexport const reshape = op({\n  reshape_\n});\nexport const squeeze = op({\n  squeeze_\n});\nexport const stack = op({\n  stack_\n});\nexport const unstack = op({\n  unstack_\n});\nexport const setdiff1dAsync = setdiff1dAsync_;","map":{"version":3,"sources":["../../src/ops/array_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAgB,YAAhB,QAAmC,WAAnC;AACA,SAAQ,eAAR,EAAyB,oBAAzB,QAAoD,oBAApD;AAEA,OAAO,KAAK,IAAZ,MAAsB,SAAtB;AAEA,SAAQ,MAAR,QAAqB,UAArB;AACA,SAAQ,EAAR,QAAiB,aAAjB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;AAwBA;;AACA,SAAS,QAAT,CACI,CADJ,EAC0B,KAD1B,EAC6C;AAC3C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,EAAoB,IAApB,CAA1B;AACA,EAAA,KAAK,GAAG,IAAI,CAAC,sBAAL,CAA4B,KAA5B,EAAmC,EAAE,CAAC,IAAtC,CAAR;AACA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,KAAY,IAAI,CAAC,aAAL,CAAmB,KAAnB,CADhB,EAEI,MAAM,gEAFV;;AAIA,QAAM,IAAI,GAAI,EAAD,IAAmB;AAC9B,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,OAAH,CAAW,EAAE,CAAC,KAAd;AAAV,KAAP;AACD,GAFD;;AAGA,QAAM,KAAK,GAAG;AAAC,IAAA;AAAD,GAAd;AACA,SAAO,MAAM,CAAC,aAAP,CACH,OAAO,IAAI,OAAO,CAAC,OAAR,CAAgB,EAAhB,EAAoB,KAApB,CADR,EACoC;AAAC,IAAA,CAAC,EAAE;AAAJ,GADpC,EAC6C,IAD7C,EACmD,SADnD,EAC8D,KAD9D,CAAP;AAED;AAED;;;;;;;;;;;;;;AAaA;;;AACA,SAAS,QAAT,CAAoC,CAApC,EAA0D,IAA1D,EAAyE;AACvE,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAA1B;AACA,SAAO,OAAO,CAAC,EAAD,EAAK,IAAI,CAAC,YAAL,CAAkB,EAAE,CAAC,KAArB,EAA4B,IAA5B,EAAkC,QAAvC,CAAd;AACD;AAED;;;;;;;;;;;AAUA;;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAkD,KAAlD,EAAiE;AAC/D,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B,CAD+D,CAG/D;;AACA,MAAI,CAAC,IAAI,CAAC,YAAL,CAAkB,KAAlB,CAAL,EAA+B;AAC7B,UAAM,IAAI,KAAJ,CAAU,mCAAmC,KAAK,EAAlD,CAAN;AACD;;AACD,MAAI,KAAK,KAAK,QAAV,IAAsB,EAAE,CAAC,KAAH,KAAa,QAAnC,IACA,KAAK,KAAK,QAAV,IAAsB,EAAE,CAAC,KAAH,KAAa,QADvC,EACiD;AAC/C,UAAM,IAAI,KAAJ,CAAU,uCAAV,CAAN;AACD;;AAED,QAAM,IAAI,GAAI,EAAD,IAAU;AACrB,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,EAAE,CAAC,KAAH;AAAV,KAAP;AACD,GAFD;;AAGA,QAAM,KAAK,GAAG;AAAC,IAAA;AAAD,GAAd;AACA,SAAO,MAAM,CAAC,aAAP,CACH,OAAO,IAAI,OAAO,CAAC,IAAR,CAAa,EAAb,EAAiB,KAAjB,CADR,EACiC;AAAC,IAAA,CAAC,EAAE;AAAJ,GADjC,EAC0C,IAD1C,EACgD,MADhD,EACwD,KADxD,CAAP;AAED;AAED;;;;;;;;;;;;;;AAaA;;;AACA,SAAS,MAAT,CACI,OADJ,EACkC,IAAI,GAAG,CADzC,EAC0C;AACxC,QAAM,QAAQ,GAAG,oBAAoB,CAAC,OAAD,EAAU,SAAV,EAAqB,OAArB,CAArC;AAEA,EAAA,IAAI,CAAC,MAAL,CACI,QAAQ,CAAC,MAAT,IAAmB,CADvB,EAC0B,MAAM,sCADhC;;AAEA,MAAI,QAAQ,CAAC,MAAT,KAAoB,CAAxB,EAA2B;AACzB,WAAO,QAAQ,CAAC,CAAD,CAAR,CAAY,UAAZ,CAAuB,IAAvB,CAAP;AACD;;AACD,QAAM,IAAI,GAAG,QAAQ,CAAC,CAAD,CAAR,CAAY,IAAzB;AACA,QAAM,KAAK,GAAG,QAAQ,CAAC,CAAD,CAAR,CAAY,KAA1B;AACA,QAAM,KAAK,GAAG,QAAQ,CAAC,CAAD,CAAR,CAAY,KAA1B;AAEA,EAAA,IAAI,CAAC,MAAL,CAAY,IAAI,IAAI,IAApB,EAA0B,MAAM,oCAAhC;AAEA,EAAA,QAAQ,CAAC,OAAT,CAAiB,CAAC,IAAG;AACnB,IAAA,IAAI,CAAC,iBAAL,CACI,KADJ,EACW,CAAC,CAAC,KADb,EAEI,uDAFJ;AAGD,GAJD;AAMA,EAAA,QAAQ,CAAC,OAAT,CAAiB,CAAC,IAAG;AACnB,IAAA,IAAI,CAAC,MAAL,CACI,KAAK,KAAK,CAAC,CAAC,KADhB,EAEI,MAAM,uDAFV;AAGD,GAJD;AAKA,QAAM,eAAe,GAAG,QAAQ,CAAC,GAAT,CAAa,CAAC,IAAI,CAAC,CAAC,UAAF,CAAa,IAAb,CAAlB,CAAxB;AACA,SAAO,MAAM,CAAC,eAAD,EAAkB,IAAlB,CAAb;AACD;AAED;;;;;;;;;;;;;AAYA;;;AACA,SAAS,QAAT,CAAkB,CAAlB,EAAwC,IAAI,GAAG,CAA/C,EAAgD;AAC9C,EAAA,IAAI,GAAG,IAAI,IAAI,CAAf;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAA1B;AACA,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,IAAI,CAAC,EAAE,CAAC,KAAH,CAAS,MAAlB,IAA4B,IAAI,GAAG,EAAE,CAAC,KAAH,CAAS,MADhD,EAEI,MACI,UAAU,IAAI,gBAAgB,EAAE,CAAC,KAAH,CAAS,MAAM,KAAK,EAAE,CAAC,KAAH,CAAS,MAAM,GAHzE;;AAIA,MAAI,IAAI,GAAG,CAAX,EAAc;AACZ,IAAA,IAAI,IAAI,EAAE,CAAC,KAAH,CAAS,MAAjB;AACD;;AACD,QAAM,IAAI,GAAI,EAAD,IAAiB;AAC5B,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,KAAK,CAAC,EAAD,EAAK,IAAL;AAAf,KAAP;AACD,GAFD;;AAGA,QAAM,KAAK,GAAG;AAAC,IAAA;AAAD,GAAd;AACA,SAAO,MAAM,CAAC,aAAP,CACH,OAAO,IAAI,OAAO,CAAC,OAAR,CAAgB,EAAhB,EAAoB,IAApB,CADR,EACmC;AAAC,IAAA,CAAC,EAAE;AAAJ,GADnC,EAC4C,IAD5C,EACkD,QADlD,EAC4D,KAD5D,CAAP;AAED;AAED;;;;;;;;;;;;;;;AAcA;;;AACA,SAAS,WAAT,CACI,CADJ,EAC0B,IAAI,GAAG,CADjC,EACkC;AAChC,QAAM,OAAO,GAAa,IAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,YAAT,EAAuB,OAAvB,CAA1B;AAEA,EAAA,IAAI,CAAC,MAAL,CAAY,IAAI,IAAI,EAAE,CAAC,IAAvB,EAA6B,MAAM,oCAAnC;AACA,QAAM,QAAQ,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,EAAjB;;AACA,MAAI,IAAI,GAAG,CAAX,EAAc;AACZ;AACA,IAAA,IAAI,CAAC,MAAL,CACI,EAAE,EAAE,CAAC,IAAH,GAAU,CAAZ,KAAkB,IADtB,EAEI,MAAM,iCAAiC,EAAG,EAAE,CAAC,IAAH,GAAU,CAAb,CAAe,KAAK,EAAE,CAAC,IAAI,GAFtE;AAGA,IAAA,IAAI,GAAG,EAAE,CAAC,IAAH,GAAU,IAAV,GAAiB,CAAxB;AACD;;AACD,EAAA,QAAQ,CAAC,MAAT,CAAgB,IAAhB,EAAsB,CAAtB,EAAyB,CAAzB;AACA,SAAO,OAAO,CAAC,EAAD,EAAK,QAAL,CAAd;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA;;;AACA,eAAe,eAAf,CACI,CADJ,EAC0B,CAD1B,EAC8C;AAC5C,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AACA,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AAEA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,KAAH,KAAa,EAAE,CAAC,KADpB,EAEI,MAAM,kDACF,EAAE,CAAC,KAAK,YAAY,EAAE,CAAC,KAAK,IAHpC;AAKA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,KAAY,CADhB,EACmB,MAAM,qCAAqC,EAAE,CAAC,KAAK,IADtE;AAGA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,KAAY,CADhB,EACmB,MAAM,qCAAqC,EAAE,CAAC,KAAK,IADtE;AAGA,QAAM,KAAK,GAAG,MAAM,EAAE,CAAC,IAAH,EAApB;AACA,QAAM,KAAK,GAAG,MAAM,EAAE,CAAC,IAAH,EAApB;AACA,QAAM,IAAI,GAAG,IAAI,GAAJ,CAAQ,KAAR,CAAb;AAEA,MAAI,UAAU,GAAG,CAAjB;;AACA,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,CAAC,MAA1B,EAAkC,CAAC,EAAnC,EAAuC;AACrC,QAAI,CAAC,IAAI,CAAC,GAAL,CAAS,KAAK,CAAC,CAAD,CAAd,CAAL,EAAyB;AACvB,MAAA,UAAU;AACX;AACF;;AAED,QAAM,MAAM,GAAG,IAAI,YAAJ,CAAiB,CAAC,UAAD,CAAjB,EAA+B,EAAE,CAAC,KAAlC,CAAf;AACA,QAAM,OAAO,GAAG,IAAI,YAAJ,CAAiB,CAAC,UAAD,CAAjB,EAA+B,OAA/B,CAAhB;;AACA,OAAK,IAAI,CAAC,GAAG,CAAR,EAAW,CAAC,GAAG,CAApB,EAAuB,CAAC,GAAG,KAAK,CAAC,MAAjC,EAAyC,CAAC,EAA1C,EAA8C;AAC5C,QAAI,CAAC,IAAI,CAAC,GAAL,CAAS,KAAK,CAAC,CAAD,CAAd,CAAL,EAAyB;AACvB,MAAA,MAAM,CAAC,MAAP,CAAc,CAAd,IAAmB,KAAK,CAAC,CAAD,CAAxB;AACA,MAAA,OAAO,CAAC,MAAR,CAAe,CAAf,IAAoB,CAApB;AACA,MAAA,CAAC;AACF;AACF;;AACD,SAAO,CAAC,MAAM,CAAC,QAAP,EAAD,EAAoB,OAAO,CAAC,QAAR,EAApB,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;AAwBA;;;AACA,OAAM,SAAU,MAAV,CACF,KADE,EACkB,KAAA,GAAW,SAD7B,EAEF,MAFE,EAEqB;AACzB,EAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;AACA,EAAA,IAAI,CAAC,kCAAL,CAAwC,KAAxC;AACA,SAAO,IAAI,YAAJ,CAAuB,KAAvB,EAA8B,KAA9B,EAAqC,MAArC,CAAP;AACD;AAED;;;;;;;;;;;;AAWA;;AACA,SAAS,KAAT,CAAiC,CAAjC,EAAuC,OAAO,GAAG,KAAjD,EAAsD;AACpD,EAAA,OAAO,CAAC,GAAR,CAAY,CAAC,CAAC,QAAF,CAAW,OAAX,CAAZ;AACD;;AAED,SACE,KADF,CACS;AADT;AAIA,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,UAAU,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAArB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,KAAK,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAhB;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,cAAc,GAAG,eAAvB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { TensorBuffer } from '../tensor';\nimport { convertToTensor, convertToTensorArray } from '../tensor_util_env';\nimport * as util from '../util';\nimport { concat } from './concat';\nimport { op } from './operation';\n/**\n * Reshapes a `tf.Tensor` to a given shape.\n *\n * Given an input tensor, returns a new tensor with the same values as the\n * input tensor with shape `shape`.\n *\n * If one component of shape is the special value -1, the size of that\n * dimension is computed so that the total size remains constant. In\n * particular, a shape of [-1] flattens into 1-D. At most one component of\n * shape can be -1.\n *\n * If shape is 1-D or higher, then the operation returns a tensor with shape\n * shape filled with the values of tensor. In this case, the number of\n * elements implied by shape must be the same as the number of elements in\n * tensor.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * x.reshape([2, 2]).print();\n * ```\n *\n * @param x The input tensor to be reshaped.\n * @param shape An array of integers defining the output tensor shape.\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nfunction reshape_(x, shape) {\n    const $x = convertToTensor(x, 'x', 'reshape', null);\n    shape = util.inferFromImplicitShape(shape, $x.size);\n    util.assert($x.size === util.sizeFromShape(shape), () => 'new shape and old shape must have the same number of elements.');\n    const grad = (dy) => {\n        return { x: () => dy.reshape($x.shape) };\n    };\n    const attrs = { shape };\n    return ENGINE.runKernelFunc(backend => backend.reshape($x, shape), { x: $x }, grad, 'Reshape', attrs);\n}\n/**\n * Removes dimensions of size 1 from the shape of a `tf.Tensor`.\n *\n * ```js\n * const x = tf.tensor([1, 2, 3, 4], [1, 1, 4]);\n * x.squeeze().print();\n * ```\n *\n * @param x The input tensor to be squeezed.\n * @param axis An optional list of numbers. If specified, only\n *     squeezes the dimensions listed. The dimension index starts at 0. It\n * is an error to squeeze a dimension that is not 1.\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nfunction squeeze_(x, axis) {\n    const $x = convertToTensor(x, 'x', 'squeeze');\n    return reshape($x, util.squeezeShape($x.shape, axis).newShape);\n}\n/**\n * Casts a `tf.Tensor` to a new dtype.\n *\n * ```js\n * const x = tf.tensor1d([1.5, 2.5, 3]);\n * tf.cast(x, 'int32').print();\n * ```\n * @param x The input tensor to be casted.\n * @param dtype The dtype to cast the input tensor to.\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nfunction cast_(x, dtype) {\n    const $x = convertToTensor(x, 'x', 'cast');\n    // Sanity checks.\n    if (!util.isValidDtype(dtype)) {\n        throw new Error(`Failed to cast to unknown dtype ${dtype}`);\n    }\n    if (dtype === 'string' && $x.dtype !== 'string' ||\n        dtype !== 'string' && $x.dtype === 'string') {\n        throw new Error('Only strings can be casted to strings');\n    }\n    const grad = (dy) => {\n        return { x: () => dy.clone() };\n    };\n    const attrs = { dtype };\n    return ENGINE.runKernelFunc(backend => backend.cast($x, dtype), { x: $x }, grad, 'Cast', attrs);\n}\n/**\n * Stacks a list of rank-`R` `tf.Tensor`s into one rank-`(R+1)` `tf.Tensor`.\n *\n * ```js\n * const a = tf.tensor1d([1, 2]);\n * const b = tf.tensor1d([3, 4]);\n * const c = tf.tensor1d([5, 6]);\n * tf.stack([a, b, c]).print();\n * ```\n *\n * @param tensors A list of tensor objects with the same shape and dtype.\n * @param axis The axis to stack along. Defaults to 0 (the first dim).\n */\n/** @doc {heading: 'Tensors', subheading: 'Slicing and Joining'} */\nfunction stack_(tensors, axis = 0) {\n    const $tensors = convertToTensorArray(tensors, 'tensors', 'stack');\n    util.assert($tensors.length >= 1, () => 'Pass at least one tensor to tf.stack');\n    if ($tensors.length === 1) {\n        return $tensors[0].expandDims(axis);\n    }\n    const rank = $tensors[0].rank;\n    const shape = $tensors[0].shape;\n    const dtype = $tensors[0].dtype;\n    util.assert(axis <= rank, () => 'Axis must be <= rank of the tensor');\n    $tensors.forEach(t => {\n        util.assertShapesMatch(shape, t.shape, 'All tensors passed to stack must have matching shapes');\n    });\n    $tensors.forEach(t => {\n        util.assert(dtype === t.dtype, () => 'All tensors passed to stack must have matching dtypes');\n    });\n    const expandedTensors = $tensors.map(t => t.expandDims(axis));\n    return concat(expandedTensors, axis);\n}\n/**\n * Unstacks a `tf.Tensor` of rank-`R` into a list of rank-`(R-1)` `tf.Tensor`s.\n *\n * ```js\n * const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * tf.unstack(a).forEach(tensor => tensor.print());\n * ```\n *\n * @param x A tensor object.\n * @param axis The axis to unstack along. Defaults to 0 (the first dim).\n */\n/** @doc {heading: 'Tensors', subheading: 'Slicing and Joining'} */\nfunction unstack_(x, axis = 0) {\n    axis = axis || 0;\n    const $x = convertToTensor(x, 'x', 'unstack');\n    util.assert(axis >= -$x.shape.length && axis < $x.shape.length, () => `Axis = ${axis} is not in [-${$x.shape.length}, ${$x.shape.length})`);\n    if (axis < 0) {\n        axis += $x.shape.length;\n    }\n    const grad = (dy) => {\n        return { x: () => stack(dy, axis) };\n    };\n    const attrs = { axis };\n    return ENGINE.runKernelFunc(backend => backend.unstack($x, axis), { x: $x }, grad, 'Unpack', attrs);\n}\n/**\n * Returns a `tf.Tensor` that has expanded rank, by inserting a dimension\n * into the tensor's shape.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * const axis = 1;\n * x.expandDims(axis).print();\n * ```\n *\n * @param x The input tensor whose dimensions to be expanded.\n * @param axis The dimension index at which to insert shape of `1`. Defaults\n *     to 0 (the first dimension).\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nfunction expandDims_(x, axis = 0) {\n    const parseAs = null;\n    const $x = convertToTensor(x, 'x', 'expandDims', parseAs);\n    util.assert(axis <= $x.rank, () => 'Axis must be <= rank of the tensor');\n    const newShape = $x.shape.slice();\n    if (axis < 0) {\n        // Negative value is counted from the tail of rank.\n        util.assert(-($x.rank + 1) <= axis, () => `Axis must be in the interval [${-($x.rank + 1)}, ${$x.rank}]`);\n        axis = $x.rank + axis + 1;\n    }\n    newShape.splice(axis, 0, 1);\n    return reshape($x, newShape);\n}\n/**\n * Computes the difference between two lists of numbers.\n *\n * Given a Tensor `x` and a Tensor `y`, this operation returns a Tensor `out`\n * that represents all values that are in `x` but not in `y`. The returned\n * Tensor `out` is sorted in the same order that the numbers appear in `x`\n * (duplicates are preserved). This operation also returns a Tensor indices that\n * represents the position of each out element in `x`. In other words:\n *\n * `out[i] = x[idx[i]] for i in [0, 1, ..., out.length - 1]`\n *\n * ```js\n * const x = [1, 2, 3, 4, 5, 6];\n * const y = [1, 3, 5];\n *\n * const [out, indices] = await tf.setdiff1dAsync(x, y);\n * out.print(); // [2, 4, 6]\n * indices.print(); // [1, 3, 5]\n * ```\n *\n * @param x 1-D Tensor. Values to keep.\n * @param y 1-D Tensor. Must have the same type as x. Values to exclude in the\n *     output.\n * @returns Promise of Tensor tuple [out, indices].\n *  out: Tensor with the same type as x.\n *  indices: A Tensor of type int32.\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nasync function setdiff1dAsync_(x, y) {\n    const $x = convertToTensor(x, 'x', 'setdiff1d');\n    const $y = convertToTensor(y, 'y', 'setdiff1d');\n    util.assert($x.dtype === $y.dtype, () => `x and y should have the same dtype, but got x (${$x.dtype}) and y (${$y.dtype}).`);\n    util.assert($x.rank === 1, () => `x should be 1D tensor, but got x (${$x.shape}).`);\n    util.assert($y.rank === 1, () => `y should be 1D tensor, but got y (${$y.shape}).`);\n    const xVals = await $x.data();\n    const yVals = await $y.data();\n    const ySet = new Set(yVals);\n    let outputSize = 0;\n    for (let i = 0; i < xVals.length; i++) {\n        if (!ySet.has(xVals[i])) {\n            outputSize++;\n        }\n    }\n    const buffer = new TensorBuffer([outputSize], $x.dtype);\n    const indices = new TensorBuffer([outputSize], 'int32');\n    for (let i = 0, p = 0; i < xVals.length; i++) {\n        if (!ySet.has(xVals[i])) {\n            buffer.values[p] = xVals[i];\n            indices.values[p] = i;\n            p++;\n        }\n    }\n    return [buffer.toTensor(), indices.toTensor()];\n}\n/**\n * Creates an empty `tf.TensorBuffer` with the specified `shape` and `dtype`.\n *\n * The values are stored in CPU as `TypedArray`. Fill the buffer using\n * `buffer.set()`, or by modifying directly `buffer.values`.\n *\n * When done, call `buffer.toTensor()` to get an immutable `tf.Tensor` with\n * those values.\n *\n * ```js\n * // Create a buffer and set values at particular indices.\n * const buffer = tf.buffer([2, 2]);\n * buffer.set(3, 0, 0);\n * buffer.set(5, 1, 0);\n *\n * // Convert the buffer back to a tensor.\n * buffer.toTensor().print();\n * ```\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param dtype The dtype of the buffer. Defaults to 'float32'.\n * @param values The values of the buffer as `TypedArray`. Defaults to\n * zeros.\n */\n/** @doc {heading: 'Tensors', subheading: 'Creation'} */\nexport function buffer(shape, dtype = 'float32', values) {\n    dtype = dtype || 'float32';\n    util.assertNonNegativeIntegerDimensions(shape);\n    return new TensorBuffer(shape, dtype, values);\n}\n/**\n * Prints information about the `tf.Tensor` including its data.\n *\n * ```js\n * const verbose = true;\n * tf.tensor2d([1, 2, 3, 4], [2, 2]).print(verbose);\n * ```\n * @param x The tensor to be printed.\n * @param verbose Whether to print verbose information about the ` Tensor`,\n * including dtype and size.\n */\n/** @doc {heading: 'Tensors', subheading: 'Creation'} */\nfunction print(x, verbose = false) {\n    console.log(x.toString(verbose));\n}\nexport { print // Not wrapped in op() since no need to increase stack trace.\n };\nexport const cast = op({ cast_ });\nexport const expandDims = op({ expandDims_ });\nexport const reshape = op({ reshape_ });\nexport const squeeze = op({ squeeze_ });\nexport const stack = op({ stack_ });\nexport const unstack = op({ unstack_ });\nexport const setdiff1dAsync = setdiff1dAsync_;\n//# sourceMappingURL=array_ops.js.map"]},"metadata":{},"sourceType":"module"}