{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as axis_util from './axis_util';\nimport { op } from './operation';\nimport { gradForMinAndMax } from './reduction_ops_util';\nimport { ones, scalar, zerosLike } from './tensor_ops';\n/**\n * Computes the log(sum(exp(elements across the reduction dimensions)).\n *\n * Reduces the input along the dimensions given in `axis`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.logSumExp().print();  // or tf.logSumExp(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)\n * ```\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. If null (the default),\n *     reduces all dimensions.\n * @param keepDims If true, retains reduced dimensions with length\n *     of 1. Defaults to false.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\nfunction logSumExp_(x, axis = null, keepDims = false) {\n  const $x = convertToTensor(x, 'x', 'logSumExp');\n  const axes = util.parseAxisParam(axis, $x.shape);\n  const xMax = $x.max(axes, true\n  /* keepDims */\n  );\n  const a = $x.sub(xMax);\n  const b = a.exp();\n  const c = b.sum(axes);\n  const d = c.log();\n  const res = xMax.reshape(d.shape).add(d);\n\n  if (keepDims) {\n    const newShape = axis_util.expandShapeToKeepDim(res.shape, axes);\n    return res.reshape(newShape);\n  }\n\n  return res;\n}\n/**\n * Computes the sum of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If axes has no entries, all dimensions are reduced, and a\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.sum().print();  // or tf.sum(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.sum(axis).print();  // or tf.sum(x, axis)\n * ```\n *\n * @param x The input tensor to compute the sum over. If the dtype is `bool`\n *   it will be converted to `int32` and the output dtype will be `int32`.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction sum_(x, axis = null, keepDims = false) {\n  let $x = convertToTensor(x, 'x', 'sum');\n\n  if ($x.dtype === 'bool') {\n    $x = $x.toInt();\n  }\n\n  const axes = util.parseAxisParam(axis, $x.shape); // Use a custom gradient to bypass 2 gradient backprops since sum is used\n  // extremely often.\n\n  const customOp = customGrad(x => {\n    const permutation = axis_util.getAxesPermutation(axes, x.rank);\n    let reductionAxes = axes;\n    let permutedX = x;\n\n    if (permutation != null) {\n      permutedX = x.transpose(permutation);\n      reductionAxes = axis_util.getInnerMostAxes(reductionAxes.length, x.rank);\n    }\n\n    const gradFunc = dy => {\n      const expandedDyShape = x.shape.slice();\n      axes.forEach(axis => {\n        expandedDyShape[axis] = 1;\n      });\n      const expandedDy = dy.reshape(expandedDyShape);\n      const derX = expandedDy.mul(ones(x.shape, 'float32'));\n      return derX;\n    };\n\n    const gradInputs = dy => {\n      return {\n        x: () => gradFunc(dy)\n      };\n    };\n\n    const attrs = {\n      axes: reductionAxes\n    };\n    let value = ENGINE.runKernelFunc(backend => backend.sum(permutedX, reductionAxes), {\n      x: permutedX\n    }, gradInputs, 'Sum', attrs);\n\n    if (keepDims) {\n      const newShape = axis_util.expandShapeToKeepDim(value.shape, axes);\n      value = value.reshape(newShape);\n    }\n\n    return {\n      value,\n      gradFunc\n    };\n  });\n  return customOp($x);\n}\n/**\n * Computes the product of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and a\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.prod().print();  // or tf.prod(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.prod(axis).print();  // or tf.prod(x, axis)\n * ```\n *\n * @param x The input tensor to compute the product over. If the dtype is `bool`\n *   it will be converted to `int32` and the output dtype will be `int32`.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction prod_(x, axis = null, keepDims = false) {\n  let $x = convertToTensor(x, 'x', 'prod');\n\n  if ($x.dtype === 'bool') {\n    $x = $x.toInt();\n  }\n\n  const axes = util.parseAxisParam(axis, $x.shape);\n  const permutation = axis_util.getAxesPermutation(axes, $x.rank);\n  let reductionAxes = axes;\n  let permutedX = $x;\n\n  if (permutation != null) {\n    permutedX = $x.transpose(permutation);\n    reductionAxes = axis_util.getInnerMostAxes(reductionAxes.length, $x.rank);\n  }\n\n  let value = ENGINE.runKernelFunc(backend => backend.prod(permutedX, reductionAxes), {\n    permutedX\n  });\n\n  if (keepDims) {\n    const newShape = axis_util.expandShapeToKeepDim(value.shape, axes);\n    value = value.reshape(newShape);\n  }\n\n  return value;\n}\n/**\n * Computes the mean of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is\n * true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with\n * a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.mean().print();  // or tf.mean(a)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.mean(axis).print();  // or tf.mean(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction mean_(x, axis = null, keepDims = false) {\n  const $x = convertToTensor(x, 'x', 'mean');\n  const axes = util.parseAxisParam(axis, $x.shape);\n  const shapes = axis_util.computeOutAndReduceShapes($x.shape, axes);\n  const reduceShape = shapes[1];\n  const reduceSize = util.sizeFromShape(reduceShape); // Use a custom gradient to bypass 2 gradient backprops since mean is used\n  // extremely often.\n\n  const customOp = customGrad(x => {\n    const reduceSizeScalar = scalar(reduceSize); // Cast if needed.\n\n    const xReduce = reduceSizeScalar.dtype === x.dtype ? x : x.cast(reduceSizeScalar.dtype);\n    const res = xReduce.div(reduceSizeScalar);\n    const value = res.sum(axis, keepDims);\n\n    const gradFunc = dy => {\n      const expandedDyShape = x.shape.slice();\n      axes.forEach(axis => {\n        expandedDyShape[axis] = 1;\n      });\n      const expandedDy = dy.reshape(expandedDyShape);\n      const derX = expandedDy.mul(ones(x.shape, 'float32')).div(reduceSize);\n      return derX;\n    };\n\n    return {\n      value,\n      gradFunc\n    };\n  });\n  return customOp($x);\n}\n/**\n * Computes the minimum value from the input.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axes`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axes` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.min().print();  // or tf.min(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.min(axis).print();  // or tf.min(x, axis)\n * ```\n *\n * @param x The input Tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction min_(x, axis = null, keepDims = false) {\n  let $x = convertToTensor(x, 'x', 'min');\n  const xOrig = $x;\n  const origAxes = util.parseAxisParam(axis, $x.shape);\n  let axes = origAxes;\n  const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n\n  if (permutedAxes != null) {\n    $x = $x.transpose(permutedAxes);\n    axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n  }\n\n  const grad = (dy, saved) => gradForMinAndMax(dy, saved[1], saved[0], origAxes, permutedAxes);\n\n  const inputsToSave = [$x];\n  const outputsToSave = [true];\n  let res = ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.min($x, axes);\n    save([xOrig, y]);\n    return y;\n  }, {\n    x: $x\n  }, grad, 'Min', {\n    axes\n  }, inputsToSave, outputsToSave);\n\n  if (keepDims) {\n    const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n    res = res.reshape(newShape);\n  }\n\n  return res;\n}\n/**\n * Returns the indices of the minimum values along an `axis`.\n *\n * The result has the same shape as `input` with the dimension along `axis`\n * removed.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.argMin().print();  // or tf.argMin(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);\n *\n * const axis = 1;\n * x.argMin(axis).print();  // or tf.argMin(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).\n *\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction argMin_(x, axis = 0) {\n  let $x = convertToTensor(x, 'x', 'argMin');\n\n  if (axis == null) {\n    axis = 0;\n  }\n\n  let axes = util.parseAxisParam(axis, $x.shape);\n  const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n\n  if (permutedAxes != null) {\n    $x = $x.transpose(permutedAxes);\n    axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n  }\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      $x: () => zerosLike($x)\n    };\n  };\n\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.argMin($x, axes[0]);\n    save([$x]);\n    return res;\n  }, {\n    $x\n  }, grad);\n}\n/**\n * Returns the indices of the maximum values along an `axis`.\n *\n * The result has the same shape as `input` with the dimension along `axis`\n * removed.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.argMax().print();  // or tf.argMax(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);\n *\n * const axis = 1;\n * x.argMax(axis).print();  // or tf.argMax(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction argMax_(x, axis = 0) {\n  let $x = convertToTensor(x, 'x', 'argMax');\n\n  if (axis == null) {\n    axis = 0;\n  }\n\n  let axes = util.parseAxisParam(axis, $x.shape);\n  const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n\n  if (permutedAxes != null) {\n    $x = $x.transpose(permutedAxes);\n    axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n  }\n\n  const grad = (dy, saved) => {\n    const [$x] = saved;\n    return {\n      x: () => zerosLike($x)\n    };\n  };\n\n  const attrs = {\n    axis: axes[0]\n  };\n  const inputsToSave = [$x];\n  return ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.argMax($x, axes[0]);\n    save([$x]);\n    return res;\n  }, {\n    x: $x\n  }, grad, 'ArgMax', attrs, inputsToSave);\n}\n/**\n * Computes the logical and of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 1, 1], 'bool');\n *\n * x.all().print();  // or tf.all(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');\n *\n * const axis = 1;\n * x.all(axis).print();  // or tf.all(x, axis)\n * ```\n *\n * @param x The input tensor. Must be of dtype bool.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction all_(x, axis = null, keepDims = false) {\n  let $x = convertToTensor(x, 'x', 'all', 'bool');\n  const origAxes = util.parseAxisParam(axis, $x.shape);\n  let axes = origAxes;\n  const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n\n  if (permutedAxes != null) {\n    $x = $x.transpose(permutedAxes);\n    axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n  }\n\n  const res = ENGINE.runKernelFunc(backend => backend.all($x, axes), {\n    $x\n  });\n\n  if (keepDims) {\n    const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n    return res.reshape(newShape);\n  }\n\n  return res;\n}\n/**\n * Computes the logical or of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 1, 1], 'bool');\n *\n * x.any().print();  // or tf.any(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');\n *\n * const axis = 1;\n * x.any(axis).print();  // or tf.any(x, axis)\n * ```\n *\n * @param x The input tensor. Must be of dtype bool.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\n\n\nfunction any_(x, axis = null, keepDims = false) {\n  let $x = convertToTensor(x, 'x', 'any', 'bool');\n  const origAxes = util.parseAxisParam(axis, $x.shape);\n  let axes = origAxes;\n  const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n\n  if (permutedAxes != null) {\n    $x = $x.transpose(permutedAxes);\n    axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n  }\n\n  const res = ENGINE.runKernelFunc(backend => backend.any($x, axes), {\n    $x\n  });\n\n  if (keepDims) {\n    const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n    return res.reshape(newShape);\n  }\n\n  return res;\n}\n/**\n * Calculates the mean and variance of `x`. The mean and variance are\n * calculated by aggregating the contents of `x` across `axes`. If `x` is\n * 1-D and `axes = [0]` this is just the mean and variance of a vector.\n *\n * @param x The input tensor.\n * @param axis The dimension(s) along with to compute mean and\n *     variance. By default it reduces all dimensions.\n * @param keepDims If true, the moments have the same dimensionality as the\n *     input.\n * @return An object with two keys: `mean` and `variance`.\n */\n\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\n\n\nfunction moments_(x, axis = null, keepDims = false) {\n  x = convertToTensor(x, 'x', 'moments');\n  const axes = util.parseAxisParam(axis, x.shape);\n  const mean = x.mean(axes, keepDims);\n  let keepDimsShape = mean.shape;\n\n  if (!keepDims) {\n    keepDimsShape = axis_util.expandShapeToKeepDim(mean.shape, axes);\n  }\n\n  const devSquared = x.toFloat().sub(mean.reshape(keepDimsShape)).square();\n  const variance = devSquared.mean(axes, keepDims);\n  return {\n    mean,\n    variance\n  };\n}\n\nexport const all = op({\n  all_\n}); // tslint:disable-next-line:variable-name\n\nexport const any = op({\n  any_\n});\nexport const argMax = op({\n  argMax_\n});\nexport const argMin = op({\n  argMin_\n});\nexport const logSumExp = op({\n  logSumExp_\n});\nexport const mean = op({\n  mean_\n});\nexport const min = op({\n  min_\n});\nexport const moments = op({\n  moments_\n});\nexport const sum = op({\n  sum_\n});\nexport const prod = op({\n  prod_\n});","map":{"version":3,"sources":["../../src/ops/reduction_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,UAAR,QAAyB,cAAzB;AAEA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,SAAtB;AAEA,OAAO,KAAK,SAAZ,MAA2B,aAA3B;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,gBAAR,QAA+B,sBAA/B;AACA,SAAQ,IAAR,EAAc,MAAd,EAAsB,SAAtB,QAAsC,cAAtC;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA;;AACA,SAAS,UAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,WAAT,CAA1B;AAEA,QAAM,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAb;AACA,QAAM,IAAI,GAAG,EAAE,CAAC,GAAH,CAAO,IAAP,EAAa;AAAK;AAAlB,GAAb;AACA,QAAM,CAAC,GAAG,EAAE,CAAC,GAAH,CAAO,IAAP,CAAV;AACA,QAAM,CAAC,GAAG,CAAC,CAAC,GAAF,EAAV;AACA,QAAM,CAAC,GAAG,CAAC,CAAC,GAAF,CAAM,IAAN,CAAV;AACA,QAAM,CAAC,GAAG,CAAC,CAAC,GAAF,EAAV;AACA,QAAM,GAAG,GAAG,IAAI,CAAC,OAAL,CAAa,CAAC,CAAC,KAAf,EAAsB,GAAtB,CAA0B,CAA1B,CAAZ;;AAEA,MAAI,QAAJ,EAAc;AACZ,UAAM,QAAQ,GAAG,SAAS,CAAC,oBAAV,CAA+B,GAAG,CAAC,KAAnC,EAA0C,IAA1C,CAAjB;AACA,WAAO,GAAG,CAAC,OAAJ,CAAY,QAAZ,CAAP;AACD;;AACD,SAAO,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BA;;;AACA,SAAS,IAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,MAAjB,EAAyB;AACvB,IAAA,EAAE,GAAG,EAAE,CAAC,KAAH,EAAL;AACD;;AACD,QAAM,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAb,CANsE,CAQtE;AACA;;AACA,QAAM,QAAQ,GAAG,UAAU,CAAE,CAAD,IAAc;AACxC,UAAM,WAAW,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,CAAC,CAAC,IAArC,CAApB;AACA,QAAI,aAAa,GAAG,IAApB;AACA,QAAI,SAAS,GAAG,CAAhB;;AACA,QAAI,WAAW,IAAI,IAAnB,EAAyB;AACvB,MAAA,SAAS,GAAG,CAAC,CAAC,SAAF,CAAY,WAAZ,CAAZ;AACA,MAAA,aAAa,GAAG,SAAS,CAAC,gBAAV,CAA2B,aAAa,CAAC,MAAzC,EAAiD,CAAC,CAAC,IAAnD,CAAhB;AACD;;AAED,UAAM,QAAQ,GAAI,EAAD,IAAe;AAC9B,YAAM,eAAe,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,EAAxB;AACA,MAAA,IAAI,CAAC,OAAL,CAAa,IAAI,IAAG;AAClB,QAAA,eAAe,CAAC,IAAD,CAAf,GAAwB,CAAxB;AACD,OAFD;AAGA,YAAM,UAAU,GAAG,EAAE,CAAC,OAAH,CAAW,eAAX,CAAnB;AACA,YAAM,IAAI,GAAG,UAAU,CAAC,GAAX,CAAe,IAAI,CAAC,CAAC,CAAC,KAAH,EAAU,SAAV,CAAnB,CAAb;AACA,aAAO,IAAP;AACD,KARD;;AAUA,UAAM,UAAU,GAAI,EAAD,IAAe;AAChC,aAAO;AAAC,QAAA,CAAC,EAAE,MAAM,QAAQ,CAAC,EAAD;AAAlB,OAAP;AACD,KAFD;;AAIA,UAAM,KAAK,GAAG;AAAC,MAAA,IAAI,EAAE;AAAP,KAAd;AACA,QAAI,KAAK,GAAG,MAAM,CAAC,aAAP,CACR,OAAO,IAAI,OAAO,CAAC,GAAR,CAAY,SAAZ,EAAuB,aAAvB,CADH,EAC0C;AAAC,MAAA,CAAC,EAAE;AAAJ,KAD1C,EAER,UAFQ,EAEI,KAFJ,EAEW,KAFX,CAAZ;;AAIA,QAAI,QAAJ,EAAc;AACZ,YAAM,QAAQ,GAAG,SAAS,CAAC,oBAAV,CAA+B,KAAK,CAAC,KAArC,EAA4C,IAA5C,CAAjB;AACA,MAAA,KAAK,GAAG,KAAK,CAAC,OAAN,CAAc,QAAd,CAAR;AACD;;AAED,WAAO;AAAC,MAAA,KAAD;AAAQ,MAAA;AAAR,KAAP;AACD,GAlC0B,CAA3B;AAoCA,SAAO,QAAQ,CAAC,EAAD,CAAf;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BA;;;AACA,SAAS,KAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAAxB;;AAEA,MAAI,EAAE,CAAC,KAAH,KAAa,MAAjB,EAAyB;AACvB,IAAA,EAAE,GAAG,EAAE,CAAC,KAAH,EAAL;AACD;;AACD,QAAM,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAb;AAEA,QAAM,WAAW,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,EAAE,CAAC,IAAtC,CAApB;AACA,MAAI,aAAa,GAAG,IAApB;AACA,MAAI,SAAS,GAAG,EAAhB;;AACA,MAAI,WAAW,IAAI,IAAnB,EAAyB;AACvB,IAAA,SAAS,GAAG,EAAE,CAAC,SAAH,CAAa,WAAb,CAAZ;AACA,IAAA,aAAa,GAAG,SAAS,CAAC,gBAAV,CAA2B,aAAa,CAAC,MAAzC,EAAiD,EAAE,CAAC,IAApD,CAAhB;AACD;;AACD,MAAI,KAAK,GAAG,MAAM,CAAC,aAAP,CACR,OAAO,IAAI,OAAO,CAAC,IAAR,CAAa,SAAb,EAAwB,aAAxB,CADH,EAC2C;AAAC,IAAA;AAAD,GAD3C,CAAZ;;AAEA,MAAI,QAAJ,EAAc;AACZ,UAAM,QAAQ,GAAG,SAAS,CAAC,oBAAV,CAA+B,KAAK,CAAC,KAArC,EAA4C,IAA5C,CAAjB;AACA,IAAA,KAAK,GAAG,KAAK,CAAC,OAAN,CAAc,QAAd,CAAR;AACD;;AAED,SAAO,KAAP;AACD;AACD;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA;;;AACA,SAAS,KAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,MAAT,CAA1B;AAEA,QAAM,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAb;AACA,QAAM,MAAM,GAAG,SAAS,CAAC,yBAAV,CAAoC,EAAE,CAAC,KAAvC,EAA8C,IAA9C,CAAf;AACA,QAAM,WAAW,GAAG,MAAM,CAAC,CAAD,CAA1B;AACA,QAAM,UAAU,GAAG,IAAI,CAAC,aAAL,CAAmB,WAAnB,CAAnB,CANsE,CAQtE;AACA;;AACA,QAAM,QAAQ,GAAG,UAAU,CAAE,CAAD,IAAc;AACxC,UAAM,gBAAgB,GAAG,MAAM,CAAC,UAAD,CAA/B,CADwC,CAExC;;AACA,UAAM,OAAO,GACT,gBAAgB,CAAC,KAAjB,KAA2B,CAAC,CAAC,KAA7B,GAAqC,CAArC,GAAyC,CAAC,CAAC,IAAF,CAAO,gBAAgB,CAAC,KAAxB,CAD7C;AAEA,UAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,gBAAZ,CAAZ;AACA,UAAM,KAAK,GAAG,GAAG,CAAC,GAAJ,CAAQ,IAAR,EAAc,QAAd,CAAd;;AAEA,UAAM,QAAQ,GAAI,EAAD,IAAe;AAC9B,YAAM,eAAe,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,EAAxB;AACA,MAAA,IAAI,CAAC,OAAL,CAAa,IAAI,IAAG;AAClB,QAAA,eAAe,CAAC,IAAD,CAAf,GAAwB,CAAxB;AACD,OAFD;AAGA,YAAM,UAAU,GAAG,EAAE,CAAC,OAAH,CAAW,eAAX,CAAnB;AACA,YAAM,IAAI,GAAG,UAAU,CAAC,GAAX,CAAe,IAAI,CAAC,CAAC,CAAC,KAAH,EAAU,SAAV,CAAnB,EAAyC,GAAzC,CAA6C,UAA7C,CAAb;AACA,aAAO,IAAP;AACD,KARD;;AASA,WAAO;AAAC,MAAA,KAAD;AAAQ,MAAA;AAAR,KAAP;AACD,GAlB0B,CAA3B;AAoBA,SAAO,QAAQ,CAAC,EAAD,CAAf;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA;;;AACA,SAAS,IAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,CAAxB;AACA,QAAM,KAAK,GAAG,EAAd;AAEA,QAAM,QAAQ,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAjB;AACA,MAAI,IAAI,GAAG,QAAX;AACA,QAAM,YAAY,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,EAAE,CAAC,IAAtC,CAArB;;AACA,MAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,IAAA,EAAE,GAAG,EAAE,CAAC,SAAH,CAAa,YAAb,CAAL;AACA,IAAA,IAAI,GAAG,SAAS,CAAC,gBAAV,CAA2B,IAAI,CAAC,MAAhC,EAAwC,EAAE,CAAC,IAA3C,CAAP;AACD;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KACT,gBAAgB,CAAC,EAAD,EAAK,KAAK,CAAC,CAAD,CAAV,EAAe,KAAK,CAAC,CAAD,CAApB,EAAyB,QAAzB,EAAmC,YAAnC,CADpB;;AAGA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,QAAM,aAAa,GAAc,CAAC,IAAD,CAAjC;AACA,MAAI,GAAG,GAAG,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC/C,UAAM,CAAC,GAAG,OAAO,CAAC,GAAR,CAAY,EAAZ,EAAgB,IAAhB,CAAV;AACA,IAAA,IAAI,CAAC,CAAC,KAAD,EAAQ,CAAR,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GAJS,EAIP;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJO,EAIE,IAJF,EAIQ,KAJR,EAIe;AAAC,IAAA;AAAD,GAJf,EAIuB,YAJvB,EAIqC,aAJrC,CAAV;;AAKA,MAAI,QAAJ,EAAc;AACZ,UAAM,QAAQ,GAAG,SAAS,CAAC,oBAAV,CAA+B,GAAG,CAAC,KAAnC,EAA0C,QAA1C,CAAjB;AACA,IAAA,GAAG,GAAG,GAAG,CAAC,OAAJ,CAAY,QAAZ,CAAN;AACD;;AACD,SAAO,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;AAuBA;;;AACA,SAAS,OAAT,CAAmC,CAAnC,EAAyD,IAAI,GAAG,CAAhE,EAAiE;AAC/D,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,QAAT,CAAxB;;AAEA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,IAAI,GAAG,CAAP;AACD;;AACD,MAAI,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAX;AACA,QAAM,YAAY,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,EAAE,CAAC,IAAtC,CAArB;;AACA,MAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,IAAA,EAAE,GAAG,EAAE,CAAC,SAAH,CAAa,YAAb,CAAL;AACA,IAAA,IAAI,GAAG,SAAS,CAAC,gBAAV,CAA2B,IAAI,CAAC,MAAhC,EAAwC,EAAE,CAAC,IAA3C,CAAP;AACD;;AACD,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,EAAE,EAAE,MAAM,SAAS,CAAC,EAAD;AAApB,KAAP;AACD,GAHD;;AAIA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,MAAR,CAAe,EAAf,EAAmB,IAAI,CAAC,CAAD,CAAvB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA;AAAD,GAJI,EAIE,IAJF,CAAP;AAKD;AAED;;;;;;;;;;;;;;;;;;;;;;;AAsBA;;;AACA,SAAS,OAAT,CAAmC,CAAnC,EAAyD,IAAI,GAAG,CAAhE,EAAiE;AAC/D,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,QAAT,CAAxB;;AAEA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,IAAI,GAAG,CAAP;AACD;;AACD,MAAI,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAX;AACA,QAAM,YAAY,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,EAAE,CAAC,IAAtC,CAArB;;AACA,MAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,IAAA,EAAE,GAAG,EAAE,CAAC,SAAH,CAAa,YAAb,CAAL;AACA,IAAA,IAAI,GAAG,SAAS,CAAC,gBAAV,CAA2B,IAAI,CAAC,MAAhC,EAAwC,EAAE,CAAC,IAA3C,CAAP;AACD;;AACD,QAAM,IAAI,GAAG,CAAC,EAAD,EAAQ,KAAR,KAA2B;AACtC,UAAM,CAAC,EAAD,IAAO,KAAb;AACA,WAAO;AAAC,MAAA,CAAC,EAAE,MAAM,SAAS,CAAC,EAAD;AAAnB,KAAP;AACD,GAHD;;AAIA,QAAM,KAAK,GAAG;AAAC,IAAA,IAAI,EAAE,IAAI,CAAC,CAAD;AAAX,GAAd;AACA,QAAM,YAAY,GAAG,CAAC,EAAD,CAArB;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,CAAC,OAAD,EAAU,IAAV,KAAkB;AAC5C,UAAM,GAAG,GAAG,OAAO,CAAC,MAAR,CAAe,EAAf,EAAmB,IAAI,CAAC,CAAD,CAAvB,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,EAAD,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ;AAAC,IAAA,CAAC,EAAE;AAAJ,GAJI,EAIK,IAJL,EAIW,QAJX,EAIqB,KAJrB,EAI4B,YAJ5B,CAAP;AAKD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA;;;AACA,SAAS,IAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,EAAgB,MAAhB,CAAxB;AAEA,QAAM,QAAQ,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAjB;AACA,MAAI,IAAI,GAAG,QAAX;AACA,QAAM,YAAY,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,EAAE,CAAC,IAAtC,CAArB;;AACA,MAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,IAAA,EAAE,GAAG,EAAE,CAAC,SAAH,CAAa,YAAb,CAAL;AACA,IAAA,IAAI,GAAG,SAAS,CAAC,gBAAV,CAA2B,IAAI,CAAC,MAAhC,EAAwC,EAAE,CAAC,IAA3C,CAAP;AACD;;AACD,QAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,GAAR,CAAY,EAAZ,EAAgB,IAAhB,CAAhC,EAAuD;AAAC,IAAA;AAAD,GAAvD,CAAZ;;AACA,MAAI,QAAJ,EAAc;AACZ,UAAM,QAAQ,GAAG,SAAS,CAAC,oBAAV,CAA+B,GAAG,CAAC,KAAnC,EAA0C,QAA1C,CAAjB;AACA,WAAO,GAAG,CAAC,OAAJ,CAAY,QAAZ,CAAP;AACD;;AACD,SAAO,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BA;;;AACA,SAAS,IAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EACwD,QAAQ,GAAG,KADnE,EACwE;AACtE,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,KAAT,EAAgB,MAAhB,CAAxB;AAEA,QAAM,QAAQ,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,EAAE,CAAC,KAA7B,CAAjB;AACA,MAAI,IAAI,GAAG,QAAX;AACA,QAAM,YAAY,GAAG,SAAS,CAAC,kBAAV,CAA6B,IAA7B,EAAmC,EAAE,CAAC,IAAtC,CAArB;;AACA,MAAI,YAAY,IAAI,IAApB,EAA0B;AACxB,IAAA,EAAE,GAAG,EAAE,CAAC,SAAH,CAAa,YAAb,CAAL;AACA,IAAA,IAAI,GAAG,SAAS,CAAC,gBAAV,CAA2B,IAAI,CAAC,MAAhC,EAAwC,EAAE,CAAC,IAA3C,CAAP;AACD;;AACD,QAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CAAqB,OAAO,IAAI,OAAO,CAAC,GAAR,CAAY,EAAZ,EAAgB,IAAhB,CAAhC,EAAuD;AAAC,IAAA;AAAD,GAAvD,CAAZ;;AACA,MAAI,QAAJ,EAAc;AACZ,UAAM,QAAQ,GAAG,SAAS,CAAC,oBAAV,CAA+B,GAAG,CAAC,KAAnC,EAA0C,QAA1C,CAAjB;AACA,WAAO,GAAG,CAAC,OAAJ,CAAY,QAAZ,CAAP;AACD;;AACD,SAAO,GAAP;AACD;AAED;;;;;;;;;;;;;AAYA;;;AACA,SAAS,QAAT,CACI,CADJ,EAC0B,IAAA,GAAwB,IADlD,EAEI,QAAQ,GAAG,KAFf,EAEoB;AAClB,EAAA,CAAC,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,SAAT,CAAnB;AACA,QAAM,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,IAApB,EAA0B,CAAC,CAAC,KAA5B,CAAb;AACA,QAAM,IAAI,GAAG,CAAC,CAAC,IAAF,CAAO,IAAP,EAAa,QAAb,CAAb;AACA,MAAI,aAAa,GAAG,IAAI,CAAC,KAAzB;;AACA,MAAI,CAAC,QAAL,EAAe;AACb,IAAA,aAAa,GAAG,SAAS,CAAC,oBAAV,CAA+B,IAAI,CAAC,KAApC,EAA2C,IAA3C,CAAhB;AACD;;AACD,QAAM,UAAU,GAAG,CAAC,CAAC,OAAF,GAAY,GAAZ,CAAgB,IAAI,CAAC,OAAL,CAAa,aAAb,CAAhB,EAA6C,MAA7C,EAAnB;AACA,QAAM,QAAQ,GAAG,UAAU,CAAC,IAAX,CAAgB,IAAhB,EAAsB,QAAtB,CAAjB;AACA,SAAO;AAAC,IAAA,IAAD;AAAO,IAAA;AAAP,GAAP;AACD;;AAED,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd,C,CACP;;AACA,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAjB;AACP,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAjB;AACP,OAAO,MAAM,SAAS,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAApB;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,OAAO,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAlB;AACP,OAAO,MAAM,GAAG,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAd;AACP,OAAO,MAAM,IAAI,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAf","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as axis_util from './axis_util';\nimport { op } from './operation';\nimport { gradForMinAndMax } from './reduction_ops_util';\nimport { ones, scalar, zerosLike } from './tensor_ops';\n/**\n * Computes the log(sum(exp(elements across the reduction dimensions)).\n *\n * Reduces the input along the dimensions given in `axis`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.logSumExp().print();  // or tf.logSumExp(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)\n * ```\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. If null (the default),\n *     reduces all dimensions.\n * @param keepDims If true, retains reduced dimensions with length\n *     of 1. Defaults to false.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction logSumExp_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'logSumExp');\n    const axes = util.parseAxisParam(axis, $x.shape);\n    const xMax = $x.max(axes, true /* keepDims */);\n    const a = $x.sub(xMax);\n    const b = a.exp();\n    const c = b.sum(axes);\n    const d = c.log();\n    const res = xMax.reshape(d.shape).add(d);\n    if (keepDims) {\n        const newShape = axis_util.expandShapeToKeepDim(res.shape, axes);\n        return res.reshape(newShape);\n    }\n    return res;\n}\n/**\n * Computes the sum of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If axes has no entries, all dimensions are reduced, and a\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.sum().print();  // or tf.sum(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.sum(axis).print();  // or tf.sum(x, axis)\n * ```\n *\n * @param x The input tensor to compute the sum over. If the dtype is `bool`\n *   it will be converted to `int32` and the output dtype will be `int32`.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction sum_(x, axis = null, keepDims = false) {\n    let $x = convertToTensor(x, 'x', 'sum');\n    if ($x.dtype === 'bool') {\n        $x = $x.toInt();\n    }\n    const axes = util.parseAxisParam(axis, $x.shape);\n    // Use a custom gradient to bypass 2 gradient backprops since sum is used\n    // extremely often.\n    const customOp = customGrad((x) => {\n        const permutation = axis_util.getAxesPermutation(axes, x.rank);\n        let reductionAxes = axes;\n        let permutedX = x;\n        if (permutation != null) {\n            permutedX = x.transpose(permutation);\n            reductionAxes = axis_util.getInnerMostAxes(reductionAxes.length, x.rank);\n        }\n        const gradFunc = (dy) => {\n            const expandedDyShape = x.shape.slice();\n            axes.forEach(axis => {\n                expandedDyShape[axis] = 1;\n            });\n            const expandedDy = dy.reshape(expandedDyShape);\n            const derX = expandedDy.mul(ones(x.shape, 'float32'));\n            return derX;\n        };\n        const gradInputs = (dy) => {\n            return { x: () => gradFunc(dy) };\n        };\n        const attrs = { axes: reductionAxes };\n        let value = ENGINE.runKernelFunc(backend => backend.sum(permutedX, reductionAxes), { x: permutedX }, gradInputs, 'Sum', attrs);\n        if (keepDims) {\n            const newShape = axis_util.expandShapeToKeepDim(value.shape, axes);\n            value = value.reshape(newShape);\n        }\n        return { value, gradFunc };\n    });\n    return customOp($x);\n}\n/**\n * Computes the product of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and a\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.prod().print();  // or tf.prod(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.prod(axis).print();  // or tf.prod(x, axis)\n * ```\n *\n * @param x The input tensor to compute the product over. If the dtype is `bool`\n *   it will be converted to `int32` and the output dtype will be `int32`.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction prod_(x, axis = null, keepDims = false) {\n    let $x = convertToTensor(x, 'x', 'prod');\n    if ($x.dtype === 'bool') {\n        $x = $x.toInt();\n    }\n    const axes = util.parseAxisParam(axis, $x.shape);\n    const permutation = axis_util.getAxesPermutation(axes, $x.rank);\n    let reductionAxes = axes;\n    let permutedX = $x;\n    if (permutation != null) {\n        permutedX = $x.transpose(permutation);\n        reductionAxes = axis_util.getInnerMostAxes(reductionAxes.length, $x.rank);\n    }\n    let value = ENGINE.runKernelFunc(backend => backend.prod(permutedX, reductionAxes), { permutedX });\n    if (keepDims) {\n        const newShape = axis_util.expandShapeToKeepDim(value.shape, axes);\n        value = value.reshape(newShape);\n    }\n    return value;\n}\n/**\n * Computes the mean of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is\n * true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with\n * a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.mean().print();  // or tf.mean(a)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.mean(axis).print();  // or tf.mean(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction mean_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'mean');\n    const axes = util.parseAxisParam(axis, $x.shape);\n    const shapes = axis_util.computeOutAndReduceShapes($x.shape, axes);\n    const reduceShape = shapes[1];\n    const reduceSize = util.sizeFromShape(reduceShape);\n    // Use a custom gradient to bypass 2 gradient backprops since mean is used\n    // extremely often.\n    const customOp = customGrad((x) => {\n        const reduceSizeScalar = scalar(reduceSize);\n        // Cast if needed.\n        const xReduce = reduceSizeScalar.dtype === x.dtype ? x : x.cast(reduceSizeScalar.dtype);\n        const res = xReduce.div(reduceSizeScalar);\n        const value = res.sum(axis, keepDims);\n        const gradFunc = (dy) => {\n            const expandedDyShape = x.shape.slice();\n            axes.forEach(axis => {\n                expandedDyShape[axis] = 1;\n            });\n            const expandedDy = dy.reshape(expandedDyShape);\n            const derX = expandedDy.mul(ones(x.shape, 'float32')).div(reduceSize);\n            return derX;\n        };\n        return { value, gradFunc };\n    });\n    return customOp($x);\n}\n/**\n * Computes the minimum value from the input.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axes`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axes` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.min().print();  // or tf.min(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.min(axis).print();  // or tf.min(x, axis)\n * ```\n *\n * @param x The input Tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction min_(x, axis = null, keepDims = false) {\n    let $x = convertToTensor(x, 'x', 'min');\n    const xOrig = $x;\n    const origAxes = util.parseAxisParam(axis, $x.shape);\n    let axes = origAxes;\n    const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n    if (permutedAxes != null) {\n        $x = $x.transpose(permutedAxes);\n        axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n    }\n    const grad = (dy, saved) => gradForMinAndMax(dy, saved[1], saved[0], origAxes, permutedAxes);\n    const inputsToSave = [$x];\n    const outputsToSave = [true];\n    let res = ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.min($x, axes);\n        save([xOrig, y]);\n        return y;\n    }, { x: $x }, grad, 'Min', { axes }, inputsToSave, outputsToSave);\n    if (keepDims) {\n        const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n        res = res.reshape(newShape);\n    }\n    return res;\n}\n/**\n * Returns the indices of the minimum values along an `axis`.\n *\n * The result has the same shape as `input` with the dimension along `axis`\n * removed.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.argMin().print();  // or tf.argMin(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);\n *\n * const axis = 1;\n * x.argMin(axis).print();  // or tf.argMin(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).\n *\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction argMin_(x, axis = 0) {\n    let $x = convertToTensor(x, 'x', 'argMin');\n    if (axis == null) {\n        axis = 0;\n    }\n    let axes = util.parseAxisParam(axis, $x.shape);\n    const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n    if (permutedAxes != null) {\n        $x = $x.transpose(permutedAxes);\n        axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n    }\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { $x: () => zerosLike($x) };\n    };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.argMin($x, axes[0]);\n        save([$x]);\n        return res;\n    }, { $x }, grad);\n}\n/**\n * Returns the indices of the maximum values along an `axis`.\n *\n * The result has the same shape as `input` with the dimension along `axis`\n * removed.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.argMax().print();  // or tf.argMax(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);\n *\n * const axis = 1;\n * x.argMax(axis).print();  // or tf.argMax(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction argMax_(x, axis = 0) {\n    let $x = convertToTensor(x, 'x', 'argMax');\n    if (axis == null) {\n        axis = 0;\n    }\n    let axes = util.parseAxisParam(axis, $x.shape);\n    const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n    if (permutedAxes != null) {\n        $x = $x.transpose(permutedAxes);\n        axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n    }\n    const grad = (dy, saved) => {\n        const [$x] = saved;\n        return { x: () => zerosLike($x) };\n    };\n    const attrs = { axis: axes[0] };\n    const inputsToSave = [$x];\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.argMax($x, axes[0]);\n        save([$x]);\n        return res;\n    }, { x: $x }, grad, 'ArgMax', attrs, inputsToSave);\n}\n/**\n * Computes the logical and of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 1, 1], 'bool');\n *\n * x.all().print();  // or tf.all(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');\n *\n * const axis = 1;\n * x.all(axis).print();  // or tf.all(x, axis)\n * ```\n *\n * @param x The input tensor. Must be of dtype bool.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction all_(x, axis = null, keepDims = false) {\n    let $x = convertToTensor(x, 'x', 'all', 'bool');\n    const origAxes = util.parseAxisParam(axis, $x.shape);\n    let axes = origAxes;\n    const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n    if (permutedAxes != null) {\n        $x = $x.transpose(permutedAxes);\n        axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n    }\n    const res = ENGINE.runKernelFunc(backend => backend.all($x, axes), { $x });\n    if (keepDims) {\n        const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n        return res.reshape(newShape);\n    }\n    return res;\n}\n/**\n * Computes the logical or of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 1, 1], 'bool');\n *\n * x.any().print();  // or tf.any(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');\n *\n * const axis = 1;\n * x.any(axis).print();  // or tf.any(x, axis)\n * ```\n *\n * @param x The input tensor. Must be of dtype bool.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n */\n/** @doc {heading: 'Operations', subheading: 'Reduction'} */\nfunction any_(x, axis = null, keepDims = false) {\n    let $x = convertToTensor(x, 'x', 'any', 'bool');\n    const origAxes = util.parseAxisParam(axis, $x.shape);\n    let axes = origAxes;\n    const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n    if (permutedAxes != null) {\n        $x = $x.transpose(permutedAxes);\n        axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n    }\n    const res = ENGINE.runKernelFunc(backend => backend.any($x, axes), { $x });\n    if (keepDims) {\n        const newShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n        return res.reshape(newShape);\n    }\n    return res;\n}\n/**\n * Calculates the mean and variance of `x`. The mean and variance are\n * calculated by aggregating the contents of `x` across `axes`. If `x` is\n * 1-D and `axes = [0]` this is just the mean and variance of a vector.\n *\n * @param x The input tensor.\n * @param axis The dimension(s) along with to compute mean and\n *     variance. By default it reduces all dimensions.\n * @param keepDims If true, the moments have the same dimensionality as the\n *     input.\n * @return An object with two keys: `mean` and `variance`.\n */\n/** @doc {heading: 'Operations', subheading: 'Normalization'} */\nfunction moments_(x, axis = null, keepDims = false) {\n    x = convertToTensor(x, 'x', 'moments');\n    const axes = util.parseAxisParam(axis, x.shape);\n    const mean = x.mean(axes, keepDims);\n    let keepDimsShape = mean.shape;\n    if (!keepDims) {\n        keepDimsShape = axis_util.expandShapeToKeepDim(mean.shape, axes);\n    }\n    const devSquared = x.toFloat().sub(mean.reshape(keepDimsShape)).square();\n    const variance = devSquared.mean(axes, keepDims);\n    return { mean, variance };\n}\nexport const all = op({ all_ });\n// tslint:disable-next-line:variable-name\nexport const any = op({ any_ });\nexport const argMax = op({ argMax_ });\nexport const argMin = op({ argMin_ });\nexport const logSumExp = op({ logSumExp_ });\nexport const mean = op({ mean_ });\nexport const min = op({ min_ });\nexport const moments = op({ moments_ });\nexport const sum = op({ sum_ });\nexport const prod = op({ prod_ });\n//# sourceMappingURL=reduction_ops.js.map"]},"metadata":{},"sourceType":"module"}