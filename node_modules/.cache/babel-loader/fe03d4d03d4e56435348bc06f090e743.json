{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport * as conv_util from '../ops/conv_util';\nimport { op } from '../ops/operation';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { add } from './add';\nimport * as broadcast_util from './broadcast_util';\nimport { conv2d as unfusedConv2d } from './conv2d';\nimport { conv2DBackpropFilter } from './conv2d_backprop_filter';\nimport { conv2DBackpropInput } from './conv2d_backprop_input';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from './depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from './depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from './depthwise_conv2d_native_backprop_input';\nimport { elu } from './elu';\nimport { shouldFuse } from './fused_util';\nimport { matMul as unfusedMatMul } from './mat_mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6'; // Returns gradient for fused activation.\n\nconst getFusedDyActivation = (dy, y, activation) => {\n  if (activation == null || activation === 'linear') {\n    return dy;\n  }\n\n  if (activation === 'relu') {\n    return dy.mul(y.step());\n  }\n\n  throw new Error(`Gradient for activation ${activation} has not been ` + `implemented yet.`);\n}; // Returns gradient for fused bias.\n\n\nconst getFusedBiasGradient = (bias, dyActivation) => {\n  let res = dyActivation;\n  const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n\n  if (reduceAxes.length > 0) {\n    res = res.sum(reduceAxes);\n  }\n\n  return res.reshape(bias.shape);\n};\n\nconst applyActivation = (x, activation, preluActivationWeights) => {\n  if (activation === 'linear') {\n    return x;\n  } else if (activation === 'relu') {\n    return relu(x);\n  } else if (activation === 'elu') {\n    return elu(x);\n  } else if (activation === 'relu6') {\n    return relu6(x);\n  } else if (activation === 'prelu') {\n    return prelu(x, preluActivationWeights);\n  }\n\n  throw new Error(`Unknown fused activation ${activation}.`);\n};\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n */\n\n\nfunction fusedMatMul_({\n  a,\n  b,\n  transposeA = false,\n  transposeB = false,\n  bias,\n  activation = 'linear',\n  preluActivationWeights\n}) {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedMatMul(a, b, transposeA, transposeB);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights);\n  }\n\n  let $a = convertToTensor(a, 'a', 'fused matMul');\n  let $b = convertToTensor(b, 'b', 'fused matMul');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  const outerDimsA = $a.shape.slice(0, -2);\n  const outerDimsB = $b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at least ` + `2, got ranks ${$a.rank} and ${$b.rank}.`);\n  util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` + `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} must match.`);\n  util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n  const a3D = transposeA ? $a.as3D(batchDimA, innerShapeA, outerShapeA) : $a.as3D(batchDimA, outerShapeA, innerShapeA);\n  const b3D = transposeB ? $b.as3D(batchDimB, outerShapeB, innerShapeB) : $b.as3D(batchDimB, innerShapeB, outerShapeB);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n    [$bias] = makeTypesMatch($bias, $a);\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n\n  const grad = (dy, saved) => {\n    const [a3D, b3D, y] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    let biasGradient = {};\n\n    if (bias != null) {\n      biasGradient = {\n        bias: () => getFusedBiasGradient($bias, dyActivation)\n      };\n    }\n\n    if (!transposeA && !transposeB) {\n      return Object.assign({\n        a: () => dyActivation.matMul(b3D, false, true),\n        b: () => a3D.matMul(dyActivation, true, false)\n      }, biasGradient);\n    } else if (!transposeA && transposeB) {\n      return Object.assign({\n        a: () => dyActivation.matMul(b3D, false, false),\n        b: () => dyActivation.matMul(a3D, true, false)\n      }, biasGradient);\n    } else if (transposeA && !transposeB) {\n      return Object.assign({\n        a: () => b3D.matMul(dyActivation, false, true),\n        b: () => a3D.matMul(dyActivation, false, false)\n      }, biasGradient);\n    } else {\n      return Object.assign({\n        a: () => b3D.matMul(dyActivation, true, true),\n        b: () => dyActivation.matMul(a3D, true, true)\n      }, biasGradient);\n    }\n  };\n\n  const inputs = {\n    a: a3D,\n    b: b3D\n  };\n\n  if (bias != null) {\n    inputs.bias = $bias;\n  }\n\n  if (preluActivationWeights != null) {\n    inputs.preluActivationWeights = $preluActivationWeights;\n  }\n\n  const inputsToSave = [a3D, b3D];\n  const outputsToSave = [true];\n  const res = ENGINE.runKernelFunc((backend, save) => {\n    const y = backend.fusedBatchMatMul({\n      a: a3D,\n      b: b3D,\n      transposeA,\n      transposeB,\n      bias: $bias,\n      activation,\n      preluActivationWeights: $preluActivationWeights\n    });\n    save([a3D, b3D, y]);\n    return y;\n  }, inputs, grad, '_FusedMatMul', {\n    transposeA,\n    transposeB,\n    activation\n  }, inputsToSave, outputsToSave);\n  return res.reshape(outShape);\n}\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n */\n\n\nfunction fusedConv2d_({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights\n}) {\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights);\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = $x.as4D(1, $x.shape[0], $x.shape[1], $x.shape[2]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` + `${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` + `${$filter.rank}.`);\n\n  if (dimRoundingMode != null) {\n    util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` + `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n  }\n\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` + `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' + `Got strides ${strides} and dilations '${dilations}'`);\n  util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy, saved) => {\n    const [$filter, x4D, y] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' + `dilation rates greater than 1 ` + `are not yet supported in gradients. Got dilations '${dilations}'`);\n    let biasGradient = {};\n\n    if (bias != null) {\n      biasGradient = {\n        bias: () => getFusedBiasGradient($bias, dyActivation)\n      };\n    }\n\n    return Object.assign({\n      x: () => conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad),\n      filter: () => conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad)\n    }, biasGradient);\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter\n  };\n\n  if (bias != null) {\n    inputs.bias = $bias;\n  }\n\n  if (preluActivationWeights != null) {\n    inputs.preluActivationWeights = $preluActivationWeights;\n  }\n\n  const inputsToSave = [$filter, x4D];\n  const outputsToSave = [true]; // Save the only output.\n\n  const res = ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.fusedConv2d({\n      input: x4D,\n      filter: $filter,\n      convInfo,\n      bias: $bias,\n      activation,\n      preluActivationWeights: $preluActivationWeights\n    });\n    save([$filter, x4D, res]);\n    return res;\n  }, inputs, grad, 'FusedConv2D', {\n    convInfo,\n    activation\n  }, inputsToSave, outputsToSave);\n\n  if (reshapedTo4D) {\n    return res.as3D(res.shape[1], res.shape[2], res.shape[3]);\n  }\n\n  return res;\n}\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n */\n\n\nfunction fusedDepthwiseConv2d_({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights\n}) {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights);\n  }\n\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n  const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = $x.as4D(1, $x.shape[0], $x.shape[1], $x.shape[2]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` + `rank ${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` + `but got rank ${$filter.rank}.`);\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` + `(${x4D.shape[3]}) must match the inChannels dimension in ` + `filter ${$filter.shape[2]}.`);\n\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' + `be 1. Got strides ${strides} and dilations '${dilations}'`);\n\n  if (dimRoundingMode != null) {\n    util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` + `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n  }\n\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true\n  /* depthwise */\n  );\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n\n  const grad = (dy, saved) => {\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' + `greater than 1 are not yet supported. Got dilations ` + `'${dilations}'`);\n    const [$filter, x4D, y] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    let biasGradient = {};\n\n    if (bias != null) {\n      biasGradient = {\n        bias: () => getFusedBiasGradient($bias, dyActivation)\n      };\n    }\n\n    return Object.assign({\n      x: () => depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, convInfo),\n      filter: () => depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, convInfo)\n    }, biasGradient);\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter\n  };\n\n  if (bias != null) {\n    inputs.bias = $bias;\n  }\n\n  if (preluActivationWeights != null) {\n    inputs.preluActivationWeights = $preluActivationWeights;\n  }\n\n  const inputsToSave = [$filter, x4D];\n  const outputsToSave = [true];\n  const res = ENGINE.runKernelFunc((backend, save) => {\n    const res = backend.fusedDepthwiseConv2D({\n      input: x4D,\n      filter: $filter,\n      convInfo,\n      bias: $bias,\n      activation,\n      preluActivationWeights: $preluActivationWeights\n    });\n    save([$filter, x4D, res]);\n    return res;\n  }, inputs, grad, 'FusedDepthwiseConv2D', {\n    convInfo,\n    activation\n  }, inputsToSave, outputsToSave);\n\n  if (reshapedTo4D) {\n    return res.as3D(res.shape[1], res.shape[2], res.shape[3]);\n  }\n\n  return res;\n}\n\nexport const matMul = op({\n  fusedMatMul_\n});\nexport const conv2d = op({\n  fusedConv2d_\n});\nexport const depthwiseConv2d = op({\n  fusedDepthwiseConv2d_\n});","map":{"version":3,"sources":["../../src/ops/fused_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQ,MAAR,QAAqB,WAArB;AACA,OAAO,KAAK,SAAZ,MAA2B,kBAA3B;AACA,SAAQ,EAAR,QAAiB,kBAAjB;AAEA,SAAQ,cAAR,QAA6B,gBAA7B;AACA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,SAAtB;AAEA,SAAQ,GAAR,QAAkB,OAAlB;AACA,OAAO,KAAK,cAAZ,MAAgC,kBAAhC;AACA,SAAQ,MAAM,IAAI,aAAlB,QAAsC,UAAtC;AACA,SAAQ,oBAAR,QAAmC,0BAAnC;AACA,SAAQ,mBAAR,QAAkC,yBAAlC;AACA,SAAQ,eAAe,IAAI,sBAA3B,QAAwD,oBAAxD;AACA,SAAQ,mCAAR,QAAkD,2CAAlD;AACA,SAAQ,kCAAR,QAAiD,0CAAjD;AACA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAoB,UAApB,QAAqC,cAArC;AACA,SAAQ,MAAM,IAAI,aAAlB,QAAsC,WAAtC;AACA,SAAQ,KAAR,QAAoB,SAApB;AACA,SAAQ,IAAR,QAAmB,QAAnB;AACA,SAAQ,KAAR,QAAoB,SAApB,C,CAEA;;AACA,MAAM,oBAAoB,GACtB,CAAC,EAAD,EAAa,CAAb,EAAwB,UAAxB,KAA0D;AACxD,MAAI,UAAU,IAAI,IAAd,IAAsB,UAAU,KAAK,QAAzC,EAAmD;AACjD,WAAO,EAAP;AACD;;AACD,MAAI,UAAU,KAAK,MAAnB,EAA2B;AACzB,WAAO,EAAE,CAAC,GAAH,CAAO,CAAC,CAAC,IAAF,EAAP,CAAP;AACD;;AACD,QAAM,IAAI,KAAJ,CACF,2BAA2B,UAAU,gBAArC,GACA,kBAFE,CAAN;AAGD,CAXL,C,CAaA;;;AACA,MAAM,oBAAoB,GAAG,CAAC,IAAD,EAAe,YAAf,KAA+C;AAC1E,MAAI,GAAG,GAAG,YAAV;AACA,QAAM,UAAU,GACZ,cAAc,CAAC,gBAAf,CAAgC,IAAI,CAAC,KAArC,EAA4C,YAAY,CAAC,KAAzD,CADJ;;AAEA,MAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;AACzB,IAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAR,CAAN;AACD;;AACD,SAAO,GAAG,CAAC,OAAJ,CAAY,IAAI,CAAC,KAAjB,CAAP;AACD,CARD;;AAUA,MAAM,eAAe,GACjB,CAAC,CAAD,EAAY,UAAZ,EAAoC,sBAApC,KACa;AACP,MAAI,UAAU,KAAK,QAAnB,EAA6B;AAC3B,WAAO,CAAP;AACD,GAFD,MAEO,IAAI,UAAU,KAAK,MAAnB,EAA2B;AAChC,WAAO,IAAI,CAAC,CAAD,CAAX;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,KAAnB,EAA0B;AAC/B,WAAO,GAAG,CAAC,CAAD,CAAV;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,OAAnB,EAA4B;AACjC,WAAO,KAAK,CAAC,CAAD,CAAZ;AACD,GAFM,MAEA,IAAI,UAAU,KAAK,OAAnB,EAA4B;AACjC,WAAO,KAAK,CAAC,CAAD,EAAI,sBAAJ,CAAZ;AACD;;AACD,QAAM,IAAI,KAAJ,CAAU,4BAA4B,UAAU,GAAhD,CAAN;AACD,CAfT;AAiBA;;;;;;;;;;;;;;;;;;;;;;AAoBA,SAAS,YAAT,CAAwC;AACtC,EAAA,CADsC;AAEtC,EAAA,CAFsC;AAGtC,EAAA,UAAU,GAAG,KAHyB;AAItC,EAAA,UAAU,GAAG,KAJyB;AAKtC,EAAA,IALsC;AAMtC,EAAA,UAAU,GAAG,QANyB;AAOtC,EAAA;AAPsC,CAAxC,EAgBC;AACC,MAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAI,MAAM,GAAG,aAAa,CAAC,CAAD,EAAI,CAAJ,EAAO,UAAP,EAAmB,UAAnB,CAA1B;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;AACD;;AAED,WAAO,eAAe,CAAC,MAAD,EAAS,UAAT,EAAqB,sBAArB,CAAtB;AACD;;AAED,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;AAEA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAEA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAGA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAEA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAGA,QAAM,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;AACA,QAAM,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;AACA,QAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;AACA,QAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;AAEA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,IAAW,CAAX,IAAgB,EAAE,CAAC,IAAH,IAAW,CAA3B,IAAgC,EAAE,CAAC,IAAH,KAAY,EAAE,CAAC,IADnD,EAEI,MACI,oEAAA,GACA,gBAAgB,EAAE,CAAC,IAAI,QAAQ,EAAE,CAAC,IAAI,GAJ9C;AAMA,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,WAAL,CAAiB,UAAjB,EAA6B,UAA7B,CADJ,EAEI,MAAM,4CAA4C,UAAU,SAAtD,GACF,GAAG,UAAU,4BAA4B,EAAE,CAAC,KAAK,OAD/C,GAEF,GAAG,EAAE,CAAC,KAAK,cAJnB;AAMA,EAAA,IAAI,CAAC,MAAL,CACI,WAAW,KAAK,WADpB,EAEI,MAAM,wCAAwC,WAAW,SAAnD,GACF,GAAG,WAAW,4BAA4B,EAAE,CAAC,KAAK,OADhD,GAEF,GAAG,EAAE,CAAC,KAAK,mBAAmB,UAAU,EAFtC,GAGF,mBAAmB,UAAU,cALrC;AAOA,QAAM,QAAQ,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,EAAsB,MAAtB,CAA6B,CAAC,WAAD,EAAc,WAAd,CAA7B,CAAjB;AAEA,QAAM,GAAG,GAAG,UAAU,GAAG,EAAE,CAAC,IAAH,CAAQ,SAAR,EAAmB,WAAnB,EAAgC,WAAhC,CAAH,GACG,EAAE,CAAC,IAAH,CAAQ,SAAR,EAAmB,WAAnB,EAAgC,WAAhC,CADzB;AAEA,QAAM,GAAG,GAAG,UAAU,GAAG,EAAE,CAAC,IAAH,CAAQ,SAAR,EAAmB,WAAnB,EAAgC,WAAhC,CAAH,GACG,EAAE,CAAC,IAAH,CAAQ,SAAR,EAAmB,WAAnB,EAAgC,WAAhC,CADzB;AAGA,MAAI,KAAJ;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAAC,KAAD,IAAU,cAAc,CAAC,KAAD,EAAQ,EAAR,CAAxB;AAEA,IAAA,cAAc,CAAC,0BAAf,CAA0C,QAA1C,EAAoD,KAAK,CAAC,KAA1D;AACD;;AAED,MAAI,uBAAJ;;AACA,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;AAED;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAe,KAAf,KAAkC;AAC7C,UAAM,CAAC,GAAD,EAAM,GAAN,EAAW,CAAX,IAAgB,KAAtB;AACA,UAAM,YAAY,GAAG,oBAAoB,CAAC,EAAD,EAAK,CAAL,EAAQ,UAAR,CAAzC;AAEA,QAAI,YAAY,GAAG,EAAnB;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,YAAY,GAAG;AAAC,QAAA,IAAI,EAAE,MAAM,oBAAoB,CAAC,KAAD,EAAQ,YAAR;AAAjC,OAAf;AACD;;AAED,QAAI,CAAC,UAAD,IAAe,CAAC,UAApB,EAAgC;AAC9B,aAAO,MAAM,CAAC,MAAP,CACH;AACE,QAAA,CAAC,EAAE,MAAM,YAAY,CAAC,MAAb,CAAoB,GAApB,EAAqC,KAArC,EAA4C,IAA5C,CADX;AAEE,QAAA,CAAC,EAAE,MAAM,GAAG,CAAC,MAAJ,CAAW,YAAX,EAAyB,IAAzB,EAA+B,KAA/B;AAFX,OADG,EAKH,YALG,CAAP;AAMD,KAPD,MAOO,IAAI,CAAC,UAAD,IAAe,UAAnB,EAA+B;AACpC,aAAO,MAAM,CAAC,MAAP,CACH;AACE,QAAA,CAAC,EAAE,MAAM,YAAY,CAAC,MAAb,CAAoB,GAApB,EAAqC,KAArC,EAA4C,KAA5C,CADX;AAEE,QAAA,CAAC,EAAE,MAAM,YAAY,CAAC,MAAb,CAAoB,GAApB,EAAqC,IAArC,EAA2C,KAA3C;AAFX,OADG,EAKH,YALG,CAAP;AAMD,KAPM,MAOA,IAAI,UAAU,IAAI,CAAC,UAAnB,EAA+B;AACpC,aAAO,MAAM,CAAC,MAAP,CACH;AACE,QAAA,CAAC,EAAE,MAAM,GAAG,CAAC,MAAJ,CAAW,YAAX,EAAyB,KAAzB,EAAgC,IAAhC,CADX;AAEE,QAAA,CAAC,EAAE,MAAM,GAAG,CAAC,MAAJ,CAAW,YAAX,EAAyB,KAAzB,EAAgC,KAAhC;AAFX,OADG,EAKH,YALG,CAAP;AAMD,KAPM,MAOA;AACL,aAAO,MAAM,CAAC,MAAP,CACH;AACE,QAAA,CAAC,EAAE,MAAM,GAAG,CAAC,MAAJ,CAAW,YAAX,EAAyB,IAAzB,EAA+B,IAA/B,CADX;AAEE,QAAA,CAAC,EAAE,MAAM,YAAY,CAAC,MAAb,CAAoB,GAApB,EAAqC,IAArC,EAA2C,IAA3C;AAFX,OADG,EAKH,YALG,CAAP;AAMD;AACF,GAtCD;;AAwCA,QAAM,MAAM,GAG4B;AAAC,IAAA,CAAC,EAAE,GAAJ;AAAS,IAAA,CAAC,EAAE;AAAZ,GAHxC;;AAIA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,MAAM,CAAC,IAAP,GAAc,KAAd;AACD;;AACD,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,MAAM,CAAC,sBAAP,GAAgC,uBAAhC;AACD;;AAED,QAAM,YAAY,GAAG,CAAC,GAAD,EAAM,GAAN,CAArB;AACA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB;AAEA,QAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CACR,CAAC,OAAD,EAAU,IAAV,KAAkB;AAChB,UAAM,CAAC,GAAG,OAAO,CAAC,gBAAR,CAAyB;AACjC,MAAA,CAAC,EAAE,GAD8B;AAEjC,MAAA,CAAC,EAAE,GAF8B;AAGjC,MAAA,UAHiC;AAIjC,MAAA,UAJiC;AAKjC,MAAA,IAAI,EAAE,KAL2B;AAMjC,MAAA,UANiC;AAOjC,MAAA,sBAAsB,EAAE;AAPS,KAAzB,CAAV;AASA,IAAA,IAAI,CAAC,CAAC,GAAD,EAAM,GAAN,EAAW,CAAX,CAAD,CAAJ;AACA,WAAO,CAAP;AACD,GAbO,EAcR,MAdQ,EAcA,IAdA,EAcM,cAdN,EAcsB;AAAC,IAAA,UAAD;AAAa,IAAA,UAAb;AAAyB,IAAA;AAAzB,GAdtB,EAeR,YAfQ,EAeM,aAfN,CAAZ;AAgBA,SAAO,GAAG,CAAC,OAAJ,CAAY,QAAZ,CAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAwDA,SAAS,YAAT,CAAmD;AACjD,EAAA,CADiD;AAEjD,EAAA,MAFiD;AAGjD,EAAA,OAHiD;AAIjD,EAAA,GAJiD;AAKjD,EAAA,UAAU,GAAG,MALoC;AAMjD,EAAA,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CANqC;AAOjD,EAAA,eAPiD;AAQjD,EAAA,IARiD;AASjD,EAAA,UAAU,GAAG,QAToC;AAUjD,EAAA;AAViD,CAAnD,EAsBC;AACC,EAAA,UAAU,GAAG,UAAU,IAAI,QAA3B;;AACA,MAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAI,MAAM,GAAG,aAAa,CACtB,CADsB,EACnB,MADmB,EACX,OADW,EACF,GADE,EACG,UADH,EACe,SADf,EAC0B,eAD1B,CAA1B;;AAEA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;AACD;;AAED,WAAO,eAAe,CAAC,MAAD,EAAS,UAAT,EAAqB,sBAArB,CAAtB;AACD;;AAED,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,QAAT,CAA1B;AACA,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,QAAnB,CAA/B;AAEA,MAAI,GAAG,GAAG,EAAV;AACA,MAAI,YAAY,GAAG,KAAnB;;AAEA,MAAI,EAAE,CAAC,IAAH,KAAY,CAAhB,EAAmB;AACjB,IAAA,YAAY,GAAG,IAAf;AACA,IAAA,GAAG,GAAG,EAAE,CAAC,IAAH,CAAQ,CAAR,EAAW,EAAE,CAAC,KAAH,CAAS,CAAT,CAAX,EAAwB,EAAE,CAAC,KAAH,CAAS,CAAT,CAAxB,EAAqC,EAAE,CAAC,KAAH,CAAS,CAAT,CAArC,CAAN;AACD;;AACD,EAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,IAAJ,KAAa,CADjB,EAEI,MAAM,4DAAA,GACF,GAAG,GAAG,CAAC,IAAI,GAHnB;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,OAAO,CAAC,IAAR,KAAiB,CADrB,EAEI,MAAM,6DAAA,GACF,GAAG,OAAO,CAAC,IAAI,GAHvB;;AAIA,MAAI,eAAe,IAAI,IAAvB,EAA6B;AAC3B,IAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,KAAL,CAAW,GAAX,CADJ,EAEI,MAAM,4DAAA,GACF,mBAAmB,eAAe,gBAAgB,GAAG,GAH7D;AAID;;AAED,EAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,CAAU,CAAV,MAAiB,OAAO,CAAC,KAAR,CAAc,CAAd,CADrB,EAEI,MAAM,oCAAoC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAY,eAAhD,GACF,0BAA0B,OAAO,CAAC,KAAR,CAAc,CAAd,CAAgB,GAHlD;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,8BAAV,CAAyC,OAAzC,EAAkD,SAAlD,CADJ,EAEI,MAAM,6DACF,eAAe,OAAO,mBAAmB,SAAS,GAH1D;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,UAAU,KAAK,MADnB,EAEI,MAAM,sCACF,UAAU,wCAHlB;AAKA,QAAM,QAAQ,GAAG,SAAS,CAAC,iBAAV,CACb,GAAG,CAAC,KADS,EACF,OAAO,CAAC,KADN,EACa,OADb,EACsB,SADtB,EACiC,GADjC,EACsC,eADtC,CAAjB;AAGA,MAAI,KAAJ;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAAC,KAAD,IAAU,cAAc,CAAC,KAAD,EAAQ,EAAR,CAAxB;AAEA,IAAA,cAAc,CAAC,0BAAf,CAA0C,QAAQ,CAAC,QAAnD,EAA6D,KAAK,CAAC,KAAnE;AACD;;AAED,MAAI,uBAAJ;;AACA,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;AAED;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAe,KAAf,KAAkC;AAC7C,UAAM,CAAC,OAAD,EAAU,GAAV,EAAe,CAAf,IAAoB,KAA1B;AAEA,UAAM,YAAY,GAAG,oBAAoB,CAAC,EAAD,EAAK,CAAL,EAAQ,UAAR,CAAzC;AAEA,IAAA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,iBAAV,CAA4B,SAA5B,CADJ,EAEI,MAAM,wCACF,gCADE,GAEF,sDAAsD,SAAS,GAJvE;AAMA,QAAI,YAAY,GAAG,EAAnB;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,YAAY,GAAG;AAAC,QAAA,IAAI,EAAE,MAAM,oBAAoB,CAAC,KAAD,EAAQ,YAAR;AAAjC,OAAf;AACD;;AAED,WAAO,MAAM,CAAC,MAAP,CACH;AACE,MAAA,CAAC,EAAE,MAAM,mBAAmB,CACxB,GAAG,CAAC,KADoB,EACb,YADa,EACC,OADD,EACU,OADV,EACmB,GADnB,CAD9B;AAGE,MAAA,MAAM,EAAE,MAAM,oBAAoB,CAC9B,GAD8B,EACzB,YADyB,EACX,OAAO,CAAC,KADG,EACI,OADJ,EACa,GADb;AAHpC,KADG,EAOH,YAPG,CAAP;AAQD,GAxBD;;AA0BA,QAAM,MAAM,GAKR;AAAC,IAAA,CAAC,EAAE,GAAJ;AAAS,IAAA,MAAM,EAAE;AAAjB,GALJ;;AAMA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,MAAM,CAAC,IAAP,GAAc,KAAd;AACD;;AACD,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,MAAM,CAAC,sBAAP,GAAgC,uBAAhC;AACD;;AAED,QAAM,YAAY,GAAG,CAAC,OAAD,EAAU,GAAV,CAArB;AACA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB,CA3GD,CA2GgC;;AAC/B,QAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CACR,CAAC,OAAD,EAAU,IAAV,KAAkB;AAChB,UAAM,GAAG,GAAG,OAAO,CAAC,WAAR,CAAoB;AAC9B,MAAA,KAAK,EAAE,GADuB;AAE9B,MAAA,MAAM,EAAE,OAFsB;AAG9B,MAAA,QAH8B;AAI9B,MAAA,IAAI,EAAE,KAJwB;AAK9B,MAAA,UAL8B;AAM9B,MAAA,sBAAsB,EAAE;AANM,KAApB,CAAZ;AAQA,IAAA,IAAI,CAAC,CAAC,OAAD,EAAU,GAAV,EAAe,GAAf,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAZO,EAaR,MAbQ,EAaA,IAbA,EAaM,aAbN,EAaqB;AAAC,IAAA,QAAD;AAAW,IAAA;AAAX,GAbrB,EAa6C,YAb7C,EAcR,aAdQ,CAAZ;;AAgBA,MAAI,YAAJ,EAAkB;AAChB,WAAO,GAAG,CAAC,IAAJ,CAAS,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAT,EAAuB,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAvB,EAAqC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAArC,CAAP;AACD;;AAED,SAAO,GAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAkDA,SAAS,qBAAT,CAA4D;AAC1D,EAAA,CAD0D;AAE1D,EAAA,MAF0D;AAG1D,EAAA,OAH0D;AAI1D,EAAA,GAJ0D;AAK1D,EAAA,UAAU,GAAG,MAL6C;AAM1D,EAAA,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAN8C;AAO1D,EAAA,eAP0D;AAQ1D,EAAA,IAR0D;AAS1D,EAAA,UAAU,GAAG,QAT6C;AAU1D,EAAA;AAV0D,CAA5D,EAsBC;AACC,MAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAI,MAAM,GAAG,sBAAsB,CAC/B,CAD+B,EAC5B,MAD4B,EACpB,OADoB,EACX,GADW,EACN,UADM,EACM,SADN,EACiB,eADjB,CAAnC;;AAEA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;AACD;;AAED,WAAO,eAAe,CAAC,MAAD,EAAS,UAAT,EAAqB,sBAArB,CAAtB;AACD;;AAED,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,iBAAT,CAA1B;AACA,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,iBAAnB,CAA/B;AAEA,MAAI,GAAG,GAAG,EAAV;AACA,MAAI,YAAY,GAAG,KAAnB;;AACA,MAAI,EAAE,CAAC,IAAH,KAAY,CAAhB,EAAmB;AACjB,IAAA,YAAY,GAAG,IAAf;AACA,IAAA,GAAG,GAAG,EAAE,CAAC,IAAH,CAAQ,CAAR,EAAW,EAAE,CAAC,KAAH,CAAS,CAAT,CAAX,EAAwB,EAAE,CAAC,KAAH,CAAS,CAAT,CAAxB,EAAqC,EAAE,CAAC,KAAH,CAAS,CAAT,CAArC,CAAN;AACD;;AACD,EAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,IAAJ,KAAa,CADjB,EAEI,MAAM,gEAAA,GACF,QAAQ,GAAG,CAAC,IAAI,GAHxB;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,OAAO,CAAC,IAAR,KAAiB,CADrB,EAEI,MAAM,yDAAA,GACF,gBAAgB,OAAO,CAAC,IAAI,GAHpC;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,CAAU,CAAV,MAAiB,OAAO,CAAC,KAAR,CAAc,CAAd,CADrB,EAEI,MAAM,2DAAA,GACF,IAAI,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAY,2CADd,GAEF,UAAU,OAAO,CAAC,KAAR,CAAc,CAAd,CAAgB,GAJlC;;AAKA,MAAI,SAAS,IAAI,IAAjB,EAAuB;AACrB,IAAA,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAZ;AACD;;AACD,EAAA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,8BAAV,CAAyC,OAAzC,EAAkD,SAAlD,CADJ,EAEI,MACI,sEACA,qBAAqB,OAAO,mBAAmB,SAAS,GAJhE;;AAMA,MAAI,eAAe,IAAI,IAAvB,EAA6B;AAC3B,IAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,KAAL,CAAW,GAAX,CADJ,EAEI,MAAM,8DAAA,GACF,yBAAyB,eAAe,gBAAgB,GAAG,GAHnE;AAID;;AAED,QAAM,QAAQ,GAAG,SAAS,CAAC,iBAAV,CACb,GAAG,CAAC,KADS,EACF,OAAO,CAAC,KADN,EACa,OADb,EACsB,SADtB,EACiC,GADjC,EACsC,eADtC,EAEb;AAAK;AAFQ,GAAjB;AAIA,MAAI,KAAJ;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAAC,KAAD,IAAU,cAAc,CAAC,KAAD,EAAQ,EAAR,CAAxB;AAEA,IAAA,cAAc,CAAC,0BAAf,CAA0C,QAAQ,CAAC,QAAnD,EAA6D,KAAK,CAAC,KAAnE;AACD;;AAED,MAAI,uBAAJ;;AACA,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,uBADJ,CAAzC;AAED;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAe,KAAf,KAAkC;AAC7C,IAAA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,iBAAV,CAA4B,SAA5B,CADJ,EAEI,MAAM,gEACF,sDADE,GAEF,IAAI,SAAS,GAJrB;AAKA,UAAM,CAAC,OAAD,EAAU,GAAV,EAAe,CAAf,IAAoB,KAA1B;AAEA,UAAM,YAAY,GAAG,oBAAoB,CAAC,EAAD,EAAK,CAAL,EAAQ,UAAR,CAAzC;AAEA,QAAI,YAAY,GAAG,EAAnB;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,YAAY,GAAG;AAAC,QAAA,IAAI,EAAE,MAAM,oBAAoB,CAAC,KAAD,EAAQ,YAAR;AAAjC,OAAf;AACD;;AAED,WAAO,MAAM,CAAC,MAAP,CACH;AACE,MAAA,CAAC,EAAE,MAAM,kCAAkC,CACtC,GAAgB,CAAC,KADqB,EACd,YADc,EACA,OADA,EAEvC,QAFuC,CAD7C;AAIE,MAAA,MAAM,EAAE,MAAM,mCAAmC,CAC7C,GAD6C,EAC5B,YAD4B,EACb,OAAoB,CAAC,KADR,EAE7C,QAF6C;AAJnD,KADG,EASH,YATG,CAAP;AAUD,GAzBD;;AA2BA,QAAM,MAAM,GAKR;AAAC,IAAA,CAAC,EAAE,GAAJ;AAAS,IAAA,MAAM,EAAE;AAAjB,GALJ;;AAMA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,MAAM,CAAC,IAAP,GAAc,KAAd;AACD;;AACD,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,MAAM,CAAC,sBAAP,GAAgC,uBAAhC;AACD;;AAED,QAAM,YAAY,GAAG,CAAC,OAAD,EAAU,GAAV,CAArB;AACA,QAAM,aAAa,GAAG,CAAC,IAAD,CAAtB;AACA,QAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CACR,CAAC,OAAD,EAAU,IAAV,KAAkB;AAChB,UAAM,GAAG,GAAG,OAAO,CAAC,oBAAR,CAA6B;AACvC,MAAA,KAAK,EAAE,GADgC;AAEvC,MAAA,MAAM,EAAE,OAF+B;AAGvC,MAAA,QAHuC;AAIvC,MAAA,IAAI,EAAE,KAJiC;AAKvC,MAAA,UALuC;AAMvC,MAAA,sBAAsB,EAAE;AANe,KAA7B,CAAZ;AAQA,IAAA,IAAI,CAAC,CAAC,OAAD,EAAU,GAAV,EAAe,GAAf,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAZO,EAaR,MAbQ,EAaA,IAbA,EAaM,sBAbN,EAa8B;AAAC,IAAA,QAAD;AAAW,IAAA;AAAX,GAb9B,EAcR,YAdQ,EAcM,aAdN,CAAZ;;AAeA,MAAI,YAAJ,EAAkB;AAChB,WAAO,GAAG,CAAC,IAAJ,CAAS,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAT,EAAuB,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAvB,EAAqC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAArC,CAAP;AACD;;AACD,SAAO,GAAP;AACD;;AAED,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAjB;AACP,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAjB;AACP,OAAO,MAAM,eAAe,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA1B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport * as conv_util from '../ops/conv_util';\nimport { op } from '../ops/operation';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { add } from './add';\nimport * as broadcast_util from './broadcast_util';\nimport { conv2d as unfusedConv2d } from './conv2d';\nimport { conv2DBackpropFilter } from './conv2d_backprop_filter';\nimport { conv2DBackpropInput } from './conv2d_backprop_input';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from './depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from './depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from './depthwise_conv2d_native_backprop_input';\nimport { elu } from './elu';\nimport { shouldFuse } from './fused_util';\nimport { matMul as unfusedMatMul } from './mat_mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\n// Returns gradient for fused activation.\nconst getFusedDyActivation = (dy, y, activation) => {\n    if (activation == null || activation === 'linear') {\n        return dy;\n    }\n    if (activation === 'relu') {\n        return dy.mul(y.step());\n    }\n    throw new Error(`Gradient for activation ${activation} has not been ` +\n        `implemented yet.`);\n};\n// Returns gradient for fused bias.\nconst getFusedBiasGradient = (bias, dyActivation) => {\n    let res = dyActivation;\n    const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n    if (reduceAxes.length > 0) {\n        res = res.sum(reduceAxes);\n    }\n    return res.reshape(bias.shape);\n};\nconst applyActivation = (x, activation, preluActivationWeights) => {\n    if (activation === 'linear') {\n        return x;\n    }\n    else if (activation === 'relu') {\n        return relu(x);\n    }\n    else if (activation === 'elu') {\n        return elu(x);\n    }\n    else if (activation === 'relu6') {\n        return relu6(x);\n    }\n    else if (activation === 'prelu') {\n        return prelu(x, preluActivationWeights);\n    }\n    throw new Error(`Unknown fused activation ${activation}.`);\n};\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at least ` +\n        `2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ? $a.as3D(batchDimA, innerShapeA, outerShapeA) :\n        $a.as3D(batchDimA, outerShapeA, innerShapeA);\n    const b3D = transposeB ? $b.as3D(batchDimB, outerShapeB, innerShapeB) :\n        $b.as3D(batchDimB, innerShapeB, outerShapeB);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        let biasGradient = {};\n        if (bias != null) {\n            biasGradient = { bias: () => getFusedBiasGradient($bias, dyActivation) };\n        }\n        if (!transposeA && !transposeB) {\n            return Object.assign({\n                a: () => dyActivation.matMul(b3D, false, true),\n                b: () => a3D.matMul(dyActivation, true, false)\n            }, biasGradient);\n        }\n        else if (!transposeA && transposeB) {\n            return Object.assign({\n                a: () => dyActivation.matMul(b3D, false, false),\n                b: () => dyActivation.matMul(a3D, true, false)\n            }, biasGradient);\n        }\n        else if (transposeA && !transposeB) {\n            return Object.assign({\n                a: () => b3D.matMul(dyActivation, false, true),\n                b: () => a3D.matMul(dyActivation, false, false)\n            }, biasGradient);\n        }\n        else {\n            return Object.assign({\n                a: () => b3D.matMul(dyActivation, true, true),\n                b: () => dyActivation.matMul(a3D, true, true)\n            }, biasGradient);\n        }\n    };\n    const inputs = { a: a3D, b: b3D };\n    if (bias != null) {\n        inputs.bias = $bias;\n    }\n    if (preluActivationWeights != null) {\n        inputs.preluActivationWeights = $preluActivationWeights;\n    }\n    const inputsToSave = [a3D, b3D];\n    const outputsToSave = [true];\n    const res = ENGINE.runKernelFunc((backend, save) => {\n        const y = backend.fusedBatchMatMul({\n            a: a3D,\n            b: b3D,\n            transposeA,\n            transposeB,\n            bias: $bias,\n            activation,\n            preluActivationWeights: $preluActivationWeights\n        });\n        save([a3D, b3D, y]);\n        return y;\n    }, inputs, grad, '_FusedMatMul', { transposeA, transposeB, activation }, inputsToSave, outputsToSave);\n    return res.reshape(outShape);\n}\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n */\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights }) {\n    activation = activation || 'linear';\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights);\n    }\n    const $x = convertToTensor(x, 'x', 'conv2d');\n    const $filter = convertToTensor(filter, 'filter', 'conv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = $x.as4D(1, $x.shape[0], $x.shape[1], $x.shape[2]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n        `${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n        `${$filter.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\n        `input depth for filter ${$filter.shape[2]}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n    }\n    const grad = (dy, saved) => {\n        const [$filter, x4D, y] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        let biasGradient = {};\n        if (bias != null) {\n            biasGradient = { bias: () => getFusedBiasGradient($bias, dyActivation) };\n        }\n        return Object.assign({\n            x: () => conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad),\n            filter: () => conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad)\n        }, biasGradient);\n    };\n    const inputs = { x: x4D, filter: $filter };\n    if (bias != null) {\n        inputs.bias = $bias;\n    }\n    if (preluActivationWeights != null) {\n        inputs.preluActivationWeights = $preluActivationWeights;\n    }\n    const inputsToSave = [$filter, x4D];\n    const outputsToSave = [true]; // Save the only output.\n    const res = ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.fusedConv2d({\n            input: x4D,\n            filter: $filter,\n            convInfo,\n            bias: $bias,\n            activation,\n            preluActivationWeights: $preluActivationWeights\n        });\n        save([$filter, x4D, res]);\n        return res;\n    }, inputs, grad, 'FusedConv2D', { convInfo, activation }, inputsToSave, outputsToSave);\n    if (reshapedTo4D) {\n        return res.as3D(res.shape[1], res.shape[2], res.shape[3]);\n    }\n    return res;\n}\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n */\nfunction fusedDepthwiseConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights);\n    }\n    const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n    const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = $x.as4D(1, $x.shape[0], $x.shape[1], $x.shape[2]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n        `rank ${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n        `but got rank ${$filter.rank}.`);\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +\n        `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n        `filter ${$filter.shape[2]}.`);\n    if (dilations == null) {\n        dilations = [1, 1];\n    }\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n        `be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` +\n            `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n        const [$filter, x4D, y] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        let biasGradient = {};\n        if (bias != null) {\n            biasGradient = { bias: () => getFusedBiasGradient($bias, dyActivation) };\n        }\n        return Object.assign({\n            x: () => depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, convInfo),\n            filter: () => depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, convInfo),\n        }, biasGradient);\n    };\n    const inputs = { x: x4D, filter: $filter };\n    if (bias != null) {\n        inputs.bias = $bias;\n    }\n    if (preluActivationWeights != null) {\n        inputs.preluActivationWeights = $preluActivationWeights;\n    }\n    const inputsToSave = [$filter, x4D];\n    const outputsToSave = [true];\n    const res = ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.fusedDepthwiseConv2D({\n            input: x4D,\n            filter: $filter,\n            convInfo,\n            bias: $bias,\n            activation,\n            preluActivationWeights: $preluActivationWeights\n        });\n        save([$filter, x4D, res]);\n        return res;\n    }, inputs, grad, 'FusedDepthwiseConv2D', { convInfo, activation }, inputsToSave, outputsToSave);\n    if (reshapedTo4D) {\n        return res.as3D(res.shape[1], res.shape[2], res.shape[3]);\n    }\n    return res;\n}\nexport const matMul = op({ fusedMatMul_ });\nexport const conv2d = op({ fusedConv2d_ });\nexport const depthwiseConv2d = op({ fusedDepthwiseConv2d_ });\n//# sourceMappingURL=fused_ops.js.map"]},"metadata":{},"sourceType":"module"}