{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\n/**\n * Linear algebra ops.\n */\nimport { ENGINE } from '../engine';\nimport { dispose } from '../globals';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assert } from '../util';\nimport { squeeze, stack, unstack } from './array_ops';\nimport { eye } from './eye';\nimport { logicalAnd, where } from './logical_ops';\nimport { norm } from './norm';\nimport { op } from './operation';\nimport { sum } from './reduction_ops';\nimport { split } from './split';\nimport { sub } from './sub';\nimport { range, scalar, tensor2d, zeros } from './tensor_ops';\n/**\n * Copy a tensor setting everything outside a central band in each innermost\n * matrix to zero.\n *\n * The band part is computed as follows: Assume input has `k` dimensions\n * `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where\n * `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n * The indicator function\n * `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower))`\n * `&& (num_upper < 0 || (n-m) <= num_upper)`\n *\n * ```js\n * const x = tf.tensor2d([[ 0,  1,  2, 3],\n *                        [-1,  0,  1, 2],\n *                        [-2, -1,  0, 1],\n *                        [-3, -2, -1, 0]]);\n * let y = tf.linalg.bandPart(x, 1, -1);\n * y.print(); // [[ 0,  1,  2, 3],\n *            //  [-1,  0,  1, 2],\n *            //  [ 0, -1,  0, 1],\n *            //  [ 0, 0 , -1, 0]]\n * let z = tf.linalg.bandPart(x, 2, 1);\n * z.print(); // [[ 0,  1,  0, 0],\n *            //  [-1,  0,  1, 0],\n *            //  [-2, -1,  0, 1],\n *            //  [ 0, -2, -1, 0]]\n * ```\n *\n * @param x Rank `k` tensor\n * @param numLower Number of subdiagonals to keep.\n *   If negative, keep entire lower triangle.\n * @param numUpper Number of subdiagonals to keep.\n *   If negative, keep entire upper triangle.\n * @returns Rank `k` tensor of the same shape as input.\n *   The extracted banded tensor.\n */\n\n/**\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\n\nfunction bandPart_(a, numLower, numUpper) {\n  if (numLower % 1 !== 0) {\n    throw new Error(`bandPart(): numLower must be an integer, got ${numLower}.`);\n  }\n\n  if (numUpper % 1 !== 0) {\n    throw new Error(`bandPart(): numUpper must be an integer, got ${numUpper}.`);\n  }\n\n  const $a = convertToTensor(a, 'a', 'bandPart');\n\n  if ($a.rank < 2) {\n    throw new Error(`bandPart(): Rank must be at least 2, got ${$a.rank}.`);\n  }\n\n  const shape = $a.shape,\n        [M, N] = $a.shape.slice(-2);\n\n  if (!(numLower <= M)) {\n    throw new Error(`bandPart(): numLower (${numLower})` + ` must not be greater than the number of rows (${M}).`);\n  }\n\n  if (!(numUpper <= N)) {\n    throw new Error(`bandPart(): numUpper (${numUpper})` + ` must not be greater than the number of columns (${N}).`);\n  }\n\n  if (numLower < 0) {\n    numLower = M;\n  }\n\n  if (numUpper < 0) {\n    numUpper = N;\n  }\n\n  const i = range(0, M, 1, 'int32').reshape([-1, 1]),\n        j = range(0, N, 1, 'int32'),\n        ij = sub(i, j);\n  const inBand = logicalAnd(ij.lessEqual(scalar(+numLower, 'int32')), ij.greaterEqual(scalar(-numUpper, 'int32')));\n  const zero = zeros([M, N], $a.dtype);\n  return stack(unstack($a.reshape([-1, M, N])).map(mat => where(inBand, mat, zero))).reshape(shape);\n}\n/**\n * Gram-Schmidt orthogonalization.\n *\n * ```js\n * const x = tf.tensor2d([[1, 2], [3, 4]]);\n * let y = tf.linalg.gramSchmidt(x);\n * y.print();\n * console.log('Othogonalized:');\n * y.dot(y.transpose()).print();  // should be nearly the identity matrix.\n * console.log('First row direction maintained:');\n * const data = await y.array();\n * console.log(data[0][1] / data[0][0]);  // should be nearly 2.\n * ```\n *\n * @param xs The vectors to be orthogonalized, in one of the two following\n *   formats:\n *   - An Array of `tf.Tensor1D`.\n *   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows\n *     of `xs`.\n *   In each case, all the vectors must have the same length and the length\n *   must be greater than or equal to the number of vectors.\n * @returns The orthogonalized and normalized vectors or matrix.\n *   Orthogonalization means that the vectors or the rows of the matrix\n *   are orthogonal (zero inner products). Normalization means that each\n *   vector or each row of the matrix has an L2 norm that equals `1`.\n */\n\n/**\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\n\n\nfunction gramSchmidt_(xs) {\n  let inputIsTensor2D;\n\n  if (Array.isArray(xs)) {\n    inputIsTensor2D = false;\n    assert(xs != null && xs.length > 0, () => 'Gram-Schmidt process: input must not be null, undefined, or ' + 'empty');\n    const dim = xs[0].shape[0];\n\n    for (let i = 1; i < xs.length; ++i) {\n      assert(xs[i].shape[0] === dim, () => 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' + `(${xs[i].shape[0]} vs. ${dim})`);\n    }\n  } else {\n    inputIsTensor2D = true;\n    xs = split(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));\n  }\n\n  assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` + `number of dimensions (${xs[0].shape[0]}).`);\n  const ys = [];\n  const xs1d = xs;\n\n  for (let i = 0; i < xs.length; ++i) {\n    ys.push(ENGINE.tidy(() => {\n      let x = xs1d[i];\n\n      if (i > 0) {\n        for (let j = 0; j < i; ++j) {\n          const proj = sum(ys[j].mul(x)).mul(ys[j]);\n          x = x.sub(proj);\n        }\n      }\n\n      return x.div(norm(x, 'euclidean'));\n    }));\n  }\n\n  if (inputIsTensor2D) {\n    return stack(ys, 0);\n  } else {\n    return ys;\n  }\n}\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n */\n\n/**\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\n\n\nfunction qr_(x, fullMatrices = false) {\n  if (x.rank < 2) {\n    throw new Error(`qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);\n  } else if (x.rank === 2) {\n    return qr2d(x, fullMatrices);\n  } else {\n    // Rank > 2.\n    // TODO(cais): Below we split the input into individual 2D tensors,\n    //   perform QR decomposition on them and then stack the results back\n    //   together. We should explore whether this can be parallelized.\n    const outerDimsProd = x.shape.slice(0, x.shape.length - 2).reduce((value, prev) => value * prev);\n    const x2ds = unstack(x.reshape([outerDimsProd, x.shape[x.shape.length - 2], x.shape[x.shape.length - 1]]), 0);\n    const q2ds = [];\n    const r2ds = [];\n    x2ds.forEach(x2d => {\n      const [q2d, r2d] = qr2d(x2d, fullMatrices);\n      q2ds.push(q2d);\n      r2ds.push(r2d);\n    });\n    const q = stack(q2ds, 0).reshape(x.shape);\n    const r = stack(r2ds, 0).reshape(x.shape);\n    return [q, r];\n  }\n}\n\nfunction qr2d(x, fullMatrices = false) {\n  return ENGINE.tidy(() => {\n    if (x.shape.length !== 2) {\n      throw new Error(`qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);\n    }\n\n    const m = x.shape[0];\n    const n = x.shape[1];\n    let q = eye(m); // Orthogonal transform so far.\n\n    let r = x.clone(); // Transformed matrix so far.\n\n    const one2D = tensor2d([[1]], [1, 1]);\n    let w = one2D.clone();\n    const iters = m >= n ? n : m;\n\n    for (let j = 0; j < iters; ++j) {\n      // This tidy within the for-loop ensures we clean up temporary\n      // tensors as soon as they are no longer needed.\n      const rTemp = r;\n      const wTemp = w;\n      const qTemp = q;\n      [w, r, q] = ENGINE.tidy(() => {\n        // Find H = I - tau * w * w', to put zeros below R(j, j).\n        const rjEnd1 = r.slice([j, j], [m - j, 1]);\n        const normX = rjEnd1.norm();\n        const rjj = r.slice([j, j], [1, 1]); // The sign() function returns 0 on 0, which causes division by zero.\n\n        const s = tensor2d([[-1]]).where(rjj.greater(0), tensor2d([[1]]));\n        const u1 = rjj.sub(s.mul(normX));\n        const wPre = rjEnd1.div(u1);\n\n        if (wPre.shape[0] === 1) {\n          w = one2D.clone();\n        } else {\n          w = one2D.concat(wPre.slice([1, 0], [wPre.shape[0] - 1, wPre.shape[1]]), 0);\n        }\n\n        const tau = s.matMul(u1).div(normX).neg(); // -- R := HR, Q := QH.\n\n        const rjEndAll = r.slice([j, 0], [m - j, n]);\n        const tauTimesW = tau.mul(w);\n        const wT = w.transpose();\n\n        if (j === 0) {\n          r = rjEndAll.sub(tauTimesW.matMul(wT.matMul(rjEndAll)));\n        } else {\n          const rTimesTau = rjEndAll.sub(tauTimesW.matMul(wT.matMul(rjEndAll)));\n          r = r.slice([0, 0], [j, n]).concat(rTimesTau, 0);\n        }\n\n        const tawTimesWT = tauTimesW.transpose();\n        const qAllJEnd = q.slice([0, j], [m, q.shape[1] - j]);\n\n        if (j === 0) {\n          q = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tawTimesWT));\n        } else {\n          const qTimesTau = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tawTimesWT));\n          q = q.slice([0, 0], [m, j]).concat(qTimesTau, 1);\n        }\n\n        return [w, r, q];\n      });\n      dispose([rTemp, wTemp, qTemp]);\n    }\n\n    if (!fullMatrices && m > n) {\n      q = q.slice([0, 0], [m, n]);\n      r = r.slice([0, 0], [n, n]);\n    }\n\n    return [q, r];\n  });\n}\n\nexport const bandPart = op({\n  bandPart_\n});\nexport const gramSchmidt = op({\n  gramSchmidt_\n});\nexport const qr = op({\n  qr_\n});","map":{"version":3,"sources":["../../src/ops/linalg_ops.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;;;AAiBA;;;AAIA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,OAAR,QAAsB,YAAtB;AAEA,SAAQ,eAAR,QAA8B,oBAA9B;AAEA,SAAQ,MAAR,QAAqB,SAArB;AAEA,SAAQ,OAAR,EAAiB,KAAjB,EAAwB,OAAxB,QAAsC,aAAtC;AACA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAQ,UAAR,EAAoB,KAApB,QAAgC,eAAhC;AACA,SAAQ,IAAR,QAAmB,QAAnB;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,GAAR,QAAkB,iBAAlB;AACA,SAAQ,KAAR,QAAoB,SAApB;AACA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAQ,KAAR,EAAe,MAAf,EAAuB,QAAvB,EAAiC,KAAjC,QAA6C,cAA7C;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAoCA;;;;;;AAKA,SAAS,SAAT,CACI,CADJ,EACqB,QADrB,EACuC,QADvC,EACuD;AACrD,MAAI,QAAQ,GAAG,CAAX,KAAiB,CAArB,EAAwB;AACtB,UAAM,IAAI,KAAJ,CACF,gDAAgD,QAAQ,GADtD,CAAN;AAED;;AACD,MAAI,QAAQ,GAAG,CAAX,KAAiB,CAArB,EAAwB;AACtB,UAAM,IAAI,KAAJ,CACF,gDAAgD,QAAQ,GADtD,CAAN;AAED;;AAED,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,UAAT,CAA1B;;AAEA,MAAI,EAAE,CAAC,IAAH,GAAU,CAAd,EAAiB;AACf,UAAM,IAAI,KAAJ,CAAU,4CAA4C,EAAE,CAAC,IAAI,GAA7D,CAAN;AACD;;AAED,QAAM,KAAK,GAAG,EAAE,CAAC,KAAjB;AAAA,QAAwB,CAAC,CAAD,EAAI,CAAJ,IAAS,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAC,CAAhB,CAAjC;;AAEA,MAAI,EAAE,QAAQ,IAAI,CAAd,CAAJ,EAAsB;AACpB,UAAM,IAAI,KAAJ,CACF,yBAAyB,QAAQ,GAAjC,GACA,iDAAiD,CAAC,IAFhD,CAAN;AAGD;;AACD,MAAI,EAAE,QAAQ,IAAI,CAAd,CAAJ,EAAsB;AACpB,UAAM,IAAI,KAAJ,CACF,yBAAyB,QAAQ,GAAjC,GACA,oDAAoD,CAAC,IAFnD,CAAN;AAGD;;AAED,MAAI,QAAQ,GAAG,CAAf,EAAkB;AAChB,IAAA,QAAQ,GAAG,CAAX;AACD;;AACD,MAAI,QAAQ,GAAG,CAAf,EAAkB;AAChB,IAAA,QAAQ,GAAG,CAAX;AACD;;AAED,QAAM,CAAC,GAAG,KAAK,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,OAAV,CAAL,CAAwB,OAAxB,CAAgC,CAAC,CAAC,CAAF,EAAK,CAAL,CAAhC,CAAV;AAAA,QACM,CAAC,GAAG,KAAK,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,OAAV,CADf;AAAA,QACmC,EAAE,GAAG,GAAG,CAAC,CAAD,EAAI,CAAJ,CAD3C;AAGA,QAAM,MAAM,GAAG,UAAU,CACrB,EAAE,CAAC,SAAH,CAAa,MAAM,CAAC,CAAC,QAAF,EAAY,OAAZ,CAAnB,CADqB,EAErB,EAAE,CAAC,YAAH,CAAgB,MAAM,CAAC,CAAC,QAAF,EAAY,OAAZ,CAAtB,CAFqB,CAAzB;AAIA,QAAM,IAAI,GAAG,KAAK,CAAC,CAAC,CAAD,EAAI,CAAJ,CAAD,EAAS,EAAE,CAAC,KAAZ,CAAlB;AAEA,SAAO,KAAK,CAAC,OAAO,CAAC,EAAE,CAAC,OAAH,CAAW,CAAC,CAAC,CAAF,EAAK,CAAL,EAAQ,CAAR,CAAX,CAAD,CAAP,CACK,GADL,CACS,GAAG,IAAI,KAAK,CAAC,MAAD,EAAS,GAAT,EAAc,IAAd,CADrB,CAAD,CAAL,CAEK,OAFL,CAEa,KAFb,CAAP;AAGD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA;;;;;;;AAKA,SAAS,YAAT,CAAsB,EAAtB,EAA6C;AAC3C,MAAI,eAAJ;;AACA,MAAI,KAAK,CAAC,OAAN,CAAc,EAAd,CAAJ,EAAuB;AACrB,IAAA,eAAe,GAAG,KAAlB;AACA,IAAA,MAAM,CACF,EAAE,IAAI,IAAN,IAAc,EAAE,CAAC,MAAH,GAAY,CADxB,EAEF,MAAM,iEACF,OAHF,CAAN;AAIA,UAAM,GAAG,GAAG,EAAE,CAAC,CAAD,CAAF,CAAM,KAAN,CAAY,CAAZ,CAAZ;;AACA,SAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,EAAE,CAAC,MAAvB,EAA+B,EAAE,CAAjC,EAAoC;AAClC,MAAA,MAAM,CACF,EAAE,CAAC,CAAD,CAAF,CAAM,KAAN,CAAY,CAAZ,MAAmB,GADjB,EAEF,MACI,kEACA,IAAK,EAAiB,CAAC,CAAD,CAAjB,CAAqB,KAArB,CAA2B,CAA3B,CAA6B,QAAQ,GAAG,GAJ/C,CAAN;AAKD;AACF,GAdD,MAcO;AACL,IAAA,eAAe,GAAG,IAAlB;AACA,IAAA,EAAE,GAAG,KAAK,CAAC,EAAD,EAAK,EAAE,CAAC,KAAH,CAAS,CAAT,CAAL,EAAkB,CAAlB,CAAL,CAA0B,GAA1B,CAA8B,CAAC,IAAI,OAAO,CAAC,CAAD,EAAI,CAAC,CAAD,CAAJ,CAA1C,CAAL;AACD;;AAED,EAAA,MAAM,CACF,EAAE,CAAC,MAAH,IAAa,EAAE,CAAC,CAAD,CAAF,CAAM,KAAN,CAAY,CAAZ,CADX,EAEF,MAAM,oCACK,EAAiB,CAAC,MAAM,YAD7B,GAEF,yBAA0B,EAAiB,CAAC,CAAD,CAAjB,CAAqB,KAArB,CAA2B,CAA3B,CAA6B,IAJzD,CAAN;AAMA,QAAM,EAAE,GAAe,EAAvB;AACA,QAAM,IAAI,GAAG,EAAb;;AACA,OAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,EAAE,CAAC,MAAvB,EAA+B,EAAE,CAAjC,EAAoC;AAClC,IAAA,EAAE,CAAC,IAAH,CAAQ,MAAM,CAAC,IAAP,CAAY,MAAK;AACvB,UAAI,CAAC,GAAG,IAAI,CAAC,CAAD,CAAZ;;AACA,UAAI,CAAC,GAAG,CAAR,EAAW;AACT,aAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,CAApB,EAAuB,EAAE,CAAzB,EAA4B;AAC1B,gBAAM,IAAI,GAAG,GAAG,CAAC,EAAE,CAAC,CAAD,CAAF,CAAM,GAAN,CAAU,CAAV,CAAD,CAAH,CAAkB,GAAlB,CAAsB,EAAE,CAAC,CAAD,CAAxB,CAAb;AACA,UAAA,CAAC,GAAG,CAAC,CAAC,GAAF,CAAM,IAAN,CAAJ;AACD;AACF;;AACD,aAAO,CAAC,CAAC,GAAF,CAAM,IAAI,CAAC,CAAD,EAAI,WAAJ,CAAV,CAAP;AACD,KATO,CAAR;AAUD;;AAED,MAAI,eAAJ,EAAqB;AACnB,WAAO,KAAK,CAAC,EAAD,EAAK,CAAL,CAAZ;AACD,GAFD,MAEO;AACL,WAAO,EAAP;AACD;AACF;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAuCA;;;;;;;AAKA,SAAS,GAAT,CAAa,CAAb,EAAwB,YAAY,GAAG,KAAvC,EAA4C;AAC1C,MAAI,CAAC,CAAC,IAAF,GAAS,CAAb,EAAgB;AACd,UAAM,IAAI,KAAJ,CACF,gEACI,CAAC,CAAC,IAAI,EAFR,CAAN;AAGD,GAJD,MAIO,IAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;AACvB,WAAO,IAAI,CAAC,CAAD,EAAgB,YAAhB,CAAX;AACD,GAFM,MAEA;AACL;AACA;AACA;AACA;AACA,UAAM,aAAa,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd,EAAiB,CAAC,CAAC,KAAF,CAAQ,MAAR,GAAiB,CAAlC,EACK,MADL,CACY,CAAC,KAAD,EAAQ,IAAR,KAAiB,KAAK,GAAG,IADrC,CAAtB;AAEA,UAAM,IAAI,GAAG,OAAO,CAChB,CAAC,CAAC,OAAF,CAAU,CACR,aADQ,EACO,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAC,KAAF,CAAQ,MAAR,GAAiB,CAAzB,CADP,EAER,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAC,KAAF,CAAQ,MAAR,GAAiB,CAAzB,CAFQ,CAAV,CADgB,EAKhB,CALgB,CAApB;AAMA,UAAM,IAAI,GAAe,EAAzB;AACA,UAAM,IAAI,GAAe,EAAzB;AACA,IAAA,IAAI,CAAC,OAAL,CAAa,GAAG,IAAG;AACjB,YAAM,CAAC,GAAD,EAAM,GAAN,IAAa,IAAI,CAAC,GAAD,EAAkB,YAAlB,CAAvB;AACA,MAAA,IAAI,CAAC,IAAL,CAAU,GAAV;AACA,MAAA,IAAI,CAAC,IAAL,CAAU,GAAV;AACD,KAJD;AAKA,UAAM,CAAC,GAAG,KAAK,CAAC,IAAD,EAAO,CAAP,CAAL,CAAe,OAAf,CAAuB,CAAC,CAAC,KAAzB,CAAV;AACA,UAAM,CAAC,GAAG,KAAK,CAAC,IAAD,EAAO,CAAP,CAAL,CAAe,OAAf,CAAuB,CAAC,CAAC,KAAzB,CAAV;AACA,WAAO,CAAC,CAAD,EAAI,CAAJ,CAAP;AACD;AACF;;AAED,SAAS,IAAT,CAAc,CAAd,EAA2B,YAAY,GAAG,KAA1C,EAA+C;AAC7C,SAAO,MAAM,CAAC,IAAP,CAAY,MAAK;AACtB,QAAI,CAAC,CAAC,KAAF,CAAQ,MAAR,KAAmB,CAAvB,EAA0B;AACxB,YAAM,IAAI,KAAJ,CACF,0CAA0C,CAAC,CAAC,KAAF,CAAQ,MAAM,WADtD,CAAN;AAED;;AAED,UAAM,CAAC,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,CAAV;AACA,UAAM,CAAC,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,CAAV;AAEA,QAAI,CAAC,GAAG,GAAG,CAAC,CAAD,CAAX,CATsB,CASF;;AACpB,QAAI,CAAC,GAAG,CAAC,CAAC,KAAF,EAAR,CAVsB,CAUF;;AAEpB,UAAM,KAAK,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAD,CAAD,CAAD,EAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,CAAtB;AACA,QAAI,CAAC,GAAa,KAAK,CAAC,KAAN,EAAlB;AAEA,UAAM,KAAK,GAAG,CAAC,IAAI,CAAL,GAAS,CAAT,GAAa,CAA3B;;AACA,SAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAApB,EAA2B,EAAE,CAA7B,EAAgC;AAC9B;AACA;AACA,YAAM,KAAK,GAAG,CAAd;AACA,YAAM,KAAK,GAAG,CAAd;AACA,YAAM,KAAK,GAAG,CAAd;AACA,OAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,IAAY,MAAM,CAAC,IAAP,CAAY,MAAqC;AAC3D;AACA,cAAM,MAAM,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAC,GAAG,CAAL,EAAQ,CAAR,CAAhB,CAAf;AACA,cAAM,KAAK,GAAG,MAAM,CAAC,IAAP,EAAd;AACA,cAAM,GAAG,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAD,EAAI,CAAJ,CAAhB,CAAZ,CAJ2D,CAM3D;;AACA,cAAM,CAAC,GAAG,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAF,CAAD,CAAD,CAAR,CAAiB,KAAjB,CAAuB,GAAG,CAAC,OAAJ,CAAY,CAAZ,CAAvB,EAAuC,QAAQ,CAAC,CAAC,CAAC,CAAD,CAAD,CAAD,CAA/C,CAAV;AAEA,cAAM,EAAE,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAC,CAAC,GAAF,CAAM,KAAN,CAAR,CAAX;AACA,cAAM,IAAI,GAAG,MAAM,CAAC,GAAP,CAAW,EAAX,CAAb;;AACA,YAAI,IAAI,CAAC,KAAL,CAAW,CAAX,MAAkB,CAAtB,EAAyB;AACvB,UAAA,CAAC,GAAG,KAAK,CAAC,KAAN,EAAJ;AACD,SAFD,MAEO;AACL,UAAA,CAAC,GAAG,KAAK,CAAC,MAAN,CACA,IAAI,CAAC,KAAL,CAAW,CAAC,CAAD,EAAI,CAAJ,CAAX,EAAmB,CAAC,IAAI,CAAC,KAAL,CAAW,CAAX,IAAgB,CAAjB,EAAoB,IAAI,CAAC,KAAL,CAAW,CAAX,CAApB,CAAnB,CADA,EAGA,CAHA,CAAJ;AAID;;AACD,cAAM,GAAG,GAAG,CAAC,CAAC,MAAF,CAAS,EAAT,EAAa,GAAb,CAAiB,KAAjB,EAAwB,GAAxB,EAAZ,CAnB2D,CAqB3D;;AACA,cAAM,QAAQ,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAC,GAAG,CAAL,EAAQ,CAAR,CAAhB,CAAjB;AACA,cAAM,SAAS,GAAa,GAAG,CAAC,GAAJ,CAAQ,CAAR,CAA5B;AACA,cAAM,EAAE,GAAa,CAAC,CAAC,SAAF,EAArB;;AACA,YAAI,CAAC,KAAK,CAAV,EAAa;AACX,UAAA,CAAC,GAAG,QAAQ,CAAC,GAAT,CAAa,SAAS,CAAC,MAAV,CAAiB,EAAE,CAAC,MAAH,CAAU,QAAV,CAAjB,CAAb,CAAJ;AACD,SAFD,MAEO;AACL,gBAAM,SAAS,GACX,QAAQ,CAAC,GAAT,CAAa,SAAS,CAAC,MAAV,CAAiB,EAAE,CAAC,MAAH,CAAU,QAAV,CAAjB,CAAb,CADJ;AAEA,UAAA,CAAC,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAD,EAAI,CAAJ,CAAhB,EAAwB,MAAxB,CAA+B,SAA/B,EAA0C,CAA1C,CAAJ;AACD;;AACD,cAAM,UAAU,GAAa,SAAS,CAAC,SAAV,EAA7B;AACA,cAAM,QAAQ,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAD,EAAI,CAAC,CAAC,KAAF,CAAQ,CAAR,IAAa,CAAjB,CAAhB,CAAjB;;AACA,YAAI,CAAC,KAAK,CAAV,EAAa;AACX,UAAA,CAAC,GAAG,QAAQ,CAAC,GAAT,CAAa,QAAQ,CAAC,MAAT,CAAgB,CAAhB,EAAmB,MAAnB,CAA0B,UAA1B,CAAb,CAAJ;AACD,SAFD,MAEO;AACL,gBAAM,SAAS,GACX,QAAQ,CAAC,GAAT,CAAa,QAAQ,CAAC,MAAT,CAAgB,CAAhB,EAAmB,MAAnB,CAA0B,UAA1B,CAAb,CADJ;AAEA,UAAA,CAAC,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAD,EAAI,CAAJ,CAAhB,EAAwB,MAAxB,CAA+B,SAA/B,EAA0C,CAA1C,CAAJ;AACD;;AACD,eAAO,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,CAAP;AACD,OA1CW,CAAZ;AA2CA,MAAA,OAAO,CAAC,CAAC,KAAD,EAAQ,KAAR,EAAe,KAAf,CAAD,CAAP;AACD;;AAED,QAAI,CAAC,YAAD,IAAiB,CAAC,GAAG,CAAzB,EAA4B;AAC1B,MAAA,CAAC,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAD,EAAI,CAAJ,CAAhB,CAAJ;AACA,MAAA,CAAC,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAC,CAAD,EAAI,CAAJ,CAAR,EAAgB,CAAC,CAAD,EAAI,CAAJ,CAAhB,CAAJ;AACD;;AAED,WAAO,CAAC,CAAD,EAAI,CAAJ,CAAP;AACD,GA1EM,CAAP;AA2ED;;AAED,OAAO,MAAM,QAAQ,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAnB;AACP,OAAO,MAAM,WAAW,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAtB;AACP,OAAO,MAAM,EAAE,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAb","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n/**\n * Linear algebra ops.\n */\nimport { ENGINE } from '../engine';\nimport { dispose } from '../globals';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assert } from '../util';\nimport { squeeze, stack, unstack } from './array_ops';\nimport { eye } from './eye';\nimport { logicalAnd, where } from './logical_ops';\nimport { norm } from './norm';\nimport { op } from './operation';\nimport { sum } from './reduction_ops';\nimport { split } from './split';\nimport { sub } from './sub';\nimport { range, scalar, tensor2d, zeros } from './tensor_ops';\n/**\n * Copy a tensor setting everything outside a central band in each innermost\n * matrix to zero.\n *\n * The band part is computed as follows: Assume input has `k` dimensions\n * `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where\n * `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n * The indicator function\n * `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower))`\n * `&& (num_upper < 0 || (n-m) <= num_upper)`\n *\n * ```js\n * const x = tf.tensor2d([[ 0,  1,  2, 3],\n *                        [-1,  0,  1, 2],\n *                        [-2, -1,  0, 1],\n *                        [-3, -2, -1, 0]]);\n * let y = tf.linalg.bandPart(x, 1, -1);\n * y.print(); // [[ 0,  1,  2, 3],\n *            //  [-1,  0,  1, 2],\n *            //  [ 0, -1,  0, 1],\n *            //  [ 0, 0 , -1, 0]]\n * let z = tf.linalg.bandPart(x, 2, 1);\n * z.print(); // [[ 0,  1,  0, 0],\n *            //  [-1,  0,  1, 0],\n *            //  [-2, -1,  0, 1],\n *            //  [ 0, -2, -1, 0]]\n * ```\n *\n * @param x Rank `k` tensor\n * @param numLower Number of subdiagonals to keep.\n *   If negative, keep entire lower triangle.\n * @param numUpper Number of subdiagonals to keep.\n *   If negative, keep entire upper triangle.\n * @returns Rank `k` tensor of the same shape as input.\n *   The extracted banded tensor.\n */\n/**\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction bandPart_(a, numLower, numUpper) {\n    if (numLower % 1 !== 0) {\n        throw new Error(`bandPart(): numLower must be an integer, got ${numLower}.`);\n    }\n    if (numUpper % 1 !== 0) {\n        throw new Error(`bandPart(): numUpper must be an integer, got ${numUpper}.`);\n    }\n    const $a = convertToTensor(a, 'a', 'bandPart');\n    if ($a.rank < 2) {\n        throw new Error(`bandPart(): Rank must be at least 2, got ${$a.rank}.`);\n    }\n    const shape = $a.shape, [M, N] = $a.shape.slice(-2);\n    if (!(numLower <= M)) {\n        throw new Error(`bandPart(): numLower (${numLower})` +\n            ` must not be greater than the number of rows (${M}).`);\n    }\n    if (!(numUpper <= N)) {\n        throw new Error(`bandPart(): numUpper (${numUpper})` +\n            ` must not be greater than the number of columns (${N}).`);\n    }\n    if (numLower < 0) {\n        numLower = M;\n    }\n    if (numUpper < 0) {\n        numUpper = N;\n    }\n    const i = range(0, M, 1, 'int32').reshape([-1, 1]), j = range(0, N, 1, 'int32'), ij = sub(i, j);\n    const inBand = logicalAnd(ij.lessEqual(scalar(+numLower, 'int32')), ij.greaterEqual(scalar(-numUpper, 'int32')));\n    const zero = zeros([M, N], $a.dtype);\n    return stack(unstack($a.reshape([-1, M, N]))\n        .map(mat => where(inBand, mat, zero)))\n        .reshape(shape);\n}\n/**\n * Gram-Schmidt orthogonalization.\n *\n * ```js\n * const x = tf.tensor2d([[1, 2], [3, 4]]);\n * let y = tf.linalg.gramSchmidt(x);\n * y.print();\n * console.log('Othogonalized:');\n * y.dot(y.transpose()).print();  // should be nearly the identity matrix.\n * console.log('First row direction maintained:');\n * const data = await y.array();\n * console.log(data[0][1] / data[0][0]);  // should be nearly 2.\n * ```\n *\n * @param xs The vectors to be orthogonalized, in one of the two following\n *   formats:\n *   - An Array of `tf.Tensor1D`.\n *   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows\n *     of `xs`.\n *   In each case, all the vectors must have the same length and the length\n *   must be greater than or equal to the number of vectors.\n * @returns The orthogonalized and normalized vectors or matrix.\n *   Orthogonalization means that the vectors or the rows of the matrix\n *   are orthogonal (zero inner products). Normalization means that each\n *   vector or each row of the matrix has an L2 norm that equals `1`.\n */\n/**\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction gramSchmidt_(xs) {\n    let inputIsTensor2D;\n    if (Array.isArray(xs)) {\n        inputIsTensor2D = false;\n        assert(xs != null && xs.length > 0, () => 'Gram-Schmidt process: input must not be null, undefined, or ' +\n            'empty');\n        const dim = xs[0].shape[0];\n        for (let i = 1; i < xs.length; ++i) {\n            assert(xs[i].shape[0] === dim, () => 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +\n                `(${xs[i].shape[0]} vs. ${dim})`);\n        }\n    }\n    else {\n        inputIsTensor2D = true;\n        xs = split(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));\n    }\n    assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` +\n        `number of dimensions (${xs[0].shape[0]}).`);\n    const ys = [];\n    const xs1d = xs;\n    for (let i = 0; i < xs.length; ++i) {\n        ys.push(ENGINE.tidy(() => {\n            let x = xs1d[i];\n            if (i > 0) {\n                for (let j = 0; j < i; ++j) {\n                    const proj = sum(ys[j].mul(x)).mul(ys[j]);\n                    x = x.sub(proj);\n                }\n            }\n            return x.div(norm(x, 'euclidean'));\n        }));\n    }\n    if (inputIsTensor2D) {\n        return stack(ys, 0);\n    }\n    else {\n        return ys;\n    }\n}\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n */\n/**\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction qr_(x, fullMatrices = false) {\n    if (x.rank < 2) {\n        throw new Error(`qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);\n    }\n    else if (x.rank === 2) {\n        return qr2d(x, fullMatrices);\n    }\n    else {\n        // Rank > 2.\n        // TODO(cais): Below we split the input into individual 2D tensors,\n        //   perform QR decomposition on them and then stack the results back\n        //   together. We should explore whether this can be parallelized.\n        const outerDimsProd = x.shape.slice(0, x.shape.length - 2)\n            .reduce((value, prev) => value * prev);\n        const x2ds = unstack(x.reshape([\n            outerDimsProd, x.shape[x.shape.length - 2],\n            x.shape[x.shape.length - 1]\n        ]), 0);\n        const q2ds = [];\n        const r2ds = [];\n        x2ds.forEach(x2d => {\n            const [q2d, r2d] = qr2d(x2d, fullMatrices);\n            q2ds.push(q2d);\n            r2ds.push(r2d);\n        });\n        const q = stack(q2ds, 0).reshape(x.shape);\n        const r = stack(r2ds, 0).reshape(x.shape);\n        return [q, r];\n    }\n}\nfunction qr2d(x, fullMatrices = false) {\n    return ENGINE.tidy(() => {\n        if (x.shape.length !== 2) {\n            throw new Error(`qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);\n        }\n        const m = x.shape[0];\n        const n = x.shape[1];\n        let q = eye(m); // Orthogonal transform so far.\n        let r = x.clone(); // Transformed matrix so far.\n        const one2D = tensor2d([[1]], [1, 1]);\n        let w = one2D.clone();\n        const iters = m >= n ? n : m;\n        for (let j = 0; j < iters; ++j) {\n            // This tidy within the for-loop ensures we clean up temporary\n            // tensors as soon as they are no longer needed.\n            const rTemp = r;\n            const wTemp = w;\n            const qTemp = q;\n            [w, r, q] = ENGINE.tidy(() => {\n                // Find H = I - tau * w * w', to put zeros below R(j, j).\n                const rjEnd1 = r.slice([j, j], [m - j, 1]);\n                const normX = rjEnd1.norm();\n                const rjj = r.slice([j, j], [1, 1]);\n                // The sign() function returns 0 on 0, which causes division by zero.\n                const s = tensor2d([[-1]]).where(rjj.greater(0), tensor2d([[1]]));\n                const u1 = rjj.sub(s.mul(normX));\n                const wPre = rjEnd1.div(u1);\n                if (wPre.shape[0] === 1) {\n                    w = one2D.clone();\n                }\n                else {\n                    w = one2D.concat(wPre.slice([1, 0], [wPre.shape[0] - 1, wPre.shape[1]]), 0);\n                }\n                const tau = s.matMul(u1).div(normX).neg();\n                // -- R := HR, Q := QH.\n                const rjEndAll = r.slice([j, 0], [m - j, n]);\n                const tauTimesW = tau.mul(w);\n                const wT = w.transpose();\n                if (j === 0) {\n                    r = rjEndAll.sub(tauTimesW.matMul(wT.matMul(rjEndAll)));\n                }\n                else {\n                    const rTimesTau = rjEndAll.sub(tauTimesW.matMul(wT.matMul(rjEndAll)));\n                    r = r.slice([0, 0], [j, n]).concat(rTimesTau, 0);\n                }\n                const tawTimesWT = tauTimesW.transpose();\n                const qAllJEnd = q.slice([0, j], [m, q.shape[1] - j]);\n                if (j === 0) {\n                    q = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tawTimesWT));\n                }\n                else {\n                    const qTimesTau = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tawTimesWT));\n                    q = q.slice([0, 0], [m, j]).concat(qTimesTau, 1);\n                }\n                return [w, r, q];\n            });\n            dispose([rTemp, wTemp, qTemp]);\n        }\n        if (!fullMatrices && m > n) {\n            q = q.slice([0, 0], [m, n]);\n            r = r.slice([0, 0], [n, n]);\n        }\n        return [q, r];\n    });\n}\nexport const bandPart = op({ bandPart_ });\nexport const gramSchmidt = op({ gramSchmidt_ });\nexport const qr = op({ qr_ });\n//# sourceMappingURL=linalg_ops.js.map"]},"metadata":{},"sourceType":"module"}